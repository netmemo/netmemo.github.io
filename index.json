[{"authors":[],"categories":["Automation","IAC","Cloud"],"content":" This post is to show an example of using CI/CD with Terraform Cloud and Github Actions in order to have a better NetDevOps approach by doing NSX-T Network Infrastructure as code (IaC). It\u0026rsquo;s almost a bingo, I think I have most of the buzz words of these last years :)\nI will describe the structure of the project, the project components, the project workflow and finish with how to test this project.\nStructure of the project The diagram below shows a high level view of the project. \nYou can find the file structure of the project below\n. ├─ main.tf ├─ .github └── workflows ├── dev-to-pr.yml ├── plan-prod.yml └── apply-prod.yml  Project components NSX-T For this project we need two NSXT environments, one for production and one for development. You can either have 2 full blown NSX-T or use different variables for prod and dev or different VRFs. In this blog post we will use two different deployments.\nTerraform Cloud Terraform Cloud will be used to store the state of the prod and dev environment. It will also allow to manage Terraform through APIs. For the project we need to create 2 differents workspaces, one for production and one for developments. The name of the workspaces should be prefix-suffix where the prefix will be what is configured in your terraform configuration file and the sufix will be what is used to select the workspace. We will cover how to select the workspace later in that post. For this blog post, the prefix will be netmemo- and the suffix will be either prod or dev. \nOnce the workspaces are created, we need to add 3 variables for the NSX-T provider: \nFinally we need to add the API key that Github Actions will use to connect to Terraform Cloud \nGithub Github is where we will store the configuration and execute our CI/CD pipelines. Once the project is forked, to make it works, we need to enable Github Actions. \nCreate a github personal access token REPO_TOKEN. This token will be used by Github Actions to automatically create the pull request.\n\nAdd the previously created token (TF_API_TOKEN and REPO_TOKEN_SECRET) to your github repository secrets.\n\nGithub Actions This project is made from 3 scripts that form 3 pipelines. The first script will handle the dev environment. The second will handle the terraform plan for the prod environments. The third will handle the terraform apply for the prod environment.\nYou can find an explanation of the main steps on the below site Github Actions.\ndev-to-pr.yml The dev-to-pr.yml Github Actions YAML file will be executed only after a push on the dev branch.\non: push: branches: - dev  The first step is to checkout the current configuration.\n - name: Checkout uses: actions/checkout@v2  Then the Setup Terraform steps retrieves the Terraform CLI used in the GitHub action workflow. This is in this step that we will use the TF_API_TOKEN that we have created previously to access Terraform Cloud from Github Actions.\n - name: Setup Terraform uses: hashicorp/setup-terraform@v1 with: # terraform_version: 0.13.0: cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}  These following steps initialize Terraform and set the terraform workspace.\n - name: Terraform Init id: init run: terraform init env: TF_WORKSPACE: \u0026quot;dev\u0026quot; - name: Terraform Workspace id: workspace run: terraform workspace select dev  We then validate the Terraform configuration\n - name: Terraform Validate id: validate run: terraform validate -no-color  This step is to execute the Terraform plan. The plan will be a speculative plan executed in Terraform Cloud. Speculative plans are not directly visible from the Terraform Cloud UI. To access it, you will need to click on the link given in the result of the Terraform plan command.\n - name: Terraform Plan id: plan run: terraform plan -no-color continue-on-error: true  This step will execute a github-script that will send a REST API query thanks to the github pre-authenticated octokit/rest.js client with pagination plugins. It will also create a comment on the commit with the result of the previous steps.\n - uses: actions/github-script@0.9.0 if: github.event_name == 'push' env: PLAN: \u0026quot;terraform\\n${{ steps.plan.outputs.stdout }}\u0026quot; with: github-token: ${{ secrets.GITHUB_TOKEN }} script: | const output = `#### Terraform Format and Style \u001b[36;63H\\`${{ steps.fmt.outcome }}\\` #### Terraform Initialization ⚙️\\`${{ steps.init.outcome }}\\` #### Terraform Validation \u001b[36;41H\\`${{ steps.validate.outcome }}\\` #### Terraform Plan \u001b[36;35H\\`${{ steps.plan.outcome }}\\` \u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;Show Plan\u0026lt;/summary\u0026gt; \\`\\`\\`\\n ${process.env.PLAN} \\`\\`\\` \u0026lt;/details\u0026gt; *Pusher: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`; github.repos.createCommitComment({ owner: context.repo.owner, repo: context.repo.repo, commit_sha: context.sha, body: output })  This step is to apply the Terraform configuration, only if the terraform plan step result has succeeded.\n - name: Terraform Apply id: apply if: steps.plan.outcome == 'success' run: terraform apply -auto-approve  If the terraform apply succeeds, we will use the github-script to create a Pull Request thanks to the octokit/rest.js like in the previous steps. We will use a personal github token PERSO_GITHUB_TOKEN to create the PR for these steps. If we are not using the personal token, the PR will not trigger other pipelines that have the on:pull_request trigger.\n - name: CreatePR if apply succeed uses: actions/github-script@v4.0.2 if: steps.apply.outcome == 'success' with: github-token: ${{ secrets.PERSO_GITHUB_TOKEN }} script: | github.pulls.create({ owner: context.repo.owner, repo: context.repo.repo, title: \u0026quot;Auto PR\u0026quot;, head: \u0026quot;dev\u0026quot;, base: \u0026quot;main\u0026quot; });  plan-prod.yml The plan-prod.yml Github Actions YAML file will be executed only after a pull request.\non: pull_request:  Most of the steps have already been described for the dev-to-pr.yml files, the only difference is the step below. It will add a comment on the PR with the description of what has been done.\n - uses: actions/github-script@v4.0.2 env: PLAN: \u0026quot;terraform\\n${{ steps.plan.outputs.stdout }}\u0026quot; with: github-token: ${{ secrets.GITHUB_TOKEN }} script: | const output = `#### Terraform Initialization ⚙️\\`${{ steps.init.outcome }}\\` #### Terraform Validation \u001b[36;41H\\`${{ steps.validate.outcome }}\\` #### Terraform Plan \u001b[36;35H\\`${{ steps.plan.outcome }}\\` \u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;Show Plan\u0026lt;/summary\u0026gt; \\`\\`\\`\\n ${process.env.PLAN} \\`\\`\\` \u0026lt;/details\u0026gt; *Pusher: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`; github.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: output })  apply-prod.yml The plan-prod.yml Github Actions YAML file will be executed only after a push on the main branch.\non: push: branches: - main  The other steps have already been discussed previously.\nTerraform file For this blog post we will have a single main.tf file. This Terraform script should have the terraform configuration, the provider definition and the resources definitions.\nTerraform variables This is where we declare the Terraform variables that will be defined in the Terrafrom Cloud workspaces. These variables will change according to the environment.\nvariable \u0026quot;password\u0026quot; { type = string } variable \u0026quot;username\u0026quot; { type = string } variable \u0026quot;nsxhost\u0026quot; { type = string }  Terraform configuration section In this section we are setting the 2 providers needed and the backend. The backend \u0026ldquo;remote\u0026ldquo; is where you find the Terraform Cloud organization and workspaces prefixes. The workspaces suffixes will be added as seen in the beginning of this post with the TF_WORKSPACE environment variables and the terraform workspace select command in the Github Actions steps.\nterraform { required_providers { random = { source = \u0026quot;hashicorp/random\u0026quot; version = \u0026quot;3.0.1\u0026quot; } nsxt = { source = \u0026quot;vmware/nsxt\u0026quot; version = \u0026quot;\u0026gt;= 3.1.1\u0026quot; } } backend \u0026quot;remote\u0026quot; { organization = \u0026quot;netmemo\u0026quot; workspaces { prefix = \u0026quot;netmemo-\u0026quot; } } }  Provider configuration This is the NSX-T provider configuration. We pass the 3 variables that are defined in the Terraform Cloud workspaces. We disable the SSL verification because this is a local lab.\nprovider \u0026quot;nsxt\u0026quot; { host = var.nsxhost username = var.username password = var.password allow_unverified_ssl = true }  Resource creation For this blog post we will create a NSX-T T1 gateway name T1-TFC.\nresource \u0026quot;nsxt_policy_tier1_gateway\u0026quot; \u0026quot;tier1_gw\u0026quot; { description = \u0026quot;Tier-1 provisioned by Terraform\u0026quot; display_name = \u0026quot;T1-TFC\u0026quot; route_advertisement_types = [\u0026quot;TIER1_CONNECTED\u0026quot;] }  Project workflow Clic the arrows to see the detail.  1. Modify the dev branch on you local git.   2. Add the modif to git, commit them and push the dev branch to your github repo.\ngit add . git commit -am \u0026quot;add T1-GW\u0026quot; [dev a0b946b] add T1-GW 2 files changed, 6 insertions(+), 6 deletions(-) git push Counting objects: 6, done. Delta compression using up to 2 threads. Compressing objects: 100% (5/5), done. Writing objects: 100% (6/6), 617 bytes | 617.00 KiB/s, done. Total 6 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To ssh://github.com/netmemo/tf-gha-nsxt-cicd.git 8534543..a0b946b dev -\u0026gt; dev    3. Dev pipeline The dev pipeline is triggered, you can see it\u0026rsquo;s starting in the Actions tab. It\u0026rsquo;s tittle will be the commit message.\n To validate that the dev pipeline is completed, you can check the Terraform cloud dev workspace state and the NSX-T dev environment to see that there is a new T1-GW.    4. Integration pipeline After all the steps in the dev pipeline are passed, the workflow will turn green. The last step of the pipeline will create an automatic PR that will trigger the integration pipeline. This pipeline will validate the configuration and creates a plan against the production environment. The PR will have the description \u0026ldquo;Auto PR generate by Github Action dev pipeline\u0026rdquo;  The integration pipeline will execute the terraform plan command against the production environment. You can then check again the Actions tab to see that the development and integration pipeline has turned green.  If the two pipelines are ok, the checks in the PR should be green.\n   5. Merge When the dev and integration pipeline are finished, the dev branch is ready to be merged to the main branch. You can then clic on the Merge pull request button to validate the merge and trigger the deployment pipeline.     6. Deployment pipeline The deployment pipeline is triggered with the push/merge to the main branch. This is the final workflow that will push the Terraform configuration to production. You can see in Terraform Cloud that Terraform is applying the configuration.  You can now double check in Github Actions that the deployment workflow has been succefully executed, \u0026ldquo;Merge pull request #10\u0026rdquo; in the picture below.    7. Checks Finally you can now verify that there is a resource on Terraform Cloud and that the T1 Gateway has been created on the production NSX-T.\n  \nHow to test this project. Requirements:\n- NSX-T environment (there are examples on the web to use AWS as provider instead of NSX, the concepts are the same).\n- Terraform Cloud account with 2 workspaces.\n- Github account and git locally.\n [NSX-T] Prepare your two NSX-T environments. [TFC] Create your two workspaces in TFC with the API Key for Github. [GITHUB] Fork this project, add the TFC API key and your personal API Key to your repo, enable actions. [LOCAL] Clone your forked project to your local git. [LOCAL] Create the dev branch locally. [LOCAL] Modify the Terraform configuration in the main.tf according to your TFC workspace. [LOCAL] Add to git, commit, push. [GITHUB] Wait until all the test pass. [GITHUB] Click merge to deploy to prod.  Pain points of the project.  Switching terraform workspace with github actions.\n Create automatically a github PR with with rest.js API of octokit.\n   ","date":1629028959,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629028959,"objectID":"1ed9e9260a9f1e929f6f1ce3b8d97cfc","permalink":"https://netmemo.github.io/post/tf-gha-nsxt-cicd/","publishdate":"2021-08-15T14:02:39+02:00","relpermalink":"/post/tf-gha-nsxt-cicd/","section":"post","summary":"This post is to show an example of using CI/CD with Terraform Cloud and Github Actions in order to have a better NetDevOps approach by doing NSX-T Network Infrastructure as code (IaC). It\u0026rsquo;s almost a bingo, I think I have most of the buzz words of these last years :)\nI will describe the structure of the project, the project components, the project workflow and finish with how to test this project.","tags":["Terrfaorm","Cloud","Github","Github Actions","NSX-T"],"title":"Github Actions with Terraform Cloud for CI/CD of NSX-T","type":"post"},{"authors":[],"categories":["Automation","IAC"],"content":"This post is to explain one of the pain point I have encountered while trying to do Github Actions with Terraform Cloud for CI/CD of NSX-T. The difficulty is to chain workflow/pipeline automatically. In my case, I wanted to launch a workflow base of a PR create by another workflow. When you use Github Actions to interface with Github, you need to authenticate your Github Actions script against Github. You can then use the GITHUB_TOKEN that has been made for this purpose. As this token is known from Github to be automation token, to avoid loops, you can use it to create a PR to trigger another workflow. The workaround to this [known limitation] is to create the PR with a personal access token.\nYou can find below an example of using the personal access token to create a PR. To create the Pull Request we are using github-script that will send a REST API query thanks to the github pre-authenticated octokit/rest.js client with pagination plugins.\n - name: CreatePR if apply succeed uses: actions/github-script@v4.0.2 if: steps.apply.outcome == 'success' with: github-token: ${{ secrets.PERSO_GITHUB_TOKEN }} script: | github.pulls.create({ owner: context.repo.owner, repo: context.repo.repo, title: \u0026quot;Auto PR\u0026quot;, head: \u0026quot;dev\u0026quot;, base: \u0026quot;main\u0026quot; });  ","date":1628780517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628780517,"objectID":"c4b088ffc875f125d2f4d494ab73dfb4","permalink":"https://netmemo.github.io/post/gha-auto-pr/","publishdate":"2021-08-12T17:01:57+02:00","relpermalink":"/post/gha-auto-pr/","section":"post","summary":"This post is to explain one of the pain point I have encountered while trying to do Github Actions with Terraform Cloud for CI/CD of NSX-T. The difficulty is to chain workflow/pipeline automatically. In my case, I wanted to launch a workflow base of a PR create by another workflow. When you use Github Actions to interface with Github, you need to authenticate your Github Actions script against Github. You can then use the GITHUB_TOKEN that has been made for this purpose.","tags":["Terrfaorm","Cloud","Github","Github Actions"],"title":"Triggering Github Actions workflow with automatic Pull Request","type":"post"},{"authors":[],"categories":["Automation","IAC"],"content":"This blog post is to explain how I did to automatically change Terraform Cloud workspace from Github Actions. As explained in the documentation remote workspace, you can use different remote workspace by specifying the prefix of you workspace in the Terraform backend configuration.\n backend \u0026quot;remote\u0026quot; { organization = \u0026quot;netmemo\u0026quot; workspaces { prefix = \u0026quot;netmemo-\u0026quot; } }  After that, you only need to select the proper workspace by entering the terraform workspace select [suffix] command. The issue comes if you want to do it in a fully automated environment like with Github Actions. You need an extra step which is to set the TF_WORKSPACE variable.\nBelow are the 2 steps that are needed to select a specific environment. In this blog the workspace will be netmemo-dev where netmemo- is the prefix configured in the main.tf and dev the suffix configured in the .yml file of the Github Actions script.\nThis step initializes Terraform and set the TF_WORKSPACE variable to indicate that we want to use the dev environment suffix before the initialization.\n - name: Terraform Init id: init run: terraform init env: TF_WORKSPACE: \u0026quot;dev\u0026quot;  If we are not setting the TF_WORKSPACE, the init command will try to get the default workspace that doesn\u0026rsquo;t exist and you will have the following error.\nThe currently selected workspace (default) does not exist. This is expected behavior when the selected workspace did not have an existing non-empty state. Please enter a number to select a workspace: 1. dev 2. prod  Even after setting the TF_WORKSPACE variable, we still need to enter the terraform workspace select command to provide the suffix of the workspace.\n - name: Terraform Workspace id: workspace run: terraform workspace select dev  If we forgot to enter the command to select workspace, the Terraform configuration section will try to load a workspace with only the prefix and trigger an error.\nError: error starting operation: The configured \u0026quot;remote\u0026quot; backend encountered an unexpected error: invalid value for workspace  You can find a full example of where we need to change workspace automatically in this blog post Github Actions with Terraform Cloud for CI/CD of NSX-T\n","date":1628774560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628774560,"objectID":"5b03adfbca58b79434d249ca21bfcd1f","permalink":"https://netmemo.github.io/post/tf-workspace-var/","publishdate":"2021-08-12T15:22:40+02:00","relpermalink":"/post/tf-workspace-var/","section":"post","summary":"This blog post is to explain how I did to automatically change Terraform Cloud workspace from Github Actions. As explained in the documentation remote workspace, you can use different remote workspace by specifying the prefix of you workspace in the Terraform backend configuration.\n backend \u0026quot;remote\u0026quot; { organization = \u0026quot;netmemo\u0026quot; workspaces { prefix = \u0026quot;netmemo-\u0026quot; } }  After that, you only need to select the proper workspace by entering the terraform workspace select [suffix] command.","tags":["Terrfaorm","Cloud","Github","Github Actions"],"title":"Changing Terraform Cloud workspace in Github Actions","type":"post"},{"authors":["Noel"],"categories":["NSXT","Terraform","IaC"],"content":" This article is to show an example of how to manage NSX-T firewall rules as a code through Terraform. You can find the project on my github account : nsxt-frac-tf-cm and nsxt-frac-tf-rm\nI will describe the structure of the project, how it works, the data model, the Terraform code explanation and finish with an example.\nStructure of the project The diagram below shows a summary of how I organized the project in order to fully use infrastructre as code.\n\nBelow is the file structure\n#Child modules ├── nsxt-frac-tf-cm ├── nsxt-tf-cm-dfw │ ├── main.tf │ ├── outputs.tf │ └── variables.tf ├── nsxt-tf-cm-grp │ ├── main.tf │ ├── outputs.tf │ └── variables.tf └── nsxt-tf-cm-svc ├── main.tf ├── outputs.tf └── variables.tf #Root modules ├── nsxt-frac-tf-rm ├── nsxt-tf-rm-dfw │ ├── main.tf │ ├── provider.tf │ ├── terraform.tfvars │ └── variables.tf └── nsxt-tf-rm-grpsvc ├── main.tf ├── outputs.tf ├── provider.tf ├── terraform.tfvars └── variables.tf  In short, child modules are the logic, root modules are the variables.\nYou can then duplicate the root module according to the NSX-T environment you want to deploy and start using the variables for the environment.\nHow it works Child Module You can find the logics and the code complexity in the Terraform child modules.\nThey will create the NSX-T resources. They allow to centralize the complexity. If you need to change your logic, you only need to modify the child module.\nAfter the modification, from the root module you only need to pull the new modifications by reinitializing or refreshing the modules with \u0026ldquo;terraform init\u0026rdquo; and then you can start using the new features.\nThere are tree child modules:\n nsxt-tf-cm-svc\nThis is for the NSX-T (TCP/UDP/IP) services creation\n nsxt-tf-cm-grp\nThis is for the NSX-T (TAG and IP) groups creation\n nsxt-tf-cm-dfw\nThis is for the NSX-T policy and rules creation\n  Root Module It will call the child module with the variables associated with the NSX-T environment. You need to create a repository per environment. The only modifications you need to do is to change the variables.\nThe root module can be declined to different NSX-T environments/deployments as long as the variable structure used in the root module respects the child module data model.\nRoot modules are separated in two :\nnsxt-tf-rm-grpsvc\nThis module will call the groups and the services child module an pass them the groups and services\u0026rsquo;s map variables. As the groups and services can be centralized and be the same for all environments, I have prefered to managed them separatly from the policies and rules.\nnsxt-tf-rm-dfw\nThis is the policies and rules definition. This root module will use the terraform state output section of the root module nsxt-tf-rm-grpsvc to get all the groups and services needed. The output.tf file is defined in nsxt-tf-rm-grpsvc root module.\nData model To have more details of the possible attributes used, you can refer to the NSX-T Terraform official documentation.\nServices map_svc\nThis is the map variable passed to the child module.\n This variable is a map of services map where the name is the service name Every service name contains lists. Every list name is the service type (IP, TCP or UDP). Every list is either a protocol number if the list is IP or port number if the list is TCP/UDP  map_svc = { NETMEMO-ESP = { IP = [\u0026quot;50\u0026quot;] } NETMEMO-NETBIOS = { TCP = [\u0026quot;135\u0026quot;,\u0026quot;137\u0026quot;,\u0026quot;139\u0026quot;,\u0026quot;139\u0026quot;], UDP = [\u0026quot;135\u0026quot;,\u0026quot;137\u0026quot;,\u0026quot;139\u0026quot;,\u0026quot;139\u0026quot;] } }  Groups map_grp\nThis is the map variable passed to the child module.\n This variable is a map of groups map where the name is the group name. Every group name contains lists. Every list name is the group type (IP or TAG). Every list is either subnet if the list is IP or tag name if the list is TAG  map_grp = { NETMEMO-LOCAL = { IP = [\u0026quot;10.0.0.0/25\u0026quot;,\u0026quot;10.1.1.0/25\u0026quot;] } NETMEMO-NAS = { TAG = [\u0026quot;NAS\u0026quot;,\u0026quot;FILES\u0026quot;] } NETMEMO-ESX = { TAG = [\u0026quot;ESX\u0026quot;,\u0026quot;VMWARE\u0026quot;] } }  Policies map_policies\nThis is the map variable passed to the child module.\n This variable is a map of policy map where the name is the policy name. Every policy name contains different attributes that are either string or map. The map named \u0026ldquo;rules\u0026rdquo; is to define the rules. Every \u0026ldquo;rules\u0026rdquo; map contains maps to define rules. Every map is a rule. Every rule map contains stings and list attrubutes (sources,destinations,services,scope)  map_policies = { NETMEMO-POL1 = { category = \u0026quot;Application\u0026quot; sequence_number = \u0026quot;10\u0026quot; rules = { netmemo-rule1 = { display = \u0026quot;NETMEMO-NAS-R\u0026quot; sources = [\u0026quot;NETMEMO-NAS\u0026quot;] destinations = [\u0026quot;NETMEMO-NAS\u0026quot;] services = [\u0026quot;HTTPS\u0026quot;] scope = [\u0026quot;NETMEMO-NAS\u0026quot;] action = \u0026quot;ALLOW\u0026quot; disabled = \u0026quot;false\u0026quot; } netmemo-rule2 = { display = \u0026quot;NETMEMO-ESX-R\u0026quot; sources = [\u0026quot;NETMEMO-ESX\u0026quot;] destinations = [\u0026quot;NETMEMO-ESX\u0026quot;] services = [\u0026quot;HTTPS\u0026quot;] scope = [\u0026quot;NETMEMO-ESX\u0026quot;] action = \u0026quot;ALLOW\u0026quot; disabled = \u0026quot;false\u0026quot; } } } NETMEMO-POL2 = { category = \u0026quot;Application\u0026quot; sequence_number = \u0026quot;20\u0026quot; rules = { netmemo-rule1 = { display = \u0026quot;NETMEMO-LOCAL-R\u0026quot; sources = [\u0026quot;NETMEMO-LOCAL\u0026quot;] destinations = [\u0026quot;NETMEMO-NAS\u0026quot;,\u0026quot;NETMEMO-ESX\u0026quot;] services = [\u0026quot;NETMEMO-NETBIOS\u0026quot;,\u0026quot;HTTPS\u0026quot;] scope = [\u0026quot;NETMEMO-NAS\u0026quot;,\u0026quot;NETMEMO-ESX\u0026quot;] action = \u0026quot;ALLOW\u0026quot; disabled = \u0026quot;false\u0026quot; } } } }  Code Explanation Root Module nsxt-tf-cm-grp and nsxt-tf-cm-svc child modules\nThese child modules use nested for_each with conditional nested dynamic.\nA brief example of the nested for_each used with dynamic can be found here. The conditional behavior is done thanks to the filter of the for loop. You can find the documentation on this link.\nDynamic terraform blocks allow to create a block for all the elements in the map you give to the for_each loop. In our child modules, the element of the dynamic for_each loop are also filtered with a for loop and a if.\n If the map contains a TAG attribute we create the criteria block with the TAG attributes.\ndynamic \u0026quot;criteria\u0026quot; { #The for_each contains a for loop with filter to create the criteriia only if there is a list with the name TAG for_each = { for key,val in each.value : key =\u0026gt; val if key == \u0026quot;TAG\u0026quot; } content { dynamic \u0026quot;condition\u0026quot; { #looping over the set to create every tags for_each = criteria.value content { key = \u0026quot;Tag\u0026quot; member_type = \u0026quot;VirtualMachine\u0026quot; operator = \u0026quot;EQUALS\u0026quot; value = condition.value } } } }  If the map contains a IP attribute we create the criteria block with the IP attributes.\ndynamic \u0026quot;criteria\u0026quot; { #The for_each contains a for loop with filter to create the criteriia only if there is a list with the name IP for_each = { for key,val in each.value : key =\u0026gt; val if key == \u0026quot;IP\u0026quot; } content { ipaddress_expression { ip_addresses = criteria.value } } }   The same logic is used to create services.\n If the map contains a TCP or UDP attribute we create the l4_port_set_entry block with the TCP/UDP attributes.\ndynamic \u0026quot;l4_port_set_entry\u0026quot; { #each.value = map of TCP,UDP or IP list where l4_port_set_entry.key will be TCP or UDP #the for_each contains a for loop with filter to create the l4_port_set_entry only if there is a list with TCP or UDP as name for_each = { for key,val in each.value : key =\u0026gt; val if key == \u0026quot;TCP\u0026quot; || key == \u0026quot;UDP\u0026quot; } content { display_name = \u0026quot;${l4_port_set_entry.key}_${each.key}\u0026quot; protocol = l4_port_set_entry.key destination_ports = l4_port_set_entry.value } }  If the map contains a IP attribute we create the ip_protocol_entry block with the IP attributes.\ndynamic \u0026quot;ip_protocol_entry\u0026quot; { #each.value = map of TCP,UDP or IP list #the for_each contains a for loop with filter to create the ip_protocol_entry only if there is a list with IP as name for_each = { for key,val in each.value : key =\u0026gt; val if key == \u0026quot;IP\u0026quot; } content { #[0] because the ip protocol will have a single IP protocol value in the set and the protocol attribut expect a number not a set protocol = ip_protocol_entry.value[0] } }   nsxt-tf-cm-dfw child modules\nThe complexity of this module is to get the NSX-T \u0026ldquo;Path\u0026rdquo; attributes of the groups and services with their name defined in their respective map variable. We want the map variables definition of the policies and rules to be as much friendly as possible and not dependant of values specific to the NSX-T implementation (path). This will also allow us to reuse the policies variables values in other environments if the rules are generic for instance.\nIn order to retreive the value we are looping over the \u0026ldquo;list\u0026rdquo; in the group or service attribute and \u0026ldquo;try\u0026rdquo; to get the path attribute.\nsource_groups = [for x in rule.value[\u0026quot;sources\u0026quot;] : try(var.nsxt_policy_grp_grp[x].path)]  Child Module nsxt-tf-rm-grpsvc\nThis is how to use a child module stored in git within a root module.\nmodule \u0026quot;nsxt-tf-cm-svc\u0026quot; { source = \u0026quot;git::https://github.com/netmemo/nsxt-frac-tf-cm.git//nsxt-tf-cm-svc\u0026quot; map_svc = var.map_svc }  Below is the code of the output.tf file that adds the groups and service maps into the terraform.state output section. It will allow other root modules to use these variables as terraform_remote_state datasource.\noutput \u0026quot;grp\u0026quot; { value = module.nsxt-tf-cm-grp.grp } output \u0026quot;svc\u0026quot; { value = module.nsxt-tf-cm-svc.svc }  nsxt-tf-rm-dfw\nBelow is the code to refer to a remote state, in this case the remote state is local.\ndata \u0026quot;terraform_remote_state\u0026quot; \u0026quot;grpsvc\u0026quot; { backend = \u0026quot;local\u0026quot; config = { path = \u0026quot;../nsxt-tf-rm-grpsvc/terraform.tfstate\u0026quot; } }  Demo Requirement:\n Terraform 0.14+\n NSX-T 3.0.2+\n Clone the root module nsxt-frac-tf-rm  To make a test, you need to follow the steps below (clic to see the detail):\n 1. Clone the root module repo git clone git@github.com:netmemo/nsxt-frac-tf-rm.git\nCloning into 'nsxt-frac-tf-rm'... remote: Enumerating objects: 21, done. remote: Counting objects: 100% (21/21), done. remote: Compressing objects: 100% (15/15), done. remote: Total 21 (delta 7), reused 20 (delta 6), pack-reused 0 Receiving objects: 100% (21/21), done. Resolving deltas: 100% (7/7), done.    2.Move to the cloned repo\ncd your directory    3.Initialize terraform terraform init\nInitializing modules... Downloading git::https://github.com/netmemo/nsxt-frac-tf-cm.git for nsxt-tf-cm-dfw... - nsxt-tf-cm-dfw in .terraform/modules/nsxt-tf-cm-dfw/nsxt-tf-cm-dfw Initializing the backend... Initializing provider plugins... - terraform.io/builtin/terraform is built in to Terraform - Finding vmware/nsxt versions matching \u0026quot;\u0026gt;= 3.1.1\u0026quot;... - Installing vmware/nsxt v3.2.2... - Installed vmware/nsxt v3.2.2 (signed by a HashiCorp partner, key ID 6B6B0F38607A2264) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \u0026quot;terraform init\u0026quot; in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026quot;terraform plan\u0026quot; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.    4.Edit the provider.tf file Change the host, the username and password according to your environment. You need to either add the variable in the terraform.tfvars file or enter them when the prompt will ask you.\nprovider \u0026quot;nsxt\u0026quot; { host = var.host username = \u0026quot;admin\u0026quot; password = var.password }    5.Create the group and services\nterraform plan -out plan.out\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-ESX\u0026quot;] will be created + resource \u0026quot;nsxt_policy_group\u0026quot; \u0026quot;grp\u0026quot; { + display_name = \u0026quot;NETMEMO-ESX\u0026quot; + domain = \u0026quot;default\u0026quot; + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + criteria { + condition { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;ESX\u0026quot; } + condition { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;VMWARE\u0026quot; } } } # module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-LOCAL\u0026quot;] will be created + resource \u0026quot;nsxt_policy_group\u0026quot; \u0026quot;grp\u0026quot; { + display_name = \u0026quot;NETMEMO-LOCAL\u0026quot; + domain = \u0026quot;default\u0026quot; + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + criteria { + ipaddress_expression { + ip_addresses = [ + \u0026quot;10.0.0.0/25\u0026quot;, + \u0026quot;10.1.1.0/25\u0026quot;, ] } } } # module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-NAS\u0026quot;] will be created + resource \u0026quot;nsxt_policy_group\u0026quot; \u0026quot;grp\u0026quot; { + display_name = \u0026quot;NETMEMO-NAS\u0026quot; + domain = \u0026quot;default\u0026quot; + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + criteria { + condition { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;NAS\u0026quot; } + condition { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;FILES\u0026quot; } } } # module.nsxt-tf-cm-svc.nsxt_policy_service.svc[\u0026quot;NETMEMO-ESP\u0026quot;] will be created + resource \u0026quot;nsxt_policy_service\u0026quot; \u0026quot;svc\u0026quot; { + display_name = \u0026quot;NETMEMO-ESP\u0026quot; + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + ip_protocol_entry { + protocol = 50 } } # module.nsxt-tf-cm-svc.nsxt_policy_service.svc[\u0026quot;NETMEMO-NETBIOS\u0026quot;] will be created + resource \u0026quot;nsxt_policy_service\u0026quot; \u0026quot;svc\u0026quot; { + display_name = \u0026quot;NETMEMO-NETBIOS\u0026quot; + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + l4_port_set_entry { + destination_ports = [ + \u0026quot;135\u0026quot;, + \u0026quot;137\u0026quot;, + \u0026quot;139\u0026quot;, ] + display_name = \u0026quot;TCP_NETMEMO-NETBIOS\u0026quot; + protocol = \u0026quot;TCP\u0026quot; + source_ports = [] } + l4_port_set_entry { + destination_ports = [ + \u0026quot;135\u0026quot;, + \u0026quot;137\u0026quot;, + \u0026quot;139\u0026quot;, ] + display_name = \u0026quot;UDP_NETMEMO-NETBIOS\u0026quot; + protocol = \u0026quot;UDP\u0026quot; + source_ports = [] } } Plan: 5 to add, 0 to change, 0 to destroy. Changes to Outputs: + grp = { + NETMEMO-ESX = { + conjunction = [] + criteria = [ + { + condition = [ + { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;ESX\u0026quot; }, + { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;VMWARE\u0026quot; }, ] + ipaddress_expression = [] + macaddress_expression = [] + path_expression = [] }, ] + description = null + display_name = \u0026quot;NETMEMO-ESX\u0026quot; + domain = \u0026quot;default\u0026quot; + extended_criteria = [] + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + tag = [] } + NETMEMO-LOCAL = { + conjunction = [] + criteria = [ + { + condition = [] + ipaddress_expression = [ + { + ip_addresses = [ + \u0026quot;10.0.0.0/25\u0026quot;, + \u0026quot;10.1.1.0/25\u0026quot;, ] }, ] + macaddress_expression = [] + path_expression = [] }, ] + description = null + display_name = \u0026quot;NETMEMO-LOCAL\u0026quot; + domain = \u0026quot;default\u0026quot; + extended_criteria = [] + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + tag = [] } + NETMEMO-NAS = { + conjunction = [] + criteria = [ + { + condition = [ + { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;NAS\u0026quot; }, + { + key = \u0026quot;Tag\u0026quot; + member_type = \u0026quot;VirtualMachine\u0026quot; + operator = \u0026quot;EQUALS\u0026quot; + value = \u0026quot;FILES\u0026quot; }, ] + ipaddress_expression = [] + macaddress_expression = [] + path_expression = [] }, ] + description = null + display_name = \u0026quot;NETMEMO-NAS\u0026quot; + domain = \u0026quot;default\u0026quot; + extended_criteria = [] + id = (known after apply) + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + tag = [] } } + svc = { + NETMEMO-ESP = { + algorithm_entry = [] + description = null + display_name = \u0026quot;NETMEMO-ESP\u0026quot; + ether_type_entry = [] + icmp_entry = [] + id = (known after apply) + igmp_entry = [] + ip_protocol_entry = [ + { + description = \u0026quot;\u0026quot; + display_name = \u0026quot;\u0026quot; + protocol = 50 }, ] + l4_port_set_entry = [] + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + tag = [] } + NETMEMO-NETBIOS = { + algorithm_entry = [] + description = null + display_name = \u0026quot;NETMEMO-NETBIOS\u0026quot; + ether_type_entry = [] + icmp_entry = [] + id = (known after apply) + igmp_entry = [] + ip_protocol_entry = [] + l4_port_set_entry = [ + { + description = \u0026quot;\u0026quot; + destination_ports = [ + \u0026quot;135\u0026quot;, + \u0026quot;137\u0026quot;, + \u0026quot;139\u0026quot;, ] + display_name = \u0026quot;TCP_NETMEMO-NETBIOS\u0026quot; + protocol = \u0026quot;TCP\u0026quot; + source_ports = [] }, + { + description = \u0026quot;\u0026quot; + destination_ports = [ + \u0026quot;135\u0026quot;, + \u0026quot;137\u0026quot;, + \u0026quot;139\u0026quot;, ] + display_name = \u0026quot;UDP_NETMEMO-NETBIOS\u0026quot; + protocol = \u0026quot;UDP\u0026quot; + source_ports = [] }, ] + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + tag = [] } } ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Saved the plan to: plan.out  terraform apply plan.out\nmodule.nsxt-tf-cm-svc.nsxt_policy_service.svc[\u0026quot;NETMEMO-NETBIOS\u0026quot;]: Creating... module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-NAS\u0026quot;]: Creating... module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-LOCAL\u0026quot;]: Creating... module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-ESX\u0026quot;]: Creating... module.nsxt-tf-cm-svc.nsxt_policy_service.svc[\u0026quot;NETMEMO-ESP\u0026quot;]: Creating... module.nsxt-tf-cm-svc.nsxt_policy_service.svc[\u0026quot;NETMEMO-ESP\u0026quot;]: Creation complete after 0s [id=434fdc00-30e6-4072-a64b-a7e9534c80c2] module.nsxt-tf-cm-svc.nsxt_policy_service.svc[\u0026quot;NETMEMO-NETBIOS\u0026quot;]: Creation complete after 0s [id=59a29e53-9901-44d2-9589-f770c36d87bb] module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-ESX\u0026quot;]: Creation complete after 0s [id=0fb48626-d903-42cc-9513-b47e7626f44d] module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-LOCAL\u0026quot;]: Creation complete after 0s [id=21d93fff-c08c-42dd-90da-d099dfa51163] module.nsxt-tf-cm-grp.nsxt_policy_group.grp[\u0026quot;NETMEMO-NAS\u0026quot;]: Creation complete after 0s [id=bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb] Apply complete! Resources: 5 added, 0 changed, 0 destroyed. ...    6.Create the policies and rules\nterraform plan -out plan.out\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[\u0026quot;NETMEMO-POL1\u0026quot;] will be created + resource \u0026quot;nsxt_policy_security_policy\u0026quot; \u0026quot;policies\u0026quot; { + category = \u0026quot;Application\u0026quot; + display_name = \u0026quot;NETMEMO-POL1\u0026quot; + domain = \u0026quot;default\u0026quot; + id = (known after apply) + locked = false + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + sequence_number = 10 + stateful = true + tcp_strict = (known after apply) + rule { + action = \u0026quot;ALLOW\u0026quot; + destination_groups = [ + \u0026quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb\u0026quot;, ] + destinations_excluded = false + direction = \u0026quot;IN_OUT\u0026quot; + disabled = false + display_name = \u0026quot;NETMEMO-NAS-R\u0026quot; + ip_version = \u0026quot;IPV4_IPV6\u0026quot; + logged = false + nsx_id = (known after apply) + revision = (known after apply) + rule_id = (known after apply) + scope = [ + \u0026quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb\u0026quot;, ] + sequence_number = (known after apply) + services = [ + \u0026quot;/infra/services/HTTPS\u0026quot;, ] + source_groups = [ + \u0026quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb\u0026quot;, ] + sources_excluded = false } + rule { + action = \u0026quot;ALLOW\u0026quot; + destination_groups = [ + \u0026quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d\u0026quot;, ] + destinations_excluded = false + direction = \u0026quot;IN_OUT\u0026quot; + disabled = false + display_name = \u0026quot;NETMEMO-ESX-R\u0026quot; + ip_version = \u0026quot;IPV4_IPV6\u0026quot; + logged = false + nsx_id = (known after apply) + revision = (known after apply) + rule_id = (known after apply) + scope = [ + \u0026quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d\u0026quot;, ] + sequence_number = (known after apply) + services = [ + \u0026quot;/infra/services/HTTPS\u0026quot;, ] + source_groups = [ + \u0026quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d\u0026quot;, ] + sources_excluded = false } } # module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[\u0026quot;NETMEMO-POL2\u0026quot;] will be created + resource \u0026quot;nsxt_policy_security_policy\u0026quot; \u0026quot;policies\u0026quot; { + category = \u0026quot;Application\u0026quot; + display_name = \u0026quot;NETMEMO-POL2\u0026quot; + domain = \u0026quot;default\u0026quot; + id = (known after apply) + locked = false + nsx_id = (known after apply) + path = (known after apply) + revision = (known after apply) + sequence_number = 20 + stateful = true + tcp_strict = (known after apply) + rule { + action = \u0026quot;ALLOW\u0026quot; + destination_groups = [ + \u0026quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d\u0026quot;, + \u0026quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb\u0026quot;, ] + destinations_excluded = false + direction = \u0026quot;IN_OUT\u0026quot; + disabled = false + display_name = \u0026quot;NETMEMO-LOCAL-R\u0026quot; + ip_version = \u0026quot;IPV4_IPV6\u0026quot; + logged = false + nsx_id = (known after apply) + revision = (known after apply) + rule_id = (known after apply) + scope = [ + \u0026quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d\u0026quot;, + \u0026quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb\u0026quot;, ] + sequence_number = (known after apply) + services = [ + \u0026quot;/infra/services/59a29e53-9901-44d2-9589-f770c36d87bb\u0026quot;, + \u0026quot;/infra/services/HTTPS\u0026quot;, ] + source_groups = [ + \u0026quot;/infra/domains/default/groups/21d93fff-c08c-42dd-90da-d099dfa51163\u0026quot;, ] + sources_excluded = false } } Plan: 2 to add, 0 to change, 0 to destroy.  terraform apply plan.out\nmodule.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[\u0026quot;NETMEMO-POL1\u0026quot;]: Creating... module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[\u0026quot;NETMEMO-POL2\u0026quot;]: Creating... module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[\u0026quot;NETMEMO-POL2\u0026quot;]: Creation complete after 1s [id=bab4c14e-30c6-4633-b9d7-db760844e0e7] module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[\u0026quot;NETMEMO-POL1\u0026quot;]: Creation complete after 1s [id=867b5893-15e3-4dab-a2f0-c8084af790a7] Apply complete! Resources: 2 added, 0 changed, 0 destroyed.    7.Check the result through the UI \n\n\n\n","date":1627578217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627578217,"objectID":"1041b1dd099f21abf99e1952f31ddc64","permalink":"https://netmemo.github.io/post/nsxt-tf-firewall/","publishdate":"2021-07-29T19:03:37+02:00","relpermalink":"/post/nsxt-tf-firewall/","section":"post","summary":"This article is to show an example of how to manage NSX-T firewall rules as a code through Terraform. You can find the project on my github account : nsxt-frac-tf-cm and nsxt-frac-tf-rm\nI will describe the structure of the project, how it works, the data model, the Terraform code explanation and finish with an example.\nStructure of the project The diagram below shows a summary of how I organized the project in order to fully use infrastructre as code.","tags":["NSXT","Firewall","Terraform","IaC"],"title":"NSX-T Firewall rules as code with Terraform","type":"post"},{"authors":["Noel"],"categories":[],"content":" Along my different jobs, one of the things that led to L2 extension and then brought risks and slew down or even blocked projects was big heterogeneous subnets. In this blog I will list the pros and cons of small, big and heterogeneous subnets.\nSmall subnet I like to think of using small subnets as a \u0026ldquo;scale out\u0026rdquo; approach. When the subnet is full you just assign a new subnet in the same reserved space.\nPros  Agility. You can do swimlane application design. When migration time is coming, you can do it per application. For application migration, if you can\u0026rsquo;t change the ip addresses of the servers, you can reroute the trafic without jeorpadizing the entire company with L2 extension.  Cons  More entries in the routing table but that can be mitigated with summarization.\n Loss of ip addresses. Does it really matter ? how full are the big subnets ?  Big subnet This is for me a more \u0026ldquo;scale up\u0026rdquo; approach. You oversize subnets then when you need a new server, you pick one IP in the big subnet without asking any question.\nPros  IP address saving. Is it worth it ? Most of the time the subnets are big only to anticipate growth and those IPs are lost anyway. Simplicity to allocate addresses. Only a couple of big subnets.\n  Cons  If the subnet is not full, it\u0026rsquo;s very difficult to carve out the unused space in the subnet to use it somewhere else. L2 broadcast domain is mitigated by arp suppression on modern fabric but still you have broadcast reaching every VM and thus can have troubles if the stormcontrol is not set properly.\n For all the firewalls in the enterprise that are not relying on tags, you might have to open firewall rules per host instead of per subnets.\n  Heterogeneous subnet It\u0026rsquo;s not rare to see big subnets also heterogeneous with a mix of applications and workload type (physical/virtual).\nPros  It allows physical devices like NAS or Loadbalancer to be in the same subnet as your application workloads (If you want to avoid putting a FW in between and can\u0026rsquo;t use VRF mechanism)  Cons  Application mutualization. It can block applications migration because not all the applications have the same requirements. If one of the application has been asked to migrate somewhere else and nobody wants to change IP addresses, you might be forced to stretch L2.\n Mix of workload type (physical/virtual). It can block applications migration because the physical devices can\u0026rsquo;t go where the VMs are going. If nobody wants to change IP addresses, you might be forced to stretch L2.\n  Other considerations  Pets vs cattle : VM vs Container\nIn the above use cases I\u0026rsquo;m talking essentially of workloads that are not containers. For containers the paradigm is often different because most of the time containers don\u0026rsquo;t share the subnet with physical device and application developper doesn\u0026rsquo;t rely on the containers IP addresses. What matter is the FQDN of the service. VM are still considered most of the time as pets and nobody wants to change the IP address mostly to avoid to change firewall rules or hard coded IP addresses in application.\n VLANs Scale\nThe number of supported VLAN are less relevent in virtualized environments because most of the time software scale better and you are not limited to 4K vlans (or even 2K in some case for ACI Bridge Domain and EVPN Layer 2 VNIs).\n Suboptimal IP addressing plan\nWhen you move an entire subnet somewhere else you can break the summarization of the company but you can always do the summarization again after all the applications have moved or even reIP the entire subnet afterwards. It\u0026rsquo;s less risky and complex than having a L2 streched \u0026ldquo;forever\u0026rdquo;.\n Distributed routing\nNow, with distributed routing you don\u0026rsquo;t have to go to the network core where the default gateway was in the old days to route between subnets. The trafic have a better distribution in the fabric and you should have less risks to congest uplinks with EAST/WEST trafic.\n  ","date":1621369719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630936020,"objectID":"1cd454f462d6d851a191d00fb920c979","permalink":"https://netmemo.github.io/post/subnets-size-mix/","publishdate":"2021-05-18T22:28:39+02:00","relpermalink":"/post/subnets-size-mix/","section":"post","summary":"Along my different jobs, one of the things that led to L2 extension and then brought risks and slew down or even blocked projects was big heterogeneous subnets. In this blog I will list the pros and cons of small, big and heterogeneous subnets.\nSmall subnet I like to think of using small subnets as a \u0026ldquo;scale out\u0026rdquo; approach. When the subnet is full you just assign a new subnet in the same reserved space.","tags":["Addressing","IP Plan","Design"],"title":"Subnet sizing and heterogeneous subnets","type":"post"},{"authors":["Noel"],"categories":["Automation"],"content":"https://www.terraform.io/docs/cli/commands/state/mv.html\nOn windows :\nterraform state mv nsxt_policy_security_policy.policy1 nsxt_policy_security_policy.policies[\\\u0026ldquo;policy1\\\u0026ldquo;]  It move resources from a construct like this\nlocals { policy1= { rule1 = { source = [\u0026quot;src1\u0026quot;,\u0026quot;src2\u0026quot;] } } policy2 = { rule1 = { source = [\u0026quot;src3\u0026quot;,\u0026quot;src4\u0026quot;] } } }  To a structure like this\nlocals { policies = { policy1 = { rule1 = { source = [\u0026quot;src1\u0026quot;,\u0026quot;src2\u0026quot;] } } policy2 = { rule2 = { source = [\u0026quot;src3\u0026quot;,\u0026quot;src4\u0026quot;] } } } }  The main moving from\nresource \u0026quot;nsxt_policy_security_policy\u0026quot; \u0026quot;policy1\u0026quot;{ display_name = \u0026quot;policy1\u0026quot; category = \u0026quot;Environment\u0026quot; dynamic \u0026quot;rule\u0026quot; { for_each = local.policy1 content { source_groups = rule.value[\u0026quot;sources\u0026quot;] } } } resource \u0026quot;nsxt_policy_security_policy\u0026quot; \u0026quot;policy2\u0026quot;{ display_name = \u0026quot;policy2\u0026quot; category = \u0026quot;Environment\u0026quot; dynamic \u0026quot;rule\u0026quot; { for_each = local.policy2 content { source_groups = rule.value[\u0026quot;sources\u0026quot;] } } }  to\nresource \u0026quot;nsxt_policy_security_policy\u0026quot; \u0026quot;policies\u0026quot; { for_each = local.policies display_name = each.key category = \u0026quot;Environment\u0026quot; dynamic \u0026quot;rule\u0026quot; { for_each = each.value content { source_groups = rule.value[\u0026quot;sources\u0026quot;] } } }  The terraform state moving fromfrom 2 resources to 1 resource with 2 instances\nFrom\n{ \u0026quot;type\u0026quot;: \u0026quot;nsxt_policy_security_policy\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;policy1\u0026quot; \u0026quot;instances\u0026quot; : [ { ... } ] }, { \u0026quot;type\u0026quot;: \u0026quot;nsxt_policy_security_policy\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;policy2\u0026quot; \u0026quot;instances\u0026quot; : [ { ... } ] }  To\n{ \u0026quot;type\u0026quot;: \u0026quot;nsxt_policy_security_policy\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;policies\u0026quot; \u0026quot;instances\u0026quot; : [ { \u0026quot;index_key\u0026quot;: \u0026quot;policy1\u0026quot; }, { \u0026quot;index_key\u0026quot;: \u0026quot;policy2\u0026quot; } ] }  ","date":1614286617,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614286617,"objectID":"e623927fcd938304d743d2b577f90d7b","permalink":"https://netmemo.github.io/post/terraform-refactoring-state-file/","publishdate":"2021-02-25T21:56:57+01:00","relpermalink":"/post/terraform-refactoring-state-file/","section":"post","summary":"https://www.terraform.io/docs/cli/commands/state/mv.html\nOn windows :\nterraform state mv nsxt_policy_security_policy.policy1 nsxt_policy_security_policy.policies[\\\u0026ldquo;policy1\\\u0026ldquo;]  It move resources from a construct like this\nlocals { policy1= { rule1 = { source = [\u0026quot;src1\u0026quot;,\u0026quot;src2\u0026quot;] } } policy2 = { rule1 = { source = [\u0026quot;src3\u0026quot;,\u0026quot;src4\u0026quot;] } } }  To a structure like this\nlocals { policies = { policy1 = { rule1 = { source = [\u0026quot;src1\u0026quot;,\u0026quot;src2\u0026quot;] } } policy2 = { rule2 = { source = [\u0026quot;src3\u0026quot;,\u0026quot;src4\u0026quot;] } } } }  The main moving from","tags":["Terraform","NSXT"],"title":"Terraform Refactoring State File","type":"post"},{"authors":["Noel"],"categories":["automation","developpment"],"content":"To install python packages offline (with no internet access), the simplest way is to dowload the packages with the dependencies on a server with internet access and the below command.\n C:\\Users\\Nono\\Desktop\\pythonpip download requests Collecting requests Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB) Collecting urllib3=1.21.1 Using cached urllib3-1.26.3-py2.py3-none-any.whl (137 kB) Collecting certifi=2017.4.17 Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB) Collecting idna=2.5 Using cached idna-2.10-py2.py3-none-any.whl (58 kB) Collecting chardet=3.0.2 Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB) Saved c:\\users\\nono\\desktop\\python\\requests-2.25.1-py2.py3-none-any.whl Saved c:\\users\\nono\\desktop\\python\\certifi-2020.12.5-py2.py3-none-any.whl Saved c:\\users\\nono\\desktop\\python\\chardet-4.0.0-py2.py3-none-any.whl Saved c:\\users\\nono\\desktop\\python\\idna-2.10-py2.py3-none-any.whl Saved c:\\users\\nono\\desktop\\python\\urllib3-1.26.3-py2.py3-none-any.whl Successfully downloaded requests certifi chardet idna urllib3  You then need to move all the files to the offline server in a directory and deploy the packages with the below command :\n pip install --no-index --find-links=file:C:\\Users\\Nono2\\tools\\python-modules\\resquests requests Looking in links: file:///C:\\Users\\Nono2\\tools\\python-modules\\resquests Collecting requestsCollecting urllib3=1.21.1 (from requests) Collecting chardet=3.0.2 (from requests) Collecting idna=2.5 (from requests)Collecting certifi=2017.4.17 (from requests) Installing collected package: urllib3, chardet, idna, certifi, requests Successfully installed certifi-2020.12.5 chardet-4.0.0 idna-2.10 requests-2.25.1 urllib3-1.26.3  ","date":1614285974,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614285974,"objectID":"4c660db6d9b4ee7ceefaf6f85bae12f0","permalink":"https://netmemo.github.io/post/python-package-offline/","publishdate":"2021-02-25T21:46:14+01:00","relpermalink":"/post/python-package-offline/","section":"post","summary":"To install python packages offline (with no internet access), the simplest way is to dowload the packages with the dependencies on a server with internet access and the below command.\n C:\\Users\\Nono\\Desktop\\pythonpip download requests Collecting requests Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB) Collecting urllib3=1.21.1 Using cached urllib3-1.26.3-py2.py3-none-any.whl (137 kB) Collecting certifi=2017.4.17 Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB) Collecting idna=2.5 Using cached idna-2.10-py2.py3-none-any.whl (58 kB) Collecting chardet=3.0.2 Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB) Saved c:\\users\\nono\\desktop\\python\\requests-2.","tags":["python"],"title":"Python Package Offline","type":"post"},{"authors":["Noel"],"categories":["Cisco","QOS"],"content":" QoS VOQ On N5K, In the case of unicast traffic, VOQ is an ingress buffer pool for 3 ingress ports (1 ASIC = 3 ports). This buffer pool is split into n x reservable buffer of the size configured in the voq-limit command. If the ingress buffer is 16000 and the VOQ limit is 1024, that mean 16 flow can reserv buffers. When the shared buffer is exausted, the dedicated ingress buffer per port is used then when it\u0026rsquo;s full, the packet is droped.\nVOQ is reservable per egress port per class (pair).\nWith small VOQ limit, there is lot of buffer available for non congested flow.\nWith no VOQ limit, a congested port can use up to 50% of the total shared memory and those, 2 congested port can exaust all the ingress resources of an ASIC (3 ingress port) and drop can happen on this ASIC\nhttps://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2015/pdf/BRKDCT-3100.pdf\nhttps://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2017/pdf/BRKDCN-3346.pdf\nhttps://www.cisco.com/c/en/us/support/docs/switches/nexus-6000-series-switches/200401-Nexus-5600-6000-Understanding-and-Troub.html\nIn case of a Nexus 5600. 1 ASIC = 3 * 40Gb or 12*10G ports. Cells are the units in which buffers are allocated. One cell is 320 Bytes. ASIC Ingress buffer size : 48840 of available total cells (16M), shared among all 3x40Gb ports. Egress buffer is 9Mb (dedicated buffer per ASIC).\nVOQ = Ingress per output port/class queues. E.g with 114 ports on the switch, with 8 queues per port there would be 1152 VOQs. On N5600 Buffer can be shared accrod port and classes, giving more burst absorption capacity.\nDefault buffer allocation  minimum fixed buffer of 312 cells (100KB) is reserved per class (up to 8 classes) per ingress port, rest of the buffer is shared. This is done to guarantee minimum performance.\n  If there is only 1 class (the class-default for instance), there is only a single fixed/dedicated buffer per port.\n- 44,331 cells of shared buffer available for data traffic for all ports. shared buffer is used first.\n- any drop class can access half of the shared buffer- no-drop class (eg fcoe) can access complete shared buffer.\n- \u0026ldquo;queue-limit\u0026rdquo; under \u0026ldquo;network-qos\u0026rdquo; policy specifies the dedicated buffer for each port and each class. The dedicated buffer can be used by the port for only that class of service.\nvoq-limit Command \u0026ldquo;hardware unicast voq-limit [threshold \u0026hellip;]\u0026rdquo; enabling voq-limit turns on a shared buffer threshold per each voq.\nlimits the amount of buffers usable on the ingress interface, for packets headed towards a specific VoQ (\u0026ldquo;egress port, class\u0026rdquo; pair). Drops happens per VOQ, when its packet in ingress buffer exceed threshold: when the traffic ingress on a port and it consumes all 1024 cells, it will get dropped as discards.\nwhen the second flow traffic comes in, it will tage another 1024 cells as well but in a different VOQ and will not get dropped; this way voq thresholding prevents the non-congested egress port traffic drop.\nHOLD mitigation and VOQ thresholding Below is discussed for scenario without voq-limit enabledCongestion on one egress port in one CoS eventually bleeds into the congestion of its corresponding VOQ on the ingress port. Once the limit is reached then traffic gets dropped.\nOn N5600 ASIC Buffers are allocated per ingress port and are shared by all the egress ports that are seeing traffic from this ingressport.\nA stuck or slow-draining egress port can causse all buffers on one or more ingress ports that are senfing traffic to the egress port to be exhausted, thereby affecting all traffic on these ingress port. This is Head of Line Blocking (HOLB) problem.\nTo avoid this scenario, the VOQ for unicast traffic may be configured with a voq-limit threshold, at which point the port will stop accepting any more packets for congested destination (drops the packets or pauses the affected class for non-drop class type). When the queue length decrease and goes below another threshold, the VOQ starts accepting packets again.\nBy default VOQ Thresholding is disabled for all classes.\nQuestions  About the limit of 8000 cells, why not setting directly 16000 or removeing the limitation ?\n  Setting larger voq-limit increases the change to improve burst absorption but also leaves non-congested VOQs to be more likely affected by congested VOQs (as the latter can dip more into shared buffer).Disadvantage of having a voq-limit is that when we have a burst traffic comming in (like for distributed storage/VSAN), it connot use more than configured threshold of allocated cells and the bursty flow will have drops even though there is un-used ingress buffer.It\u0026rsquo;s recommanded to remove voq-limit in case of bursty traffic.\n Are the VoQ per UPC or per ingress interface ?\n  VOQs are per class per egress interface. E.g. With 114 ports on the switch with 8 queues there would be 1152 VOQs per port.If there is only 1 class (class-defauklt), the number of VOQ is matching the number of egress ports.\n Our undestanding is that if we remove the command voq limit, we might have HOLB while with the command it\u0026rsquo;s not possible. Could you explain how is this possible to have HOLD if VoQ is always used ?\n  This can happen because VOQ Thresholding (voq-limit) is not enabled by default. Therefore, each VOQ can borrow from shared buffer and some of non-congested VOQs would not be able to handle traffic due to lack of buffer space, even though they are not congested.\n Effect of changing or removing VOQ-limit on the traffic.\nThere would be subsecond traffic interruption on all ports.\n Effect of changing or removing VOQ-Limit on the FEX.\nThere should not be any effect on the FEX operation\n Could you describe the difference in the behavior of buffers (shared, ingress dedicated per port, voq per cos, drop) with the command voq limit default, with the command voq limit configured with the max value, and without the voq limit command ?\n  A. Without voq-limitShared - used by all ingress ports by default. 3*40G (or) 12*10G ports compete for usage, when per-port buffers are exhausted.Dedicated - samll, reserved per port per class. Can be adjusted with queue-limit command.VOQ drop thresholds are disabled.Shared is used first, only then overflow to dedicated.One drop class per ASIC can take up to half if the shared buffer.If congestion is constant, it can result in blocking for other VOQs in the same ingress port.Slow draining port can affect others by consuming shared and dedicated buffers.ExempleUnicast traffic coming on a 40G ingress port and egress port are 2x10G. When one of the egress port 10G is congested, it moght be possible that a new ingress flow from this 40G interface to another 10G interface can get affected as well because of non-availability of the ingress buffer.\nWhen the traffic ingress on a port it first fills the shared buffer. Each flow can take up ti 50% of the total shared buffer available (22200 cells or ~7.1MB). After filling 50% of the buffer the same flow will utilize the per-port fixed buffer.When the new flow comes ingress on that port, it tries to fill in the 50% of shared buffer but if shared and per-port buffer are already filled, this traffic would be dropped.We can avoir the new flow, that is goignt o different no-congested port, from getting dropped by enabling VOQ thresholding.\nB. With voq-limit default value 1024Drop happend per VOQ, each limited by 1024 cells.Congestion in one VOQ has minimal chances to affect other VOQsVOQ drop thresholds are minimal, so burst traffic flow coming in it cannot use more than 1024 allocated cells and will have drops even though there is un-used buffer.when the traffic ingress on a port consumes all 1024 cells, it will get dropped as discards.When the second flow traffic with destination to another egress port comes in, it may consume 1024 cells as well but a different VOQ- so will not get dropped.\nC. with voq-limit default value 16384drop happens per VOQ, each limited by 16384 cells.congestion in one VOQ does not affect other VOQs if there is enough shared buffer left.VOQ drop thresholds are at maximum.Burst traffic flow coming can use up to 16384 allocated cells and will have drop everything above it. Other VOQs buffering at the same instance can use remaining buffer (but nor more than 16384each). In theory, 3 VOQs, that are fully congested at the same time, could take all the buffer. It is very difficult to prefict instant buffer usage due to unpredictable nature of bursty flows, so exact values should be taken from production, e.g. Try with 16384 and reduce the threshold if negative impact is seen on non-congested flows.\nCouple of solutions to resolve bursty trafic drop:  Remove VOQ limit completel (heavy burst trafic)\n Increase VOQ Threshold to 8000 or 16384 (max) and monitor the situation with discards=\u0026gt; show hardware profile buffer monitor interface ethernet\n Spread congested links betweek different ASICs.\n Implement policing of the traffic.\n  ","date":1614115484,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614115484,"objectID":"2c58fdeaa94f05a423d19179d5faeb30","permalink":"https://netmemo.github.io/post/n5600-buffering/","publishdate":"2021-02-23T22:24:44+01:00","relpermalink":"/post/n5600-buffering/","section":"post","summary":"QoS VOQ On N5K, In the case of unicast traffic, VOQ is an ingress buffer pool for 3 ingress ports (1 ASIC = 3 ports). This buffer pool is split into n x reservable buffer of the size configured in the voq-limit command. If the ingress buffer is 16000 and the VOQ limit is 1024, that mean 16 flow can reserv buffers. When the shared buffer is exausted, the dedicated ingress buffer per port is used then when it\u0026rsquo;s full, the packet is droped.","tags":["Nexus","QOS","Buffering","Buffer","Cisco","N5600"],"title":"N5600 Buffering","type":"post"},{"authors":["Noel"],"categories":["Terraform"],"content":"The post below shows how to create security policy groups for NSX-T with Terraform nested for_each loop and dynamic.\nThe variables are made from one map of list. Each list represents one group composed of tags.\nhttps://www.hashicorp.com/blog/hashicorp-terraform-0-12-preview-for-and-for-each\nvariable \u0026quot;mapgroups\u0026quot; { type = map default = { NBO = [\u0026quot;NBO\u0026quot;] NBO-PROD = [\u0026quot;NBO\u0026quot;,\u0026quot;PROD\u0026quot;] } } resource \u0026quot;nsxt_policy_group\u0026quot; \u0026quot;nbogroups\u0026quot; { for_each = var.mapgroups display_name = each.key criteria { dynamic \u0026quot;condition\u0026quot; { for_each = each.value content { key = \u0026quot;Tag\u0026quot; member_type = \u0026quot;VirtualMachine\u0026quot; operator = \u0026quot;EQUALS\u0026quot; value = condition.value } } } }  ","date":1613768810,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613768810,"objectID":"eea047d3620e58b7dcead6c5237d9a0f","permalink":"https://netmemo.github.io/post/tf-nsxt-nested-for-each/","publishdate":"2021-02-19T22:06:50+01:00","relpermalink":"/post/tf-nsxt-nested-for-each/","section":"post","summary":"The post below shows how to create security policy groups for NSX-T with Terraform nested for_each loop and dynamic.\nThe variables are made from one map of list. Each list represents one group composed of tags.\nhttps://www.hashicorp.com/blog/hashicorp-terraform-0-12-preview-for-and-for-each\nvariable \u0026quot;mapgroups\u0026quot; { type = map default = { NBO = [\u0026quot;NBO\u0026quot;] NBO-PROD = [\u0026quot;NBO\u0026quot;,\u0026quot;PROD\u0026quot;] } } resource \u0026quot;nsxt_policy_group\u0026quot; \u0026quot;nbogroups\u0026quot; { for_each = var.mapgroups display_name = each.key criteria { dynamic \u0026quot;condition\u0026quot; { for_each = each.","tags":["Terraform","NSX-T"],"title":"Terraform nested for_each for NSX-T with dynamic","type":"post"},{"authors":["Noel"],"categories":["Terraform"],"content":" The steps below are what I have followed to create a terraform-bundle to use terraform with non default providers on a server that doesn\u0026rsquo;t have access to Internet. You can find the tool explanation in the below link.\nhttps://github.com/hashicorp/terraform/tree/master/tools/terraform-bundle\ninstallation of golang with msi downloaded here\nhttps://golang.org/doc/install\nClone the terraform repository to get the tool\nhttps://github.com/hashicorp/terraform.git\ncd terraform-master go install .\\tools\\terraform-bundle  Check the terraform version C:\\Users\\noyel\\Desktop\\tfforeach\\nsxt\u0026gt;terraform version Terraform v0.14.6 + provider registry.terraform.io/vmware/nsxt v3.0.1  Create a terraform-bundle.hcl file terraform { # Version of Terraform to include in the bundle. An exact version number # is required. version = \u0026quot;0.14.6\u0026quot; } # Define which provider plugins are to be included providers { # Include the newest \u0026quot;nsxt\u0026quot; provider version in the 1.0 series. nsxt = { source = \u0026quot;vmware/nsxt\u0026quot; versions = [\u0026quot;~\u0026gt; 3.0.0\u0026quot;] } }  Create the bundle C:\\Users\\noyel\\Desktop\\tfforeach\\nsxt\u0026gt;terraform-bundle package terraform-bundle.hcl Fetching Terraform 0.14.6 core package... Local plugin directory \u0026quot;.plugins\u0026quot; found; scanning for provider binaries. No \u0026quot;.plugins\u0026quot; directory found, skipping local provider discovery. - Finding vmware/nsxt versions matching \u0026quot;~\u0026gt; 3.0.0\u0026quot;... - Installing vmware/nsxt v3.0.1... Creating terraform_0.14.6-bundle2021021713_windows_amd64.zip ... All done!  Move the zip file to the server you want to use. Unzip the file. For a basic utilization move the terraform.exe and plugins in the directory where your terraform files are.\nThe provider.tf file looks terraform { required_providers { nsxt = { source = \u0026quot;vmware/nsxt\u0026quot; version = \u0026quot;3.0.1\u0026quot; } } } provider \u0026quot;nsxt\u0026quot; { host = \u0026quot;1.2.3.4\u0026quot; username = \u0026quot;admin\u0026quot; password = \u0026quot;123\u0026quot; }  When initialize Terraform with the init command, specify the plugins directory.\nC:\\Users\\bubibi\\terraform\\terraform init -plugin-dir=C:\\Users\\bubibi\\terraform\\plugins Terraform has been successfuly initialized!  ","date":1613593957,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613593957,"objectID":"24ca96a911210aa9523b6b00f0ccd865","permalink":"https://netmemo.github.io/post/tf-bundle-windows/","publishdate":"2021-02-17T21:32:37+01:00","relpermalink":"/post/tf-bundle-windows/","section":"post","summary":"The steps below are what I have followed to create a terraform-bundle to use terraform with non default providers on a server that doesn\u0026rsquo;t have access to Internet. You can find the tool explanation in the below link.\nhttps://github.com/hashicorp/terraform/tree/master/tools/terraform-bundle\ninstallation of golang with msi downloaded here\nhttps://golang.org/doc/install\nClone the terraform repository to get the tool\nhttps://github.com/hashicorp/terraform.git\ncd terraform-master go install .\\tools\\terraform-bundle  Check the terraform version C:\\Users\\noyel\\Desktop\\tfforeach\\nsxt\u0026gt;terraform version Terraform v0.","tags":["Terraform","NSXT"],"title":"Create portable Terraform and plugins with Terraform-bundle for Windows","type":"post"},{"authors":["Noel"],"categories":["Terraform","AWS"],"content":" An extended explanation of the differences between for_each, for and count can be find on the link below https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9\nThe two main drawbacks of using count are :\n- Can\u0026rsquo;t be used to loop over inline blocks\n- Difficult to remove entry from a list because it changes the index and those Terraform may want to destroy the resource because it has a different index\nBelow is an example of the variables used to create subnets within AWS VPCs and the main file with the for_each. The variables contain a map of subnets maps with cidr and az (availability zone) attributes. The for_each loop over the map of subnets maps to create the subnets.\nvariables.tf variable \u0026quot;tag_name\u0026quot; { default = \u0026quot;main-vpc\u0026quot; } variable \u0026quot;vpc-cidr\u0026quot; { default = \u0026quot;10.0.0.0/16\u0026quot; } variable \u0026quot;basename\u0026quot; { description = \u0026quot;Prefix used for all resources names\u0026quot; default = \u0026quot;nbo\u0026quot; } #map of maps for create subnets variable \u0026quot;prefix\u0026quot; { type = map default = { sub-1 = { az = \u0026quot;use2-az1\u0026quot; cidr = \u0026quot;10.0.198.0/24\u0026quot; } sub-2 = { az = \u0026quot;use2-az2\u0026quot; cidr = \u0026quot;10.0.199.0/24\u0026quot; } sub-3 = { az = \u0026quot;use2-az3\u0026quot; cidr = \u0026quot;10.0.200.0/24\u0026quot; } } }  main.tf resource \u0026quot;aws_vpc\u0026quot; \u0026quot;main-vpc\u0026quot; { cidr_block = var.vpc-cidr tags = { Name = var.tag_name } } resource \u0026quot;aws_subnet\u0026quot; \u0026quot;main-subnet\u0026quot; { for_each = var.prefix availability_zone_id = each.value[\u0026quot;az\u0026quot;] cidr_block = each.value[\u0026quot;cidr\u0026quot;] vpc_id = aws_vpc.main-vpc.id tags = { Name = \u0026quot;${var.basename}-subnet-${each.key}\u0026quot; } }  You can find the output of the terraform plan/apply, the terraform.state and the others tf files in the below links.\nhttps://github.com/netmemo/tf-for-each-exemple\n","date":1613591989,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613591989,"objectID":"fe57e5853341f40c14122a3e00179b87","permalink":"https://netmemo.github.io/post/tf-for-each/","publishdate":"2021-02-17T20:59:49+01:00","relpermalink":"/post/tf-for-each/","section":"post","summary":"An extended explanation of the differences between for_each, for and count can be find on the link below https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9\nThe two main drawbacks of using count are :\n- Can\u0026rsquo;t be used to loop over inline blocks\n- Difficult to remove entry from a list because it changes the index and those Terraform may want to destroy the resource because it has a different index\nBelow is an example of the variables used to create subnets within AWS VPCs and the main file with the for_each.","tags":["Terraform","AWS"],"title":"Using Terraform for_each to create subnets in AWS VPC","type":"post"},{"authors":["Noel"],"categories":["Convergence"],"content":"One article that can help understanding and make decisions about fast failover https://blog.ipspace.net/2020/11/detecting-network-failure.html https://blog.ipspace.net/2012/09/do-we-need-lacp-and-udld.html\nIs there any benefit by enabling BFD on directly connected interface ?\nSometime it can be enabled to help veryfied that there are no issues on physical layer and data link layer of an interface. It can help if on the data link layer you are not using UDLD or LACP.\n","date":1606897790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606897790,"objectID":"07b388783220f9b83a477257a4745a54","permalink":"https://netmemo.github.io/post/bfd-directly-connected/","publishdate":"2020-12-02T09:29:50+01:00","relpermalink":"/post/bfd-directly-connected/","section":"post","summary":"One article that can help understanding and make decisions about fast failover https://blog.ipspace.net/2020/11/detecting-network-failure.html https://blog.ipspace.net/2012/09/do-we-need-lacp-and-udld.html\nIs there any benefit by enabling BFD on directly connected interface ?\nSometime it can be enabled to help veryfied that there are no issues on physical layer and data link layer of an interface. It can help if on the data link layer you are not using UDLD or LACP.","tags":["BFD"],"title":"BFD on directly connected","type":"post"},{"authors":["Noel"],"categories":["DC"],"content":" \u0026ldquo;Several of these protocols are standards\u0026rdquo; My understanding is that even if the protocols look standard, Cisco made some modifications on them : VXLAN (fiels to transport ACI Policies), ISIS (added the multidestination tree) and hence are note standard anymore.\n\u0026ldquo;Does it require proprietary server ?\u0026rdquo; Not prorietary servers but proprietary switches\u0026hellip;So you are locked in regarding the software and the hardware. Both can\u0026rsquo;t be decoupled. If you choose to move to another switch vendor, you need to change the hardware and sart learning new software and protocols skills. Previously while you probably still need to change the software and the hardware, you didn\u0026rsquo;t need to learn everything from scratch regarding the protocols.\nIn the past with Fabric Path, I have already had issues by beeing locked-in with both. When Cisco will end up the support, you then need to change the hardware and learn new software/protocols skills.\nSecurity lock-in The security policies in ACI are another lock-in. If you use the Cisco Application Centric mode, it’s even worse. When Cisco decides it’s not bankable anymore and starts moving away from it, you will need to migrate the hardware, learn new software, new protocols and migrate the security to something completely different.\nFor some organisations, this scenario will be a nightmare and probably cost more money than investing regulary in people.\nIf you don\u0026rsquo;t want to use the Application Centric mode and just use the Network Centric mode, you still have an expensive solution with lots of options you will pay for but never use. Moreover you will inherit all the software complexity and associated bugs for features that are useless for you.\nSumm up Standard Protocols not so standard:\nISIS (Multidestination tree ftag)\nBGP for multi site\nVXLAN (Field to transport ACI policies)\nProprietary Protocols\nCOOP\nVery little public documentation\nLock in : Hardware, software, security\nPreviously : some features or knobs were proprietary or tied to hardware but not the all system\n=\u0026gt; EIGRP, software lock in but you can ignore it if you want\n=\u0026gt; FP at the time of the launch no other option were standard\nSoftware complexity due to all the features you don\u0026rsquo;t want + software has been thought to be application aware and even if you don\u0026rsquo;t want to use it, you will anyway inherit the code complexity of it.\nEverything centralized in a box right in the middle of the data path (Spine COOP devices).\nStopped product:\n  Loadbalancing : ACE/CSS   Fabric Path   Ironport ?   iWAN   VPn Concentrator  \nYou need EVPN anyway for multisite.\nYou need automation anyway because it\u0026rsquo;s too complex to manage via the GUI and you want to standardise all the conf.\n","date":1606587096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606587096,"objectID":"5b1bcfa925458eb7f63027ca022c6cd1","permalink":"https://netmemo.github.io/post/aci-other-angle/","publishdate":"2020-11-28T19:11:36+01:00","relpermalink":"/post/aci-other-angle/","section":"post","summary":"\u0026ldquo;Several of these protocols are standards\u0026rdquo; My understanding is that even if the protocols look standard, Cisco made some modifications on them : VXLAN (fiels to transport ACI Policies), ISIS (added the multidestination tree) and hence are note standard anymore.\n\u0026ldquo;Does it require proprietary server ?\u0026rdquo; Not prorietary servers but proprietary switches\u0026hellip;So you are locked in regarding the software and the hardware. Both can\u0026rsquo;t be decoupled. If you choose to move to another switch vendor, you need to change the hardware and sart learning new software and protocols skills.","tags":["ACI","DC"],"title":"ACI from an other angle","type":"post"},{"authors":["Noel"],"categories":["SP"],"content":" ISIS Segment routing basics for Arista EOS References:\nhttps://www.arista.com/en/um-eos/eos-section-35-3-is-is-segment-routing\nSRGB Segment Routing Golbal Block\nPrefix-SID It\u0026rsquo;s global and unique. It identify a prefix. It\u0026rsquo;s called anycast SID when it\u0026rsquo;s send by a group of router.\nNode-SID It\u0026rsquo;s global and unique. Only one per node. It identify the node.\nAdjacent-SID It\u0026rsquo;s local and can use a dynamique range. It\u0026rsquo;s used to identify an interconnection between 2 nodes.\nWith Arista EOS 4.23 it\u0026rsquo;s automaticaly allocated from the isis dynamic range show mpls label ranges.\nBasic config for ARISTA I have added the prefix-sid to test it end to end between the VPC but actually it can work without the prefix-sid. In the next article I will add the L3 VPN on top of SR without using prefix-sid In bold you find the ISIS-SR specific command require to enable ISIS-SR\nR1 interface Loopback1 ip address 1.1.1.1/32 node-segment ipv4 index 1 isis enable ISIS-SR ! interface Ethernet1 no switchport ip address 10.10.12.1/24 isis enable ISIS-SR ! interface Ethernet2 no switchport ip address 10.10.10.1/24 isis enable ISIS-SR ! ip routing ! mpls ip ! router isis ISIS-SR net 10.0000.0010.0100.1001.00 is-type level-2 ! address-family ipv4 unicast ! segment-routing mpls  no shutdown prefix-segment 10.10.10.0/24 index 51 !  R2 interface Loopback1 ip address 2.2.2.2/32 node-segment ipv4 index 2 isis enable ISIS-SR ! interface Ethernet1 no switchport ip address 10.10.12.2/24 isis enable ISIS-SR ! interface Ethernet2 no switchport ip address 10.10.23.2/24 isis enable ISIS-SR ! ip routing ! mpls ip ! router isis ISIS-SR net 10.0000.0020.0200.2002.00 is-type level-2 ! address-family ipv4 unicast ! segment-routing mpls no shutdown !  R3 interface Loopback1 ip address 3.3.3.3/32 node-segment ipv4 index 3 isis enable ISIS-SR ! interface Ethernet1 no switchport ip address 10.10.23.3/24 isis enable ISIS-SR ! interface Ethernet2 no switchport ip address 10.10.30.1/24 isis enable ISIS-SR ! ip routing ! mpls ip ! router isis ISIS-SR net 10.0000.0030.0300.3003.00 is-type level-2 ! address-family ipv4 unicast ! segment-routing mpls no shutdown prefix-segment 10.10.30.0/24 index 53 !  show commands R1#show isis segment-routing System ID: R1 Instance: ISIS-SR SR supported Data-plane: MPLS SR Router ID: 1.1.1.1 SR Global Block( SRGB ): Base: 900000 Size: 65536 Adj-SID allocation mode: SR-adjacencies Adj-SID allocation pool: Base: 100000 Size: 16384 All Prefix Segments have : P:0 E:0 V:0 L:0 IS-IS Reachability Algorithm : SPF (0) Number of IS-IS segment routing capable peers: 2 Self-Originated Segment Statistics: Node-Segments : 1 Prefix-Segments : 1 Proxy-Node-Segments : 0 Adjacency Segments : 1  R1#show mpls segment-routing bindings 1.1.1.1/32 Local binding: Label: imp-null Remote binding: Peer ID: 0020.0200.2002, Label: 900001 2.2.2.2/32 Local binding: Label: 900002 Remote binding: Peer ID: 0020.0200.2002, Label: imp-null 3.3.3.3/32 Local binding: Label: 900003 Remote binding: Peer ID: 0020.0200.2002, Label: 900003 10.10.10.0/24 Local binding: Label: imp-null Remote binding: Peer ID: 0020.0200.2002, Label: 900051 10.10.30.0/24 Local binding: Label: 900053 Remote binding: Peer ID: 0020.0200.2002, Label: 900053  R1#show isis segment-routing prefix-segments System ID: 0010.0100.1001 Instance: 'ISIS-SR' SR supported Data-plane: MPLS SR Router ID: 1.1.1.1 Node: 3 Proxy-Node: 0 Prefix: 2 Total Segments: 5 Flag Descriptions: R: Re-advertised, N: Node Segment, P: no-PHP E: Explicit-NULL, V: Value, L: Local Segment status codes: * - Self originated Prefix, L1 - level 1, L2 - level 2 Prefix SID Type Flags System ID Level Protection ------------------------- ----- ---------- ----------------------- --------------- ----- ---------- * 1.1.1.1/32 1 Node R:0 N:1 P:0 E:0 V:0 L:0 0010.0100.1001 L2 unprotected 2.2.2.2/32 2 Node R:0 N:1 P:0 E:0 V:0 L:0 0020.0200.2002 L2 unprotected 3.3.3.3/32 3 Node R:0 N:1 P:0 E:0 V:0 L:0 0030.0300.3003 L2 unprotected * 10.10.10.0/24 51 Prefix R:0 N:0 P:0 E:0 V:0 L:0 0010.0100.1001 L2 unprotected 10.10.30.0/24 53 Prefix R:0 N:0 P:0 E:0 V:0 L:0 0030.0300.3003 L2 unprotected  R1#show isis segment-routing adjacency-segments System ID: R1 Instance: ISIS-SR SR supported Data-plane: MPLS SR Router ID: 1.1.1.1 Adj-SID allocation mode: SR-adjacencies Adj-SID allocation pool: Base: 100000 Size: 16384 Adjacency Segment Count: 1 Flag Descriptions: F: Ipv6 address family, B: Backup, V: Value L: Local, S: Set Segment Status codes: L1 - Level-1 adjacency, L2 - Level-2 adjacency, P2P - Point-to-Point adjacency, LAN - Broadcast adjacency Locally Originated Adjacency Segments Adj IP Address Local Intf SID SID Source Flags Type --------------- ----------- ------- ------------ --------------------- -------- 10.10.12.2 Et1 100000 Dynamic F:0 B:0 V:1 L:1 S:0 LAN L2 Protection ----------- unprotected  ","date":1606237925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606237925,"objectID":"39ad9fd59aab0ba8ffe0b924874c0c4c","permalink":"https://netmemo.github.io/post/aristabasesr/","publishdate":"2020-11-24T18:12:05+01:00","relpermalink":"/post/aristabasesr/","section":"post","summary":"ISIS Segment routing basics for Arista EOS References:\nhttps://www.arista.com/en/um-eos/eos-section-35-3-is-is-segment-routing\nSRGB Segment Routing Golbal Block\nPrefix-SID It\u0026rsquo;s global and unique. It identify a prefix. It\u0026rsquo;s called anycast SID when it\u0026rsquo;s send by a group of router.\nNode-SID It\u0026rsquo;s global and unique. Only one per node. It identify the node.\nAdjacent-SID It\u0026rsquo;s local and can use a dynamique range. It\u0026rsquo;s used to identify an interconnection between 2 nodes.\nWith Arista EOS 4.","tags":["SR","Segment Routing","Arista","EOS","ISIS"],"title":"Arista basic ISIS-SR","type":"post"},{"authors":[],"categories":[],"content":" Maximum path In the RIB + FIB ECMP (multipath)\nIs PIC supported by default ?\nhttps://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/irg-bgp-mp-pic.html\n=\u0026gt; With BGP Multipath, the BGP prefix-independant convergence (PIC) feature is supported\n Attribut that should be identical \n- Weight\n- LP\n- AS Path (AS Number unless relax us used, AS length)\n- Origin Code\n- MED\n- IGP Metric Next hop should be different\nAdd Path If the path are equal, allow to advertise more than one bes oath (need to test in eBGP).\nMostly used with BGP without MPLS. If MPLS is used it\u0026rsquo;s better to have an RD different for each VRF of each router (easier to troubleshoot).\nhttps://orhanergun.net/wp-content/uploads/2019/11/BGP-Add-path-vs-Shadow-RR-vs-Shadow-Session-vs-Unique-RD.pdf\nPIC https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2016/pdf/BRKRST-3321.pdf I don\u0026rsquo;t neded it at the moment because we don\u0026rsquo;t need this level if convergence (couple of minutes vs ms if you have hundres of thousand of prefixes)\nhttps://blog.ipspace.net/2012/01/prefix-independent-convergence-pic.html\nThe generic optimization of the RIB-to-FIB update process is known as Prefix-Independent Convergence (PIC) - if the routing protocols can pre-compute alternate paths, suitably designed FIB can use that information to cache alternate next hops. Updating such a FIB no longer involves numerous updates to individual prefixes; you have to change only the next hop reachability information.\nBest External Allow a router to advertise it\u0026rsquo;s best external path even if in it\u0026rsquo;s BGP table it does have a beter route from inside\nLABEL ALLOCATION Per VRF  Cons \n- IP Lookup needed after label lookup (can be a benefit, cf route sum issue)\n- No granular load balancing because the bottom label is the same for all prefixes, if platform load balances on bottom label\n- Potential forwarding loop during local traffic diversion to support PIC (Transient loop)\n- No support for EIBGP multipath\n Pros \n- 1 label per vrf (less label used)\nPer CE  Cons \n- No granular load balancing because the bottom label is the same for all prefixes from one CE, if platform load balances on bottom label\n- eBGPload balancing \u0026amp; BGP PIC is not supported (it makes usage of label diversity), unless resilient per-ce label\n- Only single hop eBGPsupported, no multihop\n Pros \n- No IP lookup needed after label lookup\n- Per-CE : one MPLS label per next-hop (so per connected CE router)(Number of MPLS labels used is very low)\n","date":1589176941,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589176941,"objectID":"b539297dbff77ad990caec0ccd889e01","permalink":"https://netmemo.github.io/post/bgp-multipath/","publishdate":"2020-05-11T08:02:21+02:00","relpermalink":"/post/bgp-multipath/","section":"post","summary":"Maximum path In the RIB + FIB ECMP (multipath)\nIs PIC supported by default ?\nhttps://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/irg-bgp-mp-pic.html\n=\u0026gt; With BGP Multipath, the BGP prefix-independant convergence (PIC) feature is supported\n Attribut that should be identical \n- Weight\n- LP\n- AS Path (AS Number unless relax us used, AS length)\n- Origin Code\n- MED\n- IGP Metric Next hop should be different\nAdd Path If the path are equal, allow to advertise more than one bes oath (need to test in eBGP).","tags":[],"title":"Bgp Multipath","type":"post"},{"authors":["Noel"],"categories":["ROUTING"],"content":"prerequis : - github desktop - hugo\nconfig.toml =\u0026gt; need to be modify to change the copyright date\nlancer cmd\nGo to the folder blog : C:\\Users\\noyel\\Documents\\Site\\blog\u0026gt;\ntaper : hugo new post/my-first-post.md\ntry the web site with : hugo server -D\nentrer http://localhost:1313/\ngenerate the web site with : hugo -t \u0026ldquo;academic\u0026rdquo; Move the content of the folder C:\\Users\\noyel\\Documents\\Site\\blog\\public\nin GitHub\\netmemo.github.io lancer github desktop, commit/push to master\nTo have a file attached for the post, create a directory with the exact same name as the post md file.\nThe language used to write a blog post is Markdown https://www.markdownguide.org/basic-syntax https://www.tutorialspoint.com/html/html_ascii_codes.htm\nBold and underline **\u0026lt;ins\u0026gt; Cons \u0026lt;/ins\u0026gt;**\n","date":1589176811,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589176811,"objectID":"b574d36d30607eb31ef58b610a3fbd9a","permalink":"https://netmemo.github.io/post/how-to-hugo/","publishdate":"2020-05-11T08:00:11+02:00","relpermalink":"/post/how-to-hugo/","section":"post","summary":"prerequis : - github desktop - hugo\nconfig.toml =\u0026gt; need to be modify to change the copyright date\nlancer cmd\nGo to the folder blog : C:\\Users\\noyel\\Documents\\Site\\blog\u0026gt;\ntaper : hugo new post/my-first-post.md\ntry the web site with : hugo server -D\nentrer http://localhost:1313/\ngenerate the web site with : hugo -t \u0026ldquo;academic\u0026rdquo; Move the content of the folder C:\\Users\\noyel\\Documents\\Site\\blog\\public\nin GitHub\\netmemo.github.io lancer github desktop, commit/push to master\nTo have a file attached for the post, create a directory with the exact same name as the post md file.","tags":["ANYCAST","OSPF","BGP","ROUTING"],"title":"How to hugo","type":"post"},{"authors":["Noel"],"categories":["ROUTING"],"content":" Issue R3 route traffic to R4 instead of R1. Route 10.10.10.10 toward the WAN is prefered instead of the OSPF LAN DC route. DC Client that try to reach the DC\u0026rsquo;s 10.10.10.10 address are routed toward the WAN\nWhy R3 is installing in it RIB and redistributing the wrong routes 10.10.10.10 because it does have a better AD 20.\nWorkarround  If we filter the redistribution that doesn\u0026rsquo;t help because when the packet arrive to R3 he will still prefer the BGP route. If we drop the prefix on R3 we loose the redundancy Increase the AD of the WAN route on R3  ","date":1582065947,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582065947,"objectID":"cbd0cba140a1956254039ffbd8bdf2a3","permalink":"https://netmemo.github.io/post/ospfbgpanycast/","publishdate":"2020-02-18T23:45:47+01:00","relpermalink":"/post/ospfbgpanycast/","section":"post","summary":"Issue R3 route traffic to R4 instead of R1. Route 10.10.10.10 toward the WAN is prefered instead of the OSPF LAN DC route. DC Client that try to reach the DC\u0026rsquo;s 10.10.10.10 address are routed toward the WAN\nWhy R3 is installing in it RIB and redistributing the wrong routes 10.10.10.10 because it does have a better AD 20.\nWorkarround  If we filter the redistribution that doesn\u0026rsquo;t help because when the packet arrive to R3 he will still prefer the BGP route.","tags":["ANYCAST","OSPF","BGP","ROUTING"],"title":"OSPF vs BGP with anycast prefix","type":"post"},{"authors":["Noel"],"categories":["tools"],"content":" Below is a very light virtual machine based on Core Linux kernel 4.8 (TinyCore) 26 Mo with network tools like iperf3, tcpdump, net-bridging, iproute2, busybox (httpd), tcpreplay, nmap, openssh.\ncorelinux1.5.ova\nbasic commands/directory /etc/sysconfig/tcedir/optional =\u0026gt; packages /etc/sysconfig/tcedir/onboot.lst =\u0026gt; on boot package to be loaded  sudo vi /opt/eth0.sh =\u0026gt; change interfaces parameters #configure an interface pkill udhcp =\u0026gt; stop dhcp for this interface ifconfig eth0 10.253.106.2 netmask 255.255.255.192 up route add default gw 10.253.106.1  filetool.sh -b =\u0026gt; save the configuration changes  vi /opt/hostnameAuto.sh =\u0026gt; change the hostname vi /opt/bootlocal.sh =\u0026gt; the script that is executed after the device has booted.  ","date":1540215355,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540215355,"objectID":"e1a31189949599c96ef2ff11865654fa","permalink":"https://netmemo.github.io/post/corelinuxnetmemo/","publishdate":"2018-10-22T15:35:55+02:00","relpermalink":"/post/corelinuxnetmemo/","section":"post","summary":"Below is a very light virtual machine based on Core Linux kernel 4.8 (TinyCore) 26 Mo with network tools like iperf3, tcpdump, net-bridging, iproute2, busybox (httpd), tcpreplay, nmap, openssh.\ncorelinux1.5.ova\nbasic commands/directory /etc/sysconfig/tcedir/optional =\u0026gt; packages /etc/sysconfig/tcedir/onboot.lst =\u0026gt; on boot package to be loaded  sudo vi /opt/eth0.sh =\u0026gt; change interfaces parameters #configure an interface pkill udhcp =\u0026gt; stop dhcp for this interface ifconfig eth0 10.253.106.2 netmask 255.255.255.192 up route add default gw 10.","tags":["tools","VM","Core Linux"],"title":"Core Linux Netmemo","type":"post"},{"authors":["Noel"],"categories":["container"],"content":" This post is a memo on how I did the installtion of Kubernetes and Calico on VMs. It\u0026rsquo;s not some best pactrices in anyway.\nI\u0026rsquo;ve chose VM because I didn\u0026rsquo;t want to depend on any Cloud infrastructure. I\u0026rsquo;ve also wanted to understand the network interaction between K8s parts from an infrastructure point of view.\nPrerequisite : know how to create VMs on any hypervisors\nSteps to deploy K8s :  Install 1 ubuntu router with 3 interfaces. 1 for NAT/Internet access and 2 for the K8s LAN. I\u0026rsquo;ve created 2 LAN to see what happen under the hood when K8s nodes communicates.\n Install 3 Ubuntu servers, 1 for the master and 2 for the workers. 1 worker in the same ethernet segment and subnet than the master. 1 worker in another network.\n Gotchas:  By default, the K8s interface is the one with the default route. All my servers have one OOB interface and one production interface. Special tunning for k8s =\u0026gt; turn off the swap   Install runtime and enable it on boot\n Installing kubeadm, kubelet and kubectl\n Initializing the master (choose the pod network add-on before to add the relevent parameters, Calico parameters in my case)\n Install the pod network add-on\n Join node/workers to the cluster\n That it, you can now play with the K8s cluster\n  Optional : Install ctl for calico\nComments : To create anything you just have to kubectl apply -f myfile The magic happen in myfile where you describe what you want to create.\nBelow the capture after the lab is completed CaptureCalicok8s\nDetails of the tasks 3. https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-runtime\nI\u0026rsquo;ve needed to add the following commands\nsystemctl enable docker.service systemctl start docker.service  4. https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl\napt-get update \u0026amp;amp;\u0026amp;amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat \u0026amp;lt;\u0026amp;lt;EOF \u0026amp;gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl  5.\nkubeadm init --pod-network-cidr=192.168.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  6.\nkubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml  7.\nkubeadm join 10.0.1.10:6443 --token d34b9i.v03t2yiozio63cq6 --discovery-token-ca-cert-hash sha256:c21d04ea23790a0bf81cf64118e3a9075ffb63ed90bc697acef5793386e9eb16  ","date":1538776759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538776759,"objectID":"04e4be9702fa5b85a26f6612d64762cc","permalink":"https://netmemo.github.io/post/k8s-on-vms-with-calico/","publishdate":"2018-10-05T23:59:19+02:00","relpermalink":"/post/k8s-on-vms-with-calico/","section":"post","summary":"This post is a memo on how I did the installtion of Kubernetes and Calico on VMs. It\u0026rsquo;s not some best pactrices in anyway.\nI\u0026rsquo;ve chose VM because I didn\u0026rsquo;t want to depend on any Cloud infrastructure. I\u0026rsquo;ve also wanted to understand the network interaction between K8s parts from an infrastructure point of view.\nPrerequisite : know how to create VMs on any hypervisors\nSteps to deploy K8s :  Install 1 ubuntu router with 3 interfaces.","tags":["calico","docker","k8s","kubernetes","netplan","ubuntu","virtualbox"],"title":"K8s on Vms With Calico","type":"post"},{"authors":["Noel"],"categories":["cisco","nexus"],"content":"On N7K\nhttps://www.cisco.com/c/en/us/support/docs/switches/nexus-7000-series-switches/116647-technote-product-00.html\n=\u0026gt; flanker car avec la commande show hardware internal dev-port-map, il n\u0026rsquo;y a pas d\u0026rsquo;asic Clipper, uniquement des flanker\nSample of icmp troubleshooting from the Admin VDC\nshow module attach module 1 show hardware internal dev-port-map elam asic flanker instance 2 layer2 trigger dbus ipv4 egress if destination-ipv4-address 10.253.108.90 start status  elam asic flanker instance 2 layer2 trigger dbus ipv4 ingress if destination-ipv4-address 10.253.108.90 start status  On N5K\nhttps://www.cisco.com/c/en/us/support/docs/switches/nexus-6000-series-switches/118902-technote-nexus-00.html\nMY-SWITCH# show platform fwm info pif ethernet 2/2 | inc slot_asic Eth2/2 pd: slot 1 logical port num 1 slot_asic_num 0 global_asic_num 5 fw_inst 4 phy_fw_inst 1 fc 0  Note: The slot numbers are 0-based, whereas the bigsur instances are 1-based. Therefore, in this example slot 1 corresponds to bigsur instance 2.\n=\u0026gt; slot 1 become 2 in the elam command, the instance stay the same as the slot_asic_num =\u0026gt; ingress for the traffic ingressing the interface, egress for the egressing traffic.\nelam slot 2 asic bigsur instance 0 trigger lu ingress ipv4 if source-ipv4-address_ipv4 10.253.108.226 destination-ipv4-address_ipv4 10.253.108.90 start capture show elam asic bigsur show capture lu stop capture  ","date":1538606937,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538606937,"objectID":"f8fba9a12db76da45dc28dbce3e682ba","permalink":"https://netmemo.github.io/post/troubleshooting-elam-cisco-n7k-n5k/","publishdate":"2018-10-04T00:48:57+02:00","relpermalink":"/post/troubleshooting-elam-cisco-n7k-n5k/","section":"post","summary":"On N7K\nhttps://www.cisco.com/c/en/us/support/docs/switches/nexus-7000-series-switches/116647-technote-product-00.html\n=\u0026gt; flanker car avec la commande show hardware internal dev-port-map, il n\u0026rsquo;y a pas d\u0026rsquo;asic Clipper, uniquement des flanker\nSample of icmp troubleshooting from the Admin VDC\nshow module attach module 1 show hardware internal dev-port-map elam asic flanker instance 2 layer2 trigger dbus ipv4 egress if destination-ipv4-address 10.253.108.90 start status  elam asic flanker instance 2 layer2 trigger dbus ipv4 ingress if destination-ipv4-address 10.253.108.90 start status  On N5K","tags":["cisco","elam","N5K","N7K","troubleshooting"],"title":"Troubleshooting Elam Cisco N7k N5k","type":"post"},{"authors":["Noel"],"categories":["container"],"content":"This is the cheat sheet for the post : https://netmemo.github.io/post/k8s-on-vms-with-calico/\nThe following post contain raw entry only for reminder purpose.\nBellow are the links I\u0026rsquo;ve used to understand/did my lab\nhttps://fr.wikipedia.org/wiki/Kubernetes#/media/File:Kubernetes.png https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/tutorials/k8s101/ https://kubernetes.io/docs/tutorials/k8s201/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/\nJoin a node/worker to the master\nkubeadm join 10.0.1.10:6443 --token d34b9i.v03t2yiozio63cq6 --discovery-token-ca-cert-hash sha256:c21d04ea23790a0bf81cf64118e3a9075ffb63ed90bc697acef5793386e9eb16  Delete a deployment\nkubectl delete deployment nginx-deployment-nbo  To get the logs of a specific container. -n is to specify the namespace\nkubectl logs calico-node-zxvjv -n kube-system calico-node  Allow to launch a shell for a specific container\nkubectl exec -it nginx-deployment-nbo-fd57b7b88-l8xsv -- /bin/bash  Create a static page in the container to differentiate it from the others. The -c option is to ask bash to execute the command.\nkubectl exec -it nginx-deployment-nbo-fd57b7b88-kkw9s -- /bin/bash -c \u0026quot;echo Hello shell demo SRV1 \u0026gt; /usr/share/nginx/html/index.html\u0026quot; kubectl exec -it nginx-deployment-nbo-fd57b7b88-kkw9s cat /usr/share/nginx/html/index.html  To troubleshhot\njournalctl -r  Display all pods, even with the system name space, -o wide allow to see the IP addresses\nkubectl get pods --all-namespaces -o wide  To see the last messages of container associated to the pode\nkubectl describe pod -n kube-system calico-node-zxvjv  Allow to see the node/server/worker ip addresses (-o wide)\nsudo kubectl get node -o wide  by default kubernetes don\u0026rsquo;t work with swap, so I needed to disable it with the command swapoff and to comment the swap line in the fstab file.\nswapoff vi /etc/fstab # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # /dev/mapper/ubuntu--srv--base--vg-root / ext4 errors=remount-ro 0 1 #/dev/mapper/ubuntu--srv--base--vg-swap_1 none swap sw 0 0  Not related to Kubernets but you need to modify the interfaces\nvi /etc/netplan/01-netcfg.yaml  Add interfaces to ubuntu\n/etc/netplan/01-netcfg.yaml  This file describes the network interfaces available on your system For more information, see netplan(5).\nnetwork: version: 2 renderer: networkd ethernets: enp0s3: dhcp4: no addresses: - 10.0.1.10/24 routes: - to: 0.0.0.0/0 via: 10.0.1.253 nameservers: addresses: [1.1.1.1]  apply the /etc/netplan/01-netcfg.yaml configuration\nnetplan apply  display ip addresses on interfaces\nip address show  display all interfaces\nip link show  display routes\nroute -n  In order for Kubernetes to work, you need container runtime to be started\nsystemctl enable docker.service systemctl start docker.service  Download calicoctl, to be able to interact with calico with CLI\nsudo curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.2.1/calicoctl sudo chmod +x calicoctl  To see the state of calico on nodes (BGP,Peer-type,up/down,time)\nsudo calicoctl node status  The following commands allow to export a variables with the IP address and ports of nginx-service previously created and access the content from the host or the container\nexport SERVICE_IP=$(kubectl get service nginx-service -o go-template='{{.spec.clusterIP}}') export SERVICE_PORT=$(kubectl get service nginx-service -o go-template='{{(index .spec.ports 0).port}}') wget -qO- http://$SERVICE_IP:$SERVICE_PORT kubectl run busybox --generator=run-pod/v1 --image=busybox --restart=Never --tty -i --env \u0026quot;SERVICE_IP=$SERVICE_IP\u0026quot; --env \u0026quot;SERVICE_PORT=$SERVICE_PORT\u0026quot; u@busybox$ wget -qO- http://$SERVICE_IP:$SERVICE_PORT # Run in the busybox container u@busybox$ exit # Exit the busybox container  noel@ubuntu-srv-1:~$ cat nginx-test.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: nginx-deployment-nbo spec: selector: matchLabels: app: nginx replicas: 3 # tells deployment to run 3 pods matching the template template: metadata: labels: app: nginx spec: volumes: - name: shared-data emptyDir: {} containers: - name: nginx image: nginx:1.7.9 volumeMounts: - name: shared-data mountPath: /usr/share/nginx/html ports: - containerPort: 80  https://kubernetes.io/docs/tutorials/k8s201/\napiVersion: v1 kind: Service metadata: name: nginx-service spec: ports: - port: 8000 # the port that this service should serve on # the container on each pod to connect to, can be a name # (e.g. 'www') or a number (e.g. 80) targetPort: 80 protocol: TCP # just like the selector in the deployment, # but this time it identifies the set of pods to load balance # traffic to. selector: app: nginx  These commands are to configure calicoctl in order to work with the local k8s\nexport CALICO_DATASTORE_TYPE=kubernetes export CALICO_KUBECONFIG=~/.kube/config Pour le root export CALICO_KUBECONFIG=/home/noel/.kube/config  Move the Calico mode from Always to CrossSubnet. First we get the calico ippool configuration, then we need to modify the ipipMode in the yaml file and eventually to apply the new configuration\ncalicoctl get ippool -o yaml \u0026gt; ippool.yaml  Change the mode ipipMode: CrossSubnet\ncalicoctl apply -f ippool.yaml  ","date":1538606240,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538606240,"objectID":"507e52dad9b1e639033f350fdc69e5dc","permalink":"https://netmemo.github.io/post/cheat-sheet-k8s-on-vms-with-calico/","publishdate":"2018-10-04T00:37:20+02:00","relpermalink":"/post/cheat-sheet-k8s-on-vms-with-calico/","section":"post","summary":"This is the cheat sheet for the post : https://netmemo.github.io/post/k8s-on-vms-with-calico/\nThe following post contain raw entry only for reminder purpose.\nBellow are the links I\u0026rsquo;ve used to understand/did my lab\nhttps://fr.wikipedia.org/wiki/Kubernetes#/media/File:Kubernetes.png https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/tutorials/k8s101/ https://kubernetes.io/docs/tutorials/k8s201/ https://kubernetes.io/docs/reference/kubectl/cheatsheet/\nJoin a node/worker to the master\nkubeadm join 10.0.1.10:6443 --token d34b9i.v03t2yiozio63cq6 --discovery-token-ca-cert-hash sha256:c21d04ea23790a0bf81cf64118e3a9075ffb63ed90bc697acef5793386e9eb16  Delete a deployment\nkubectl delete deployment nginx-deployment-nbo  To get the logs of a specific container. -n is to specify the namespace","tags":["calico","docker","k8s","kubernetes","netplan","ubuntu","virtualbox"],"title":"[Cheat Sheet] K8s on VMs with Calico","type":"post"}]