<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>netmemo.github.io on netmemo.github.io</title>
    <link>https://netmemo.github.io/</link>
    <description>Recent content in netmemo.github.io on netmemo.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>No&amp;euml;l Boul&amp;egrave;ne &amp;copy; 2021. This blog is strictly personnal and opinions expressed here are only mine and doesn&amp;#39;t reflect those of my past, current or futur employers. No warranty whatsoever is made that any of the posts are accurate. There is absolutely no assurance (apart from author&amp;acute;s professional integrity) that any statement contained in a post is true, correct or precise.</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Github Actions with Terraform Cloud for CI/CD of NSX-T</title>
      <link>https://netmemo.github.io/post/tf-gha-nsxt-cicd/</link>
      <pubDate>Sun, 15 Aug 2021 14:02:39 +0200</pubDate>
      
      <guid>https://netmemo.github.io/post/tf-gha-nsxt-cicd/</guid>
      <description>

&lt;p&gt;This post is to show an example of using CI/CD with Terraform Cloud and &lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/github-actions&#34; target=&#34;_blank&#34;&gt;Github Actions&lt;/a&gt; in order to have a better NetDevOps approach by doing NSX-T Network Infrastructure as code (IaC).
It&amp;rsquo;s almost a bingo, I think I have most of the buzz words of these last years :)&lt;/p&gt;

&lt;p&gt;I will describe the structure of the project, the project components, the project workflow and finish with how to test this project.&lt;/p&gt;

&lt;h1 id=&#34;structure-of-the-project&#34;&gt;Structure of the project&lt;/h1&gt;

&lt;p&gt;The diagram below shows a high level view of the &lt;a href=&#34;https://github.com/netmemo/tf-gha-nsxt-cicd&#34; target=&#34;_blank&#34;&gt;project&lt;/a&gt;.
&lt;a href=&#34;tf-gha-cicd-nsx.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;tf-gha-cicd-nsx.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can find the file structure of the project below&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
‚îú‚îÄ main.tf
‚îú‚îÄ .github
¬†¬† ‚îî‚îÄ‚îÄ workflows
¬†¬†     ‚îú‚îÄ‚îÄ dev-to-pr.yml
¬†¬†     ‚îú‚îÄ‚îÄ plan-prod.yml
¬†¬†     ‚îî‚îÄ‚îÄ apply-prod.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;project-components&#34;&gt;Project components&lt;/h1&gt;

&lt;h2 id=&#34;nsx-t&#34;&gt;NSX-T&lt;/h2&gt;

&lt;p&gt;For this project we need two NSXT environments, one for production and one for development. You can either have 2 full blown NSX-T or use different variables for prod and dev or different VRFs.
In this blog post we will use two different deployments.&lt;/p&gt;

&lt;h2 id=&#34;terraform-cloud&#34;&gt;Terraform Cloud&lt;/h2&gt;

&lt;p&gt;Terraform Cloud will be used to store the state of the prod and dev environment. It will also allow to manage Terraform through APIs.
For the project we need to create 2 differents workspaces, one for production and one for developments.
The name of the workspaces should be prefix-suffix where the prefix will be what is configured in your terraform configuration file and the sufix will be what is used to select the workspace. We will cover how to select the workspace later in that post.
For this blog post, the prefix will be netmemo- and the suffix will be either prod or dev.
&lt;a href=&#34;workspaces.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;workspaces.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once the workspaces are created, we need to add 3 variables for the NSX-T provider:
&lt;a href=&#34;tf-variables.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;tf-variables.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally we need to add the API key that Github Actions will use to connect to Terraform Cloud
&lt;a href=&#34;tf-token.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;tf-token.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;github&#34;&gt;Github&lt;/h2&gt;

&lt;p&gt;Github is where we will store the configuration and execute our CI/CD pipelines.
Once the project is forked, to make it works, we need to enable Github Actions.
&lt;a href=&#34;enable-gha.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;enable-gha.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create a github &lt;a href=&#34;https://github.com/settings/tokens&#34; target=&#34;_blank&#34;&gt;personal access token&lt;/a&gt; &lt;em&gt;REPO_TOKEN&lt;/em&gt;. This token will be used by Github Actions to automatically create the pull request.&lt;br /&gt;
&lt;a href=&#34;gh-perso-token.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;gh-perso-token.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Add the previously created token (TF_API_TOKEN and REPO_TOKEN_SECRET) to your github &lt;a href=&#34;https://github.com/netmemo/nsxt-tfc-rm/settings/secrets/actions&#34; target=&#34;_blank&#34;&gt;repository secrets&lt;/a&gt;.&lt;br /&gt;
&lt;a href=&#34;repo-secret.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;repo-secret.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;github-actions&#34;&gt;Github Actions&lt;/h2&gt;

&lt;p&gt;This project is made from 3 scripts that form 3 pipelines. The first script will handle the dev environment. The second will handle the &lt;em&gt;terraform plan&lt;/em&gt; for the prod environments. The third will handle the &lt;em&gt;terraform apply&lt;/em&gt; for the prod environment.&lt;/p&gt;

&lt;p&gt;You can find an explanation of the main steps on the below site &lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/github-actions&#34; target=&#34;_blank&#34;&gt;Github Actions&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;dev-to-pr-yml&#34;&gt;dev-to-pr.yml&lt;/h4&gt;

&lt;p&gt;The dev-to-pr.yml Github Actions YAML file will be executed only after a &lt;em&gt;push&lt;/em&gt; on the &lt;em&gt;dev&lt;/em&gt; branch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;on:
  push:
    branches:
      - dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step is to checkout the current configuration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Checkout
        uses: actions/checkout@v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then the &lt;em&gt;Setup Terraform&lt;/em&gt; steps retrieves the Terraform CLI used in the GitHub action workflow.
This is in this step that we will use the TF_API_TOKEN that we have created previously to access Terraform Cloud from Github Actions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1
        with:
          # terraform_version: 0.13.0:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These following steps initialize Terraform and set the &lt;a href=&#34;https://netmemo.github.io/post/tf-workspace-var&#34; target=&#34;_blank&#34;&gt;terraform workspace&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Terraform Init
        id: init
        run: terraform init
        env:
          TF_WORKSPACE: &amp;quot;dev&amp;quot;

      - name: Terraform Workspace
        id: workspace
        run: terraform workspace select dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then validate the Terraform configuration&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Terraform Validate
        id: validate
        run: terraform validate -no-color
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This step is to execute the Terraform plan. The plan will be a speculative plan executed in Terraform Cloud. Speculative plans are not directly visible from the Terraform Cloud UI. To access it, you will need to click on the link given in the result of the Terraform plan command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Terraform Plan
        id: plan
        run: terraform plan -no-color
        continue-on-error: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This step will execute a &lt;a href=&#34;https://github.com/marketplace/actions/github-script?version=v4.0.2&#34; target=&#34;_blank&#34;&gt;github-script&lt;/a&gt; that will send a REST API query thanks to the &lt;em&gt;github&lt;/em&gt; pre-authenticated &lt;a href=&#34;https://octokit.github.io/rest.js/v18&#34; target=&#34;_blank&#34;&gt;octokit/rest.js&lt;/a&gt; client with pagination plugins. It will also create a comment on the commit with the result of the previous steps.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - uses: actions/github-script@0.9.0
        if: github.event_name == &#39;push&#39;
        env:
          PLAN: &amp;quot;terraform\n${{ steps.plan.outputs.stdout }}&amp;quot;
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const output = `#### Terraform Format and Style [36;63H\`${{ steps.fmt.outcome }}\`
            #### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
            #### Terraform Validation [36;41H\`${{ steps.validate.outcome }}\`
            #### Terraform Plan [36;35H\`${{ steps.plan.outcome }}\`
            &amp;lt;details&amp;gt;&amp;lt;summary&amp;gt;Show Plan&amp;lt;/summary&amp;gt;
            \`\`\`\n
            ${process.env.PLAN}
            \`\`\`
            &amp;lt;/details&amp;gt;
            *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            github.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: output
            })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This step is to apply the Terraform configuration, only if the &lt;em&gt;terraform plan&lt;/em&gt; step result has succeeded.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Terraform Apply
        id: apply
        if: steps.plan.outcome == &#39;success&#39;
        run: terraform apply -auto-approve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the &lt;em&gt;terraform apply&lt;/em&gt; succeeds, we will use the &lt;a href=&#34;https://github.com/marketplace/actions/github-script?version=v4.0.2&#34; target=&#34;_blank&#34;&gt;github-script&lt;/a&gt; to create a &lt;a href=&#34;https://octokit.github.io/rest.js/v18#pulls-create&#34; target=&#34;_blank&#34;&gt;Pull Request&lt;/a&gt; thanks to the &lt;a href=&#34;https://octokit.github.io/rest.js/v18&#34; target=&#34;_blank&#34;&gt;octokit/rest.js&lt;/a&gt; like in the previous steps.
We will use a personal github token &lt;em&gt;PERSO_GITHUB_TOKEN&lt;/em&gt; to create the PR for these steps. If we are not using the personal token, the &lt;a href=&#34;https://github.com/peter-evans/create-pull-request/issues/48&#34; target=&#34;_blank&#34;&gt;PR will not trigger other pipelines&lt;/a&gt; that have the &lt;em&gt;on:pull_request trigger&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: CreatePR if apply succeed
        uses: actions/github-script@v4.0.2
        if: steps.apply.outcome == &#39;success&#39;
        with:
          github-token: ${{ secrets.PERSO_GITHUB_TOKEN }}
          script: |
            github.pulls.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: &amp;quot;Auto PR&amp;quot;,
              head: &amp;quot;dev&amp;quot;,
              base: &amp;quot;main&amp;quot;
            });
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;plan-prod-yml&#34;&gt;plan-prod.yml&lt;/h4&gt;

&lt;p&gt;The plan-prod.yml Github Actions YAML file will be executed only after a &lt;em&gt;pull request&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;on:
  pull_request:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Most of the steps have already been described for the &lt;em&gt;dev-to-pr.yml&lt;/em&gt; files, the only difference is the step below.
It will add a comment on the PR with the description of what has been done.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - uses: actions/github-script@v4.0.2
        env:
          PLAN: &amp;quot;terraform\n${{ steps.plan.outputs.stdout }}&amp;quot;
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const output = `#### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
            #### Terraform Validation [36;41H\`${{ steps.validate.outcome }}\`
            #### Terraform Plan [36;35H\`${{ steps.plan.outcome }}\`
            &amp;lt;details&amp;gt;&amp;lt;summary&amp;gt;Show Plan&amp;lt;/summary&amp;gt;
            \`\`\`\n
            ${process.env.PLAN}
            \`\`\`
            &amp;lt;/details&amp;gt;
            *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;apply-prod-yml&#34;&gt;apply-prod.yml&lt;/h4&gt;

&lt;p&gt;The plan-prod.yml Github Actions YAML file will be executed only after a &lt;em&gt;push&lt;/em&gt; on the main branch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;on:
  push:
    branches:
      - main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The other steps have already been discussed previously.&lt;/p&gt;

&lt;h2 id=&#34;terraform-file&#34;&gt;Terraform file&lt;/h2&gt;

&lt;p&gt;For this blog post we will have a single main.tf file.
This Terraform script should have the terraform configuration, the provider definition and the resources definitions.&lt;/p&gt;

&lt;h4 id=&#34;terraform-variables&#34;&gt;Terraform variables&lt;/h4&gt;

&lt;p&gt;This is where we declare the Terraform variables that will be defined in the Terrafrom Cloud workspaces. These variables will change according to the environment.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;variable &amp;quot;password&amp;quot; {
  type = string
}
variable &amp;quot;username&amp;quot; {
  type = string
}
variable &amp;quot;nsxhost&amp;quot; {
  type = string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;terraform-configuration-section&#34;&gt;Terraform configuration section&lt;/h4&gt;

&lt;p&gt;In this section we are setting the 2 providers needed and the backend.
The &lt;em&gt;backend &amp;ldquo;&lt;a href=&#34;https://www.terraform.io/docs/language/settings/backends/remote.html#workspaces&#34; target=&#34;_blank&#34;&gt;remote&lt;/a&gt;&amp;ldquo;&lt;/em&gt; is where you find the Terraform Cloud organization and workspaces prefixes.
The workspaces suffixes will be added as seen in the beginning of this post with the &lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/automate-terraform?in=terraform/automation&#34; target=&#34;_blank&#34;&gt;TF_WORKSPACE&lt;/a&gt; environment variables and the &lt;a href=&#34;https://www.terraform.io/docs/language/settings/backends/remote.html#workspaces&#34; target=&#34;_blank&#34;&gt;terraform workspace select&lt;/a&gt; command in the Github Actions steps.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  required_providers {
    random = {
      source = &amp;quot;hashicorp/random&amp;quot;
      version = &amp;quot;3.0.1&amp;quot;
    }
    nsxt = {
      source = &amp;quot;vmware/nsxt&amp;quot;
      version = &amp;quot;&amp;gt;= 3.1.1&amp;quot;
    }
  }

  backend &amp;quot;remote&amp;quot; {
    organization = &amp;quot;netmemo&amp;quot;

    workspaces {
      prefix = &amp;quot;netmemo-&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;provider-configuration&#34;&gt;Provider configuration&lt;/h4&gt;

&lt;p&gt;This is the NSX-T provider configuration. We pass the 3 variables that are defined in the Terraform Cloud workspaces.
We disable the SSL verification because this is a local lab.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;provider &amp;quot;nsxt&amp;quot; {
    host = var.nsxhost
    username = var.username
    password = var.password
    allow_unverified_ssl = true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;resource-creation&#34;&gt;Resource creation&lt;/h4&gt;

&lt;p&gt;For this blog post we will create a &lt;a href=&#34;https://registry.terraform.io/providers/vmware/nsxt/latest/docs/resources/policy_tier1_gateway&#34; target=&#34;_blank&#34;&gt;NSX-T T1 gateway&lt;/a&gt; name &lt;strong&gt;T1-TFC&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;nsxt_policy_tier1_gateway&amp;quot; &amp;quot;tier1_gw&amp;quot; {
  description               = &amp;quot;Tier-1 provisioned by Terraform&amp;quot;
  display_name              = &amp;quot;T1-TFC&amp;quot;
  route_advertisement_types = [&amp;quot;TIER1_CONNECTED&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;project-workflow&#34;&gt;Project workflow&lt;/h1&gt;

&lt;p&gt;Clic the arrows to see the detail.
&lt;details&gt;
&lt;summary&gt;1. Modify the dev branch on you local git.&lt;/summary&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;2. Add the modif to git, commit them and push the dev branch to your github repo.&lt;/summary&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git add .

git commit -am &amp;quot;add T1-GW&amp;quot;
[dev a0b946b] add T1-GW
 2 files changed, 6 insertions(+), 6 deletions(-)

git push
Counting objects: 6, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (5/5), done.
Writing objects: 100% (6/6), 617 bytes | 617.00 KiB/s, done.
Total 6 (delta 2), reused 0 (delta 0)
remote: Resolving deltas: 100% (2/2), completed with 2 local objects.
To ssh://github.com/netmemo/tf-gha-nsxt-cicd.git
   8534543..a0b946b  dev -&amp;gt; dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;3. Dev pipeline&lt;/summary&gt;
The dev pipeline is triggered, you can see it&amp;rsquo;s starting in the Actions tab. It&amp;rsquo;s tittle will be the commit message.&lt;br /&gt;
&lt;a href=&#34;workflow-start.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;workflow-start.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
To validate that the dev pipeline is completed, you can check the Terraform cloud dev workspace state and the NSX-T dev environment to see that there is a new T1-GW.
&lt;a href=&#34;tfc-state-dev-after.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;tfc-state-dev-after.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;4. Integration pipeline&lt;/summary&gt;
After all the steps in the dev pipeline are passed, the workflow will turn green. The last step of the pipeline will create an automatic PR that will trigger the integration pipeline. This pipeline will validate the configuration and creates a plan against the production environment.
The PR will have the description &lt;strong&gt;&amp;ldquo;Auto PR generate by Github Action dev pipeline&amp;rdquo;&lt;/strong&gt;
&lt;a href=&#34;pr-list.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;pr-list.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
The integration pipeline will execute the &lt;em&gt;terraform plan&lt;/em&gt; command against the production environment. You can then check again the Actions tab to see that the development and integration pipeline has turned green.
&lt;a href=&#34;workflow-2-pipeline-ok.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;workflow-2-pipeline-ok.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
If the two pipelines are ok, the checks in the PR should be green.&lt;br /&gt;
&lt;a href=&#34;\pr-test-ok.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;pr-test-ok.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;5. Merge&lt;/summary&gt;
When the dev and integration pipeline are finished, the dev branch is ready to be merged to the main branch. You can then clic on the &lt;strong&gt;Merge pull request&lt;/strong&gt; button to validate the merge and trigger the deployment pipeline.
&lt;a href=&#34;going-to-prod.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;going-to-prod.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;a href=&#34;pr-merged.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;pr-merged.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;6. Deployment pipeline&lt;/summary&gt;
The deployment pipeline is triggered with the push/merge to the main branch. This is the final workflow that will push the Terraform configuration to production. You can see in Terraform Cloud that Terraform is applying the configuration.
&lt;a href=&#34;tfc-prod-applying.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;tfc-prod-applying.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
You can now double check in Github Actions that the deployment workflow has been succefully executed, &lt;strong&gt;&amp;ldquo;Merge pull request #10&amp;rdquo;&lt;/strong&gt; in the picture below.
&lt;a href=&#34;pipeline-apply-ok-list.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;pipeline-apply-ok-list.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;7. Checks&lt;/summary&gt;
Finally you can now verify that there is a resource on Terraform Cloud and that the T1 Gateway has been created on the production NSX-T.&lt;br /&gt;
&lt;a href=&#34;tfc-prod-ok.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;tfc-prod-ok.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;a href=&#34;nsxt-prod-ok.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;nsxt-prod-ok.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;
&lt;/details&gt;&lt;/p&gt;

&lt;h1 id=&#34;how-to-test-this-project&#34;&gt;How to test this project.&lt;/h1&gt;

&lt;p&gt;Requirements:&lt;br /&gt;
- NSX-T environment (there are examples on the web to use AWS as provider instead of NSX, the concepts are the same).&lt;br /&gt;
- Terraform Cloud account with 2 workspaces.&lt;br /&gt;
- Github account and git locally.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;[NSX-T]&lt;/strong&gt; Prepare your two NSX-T environments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TFC]&lt;/strong&gt; Create your two workspaces in TFC with the API Key for Github.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[GITHUB]&lt;/strong&gt; Fork this &lt;a href=&#34;https://github.com/netmemo/tf-gha-nsxt-cicd&#34; target=&#34;_blank&#34;&gt;project&lt;/a&gt;, add the TFC API key and your personal API Key to your repo, enable actions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[LOCAL]&lt;/strong&gt; Clone your forked project to your local git.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[LOCAL]&lt;/strong&gt; Create the dev branch locally.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[LOCAL]&lt;/strong&gt; Modify the Terraform configuration in the main.tf according to your TFC workspace.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[LOCAL]&lt;/strong&gt; Add to git, commit, push.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[GITHUB]&lt;/strong&gt; Wait until all the test pass.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[GITHUB]&lt;/strong&gt; Click merge to deploy to prod.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;pain-points-of-the-project&#34;&gt;Pain points of the project.&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Switching &lt;a href=&#34;https://netmemo.github.io/post/tf-workspace-var&#34; target=&#34;_blank&#34;&gt;terraform workspace&lt;/a&gt; with github actions.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Create &lt;a href=&#34;https://netmemo.github.io/post/gha-auto-pr&#34; target=&#34;_blank&#34;&gt;automatically&lt;/a&gt; a github PR with with rest.js API of octokit.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script src=&#34;https://utteranc.es/client.js&#34;
        repo=&#34;[ENTER REPO HERE]&#34;
        issue-term=&#34;pathname&#34;
        theme=&#34;github-dark&#34;
        crossorigin=&#34;anonymous&#34;
        async&gt;
&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Triggering Github Actions workflow with automatic Pull Request</title>
      <link>https://netmemo.github.io/post/gha-auto-pr/</link>
      <pubDate>Thu, 12 Aug 2021 17:01:57 +0200</pubDate>
      
      <guid>https://netmemo.github.io/post/gha-auto-pr/</guid>
      <description>&lt;p&gt;This post is to explain one of the pain point I have encountered while trying to do &lt;a href=&#34;https://netmemo.github.io/post/tf-gha-nsxt-cicd&#34; target=&#34;_blank&#34;&gt;Github Actions with Terraform Cloud for CI/CD of NSX-T&lt;/a&gt;.
The difficulty is to chain workflow/pipeline automatically. In my case, I wanted to launch a workflow base of a PR create by another workflow.
When you use Github Actions to interface with Github, you need to authenticate your Github Actions script against Github.
You can then use the &lt;a href=&#34;https://docs.github.com/en/actions/reference/authentication-in-a-workflow&#34; target=&#34;_blank&#34;&gt;GITHUB_TOKEN&lt;/a&gt; that has been made for this purpose. As this token is known from Github to be automation token, to avoid loops, you can use it to create a PR to trigger another workflow.
The workaround to this [known limitation] is to create the PR with a personal access token.&lt;/p&gt;

&lt;p&gt;You can find below an example of using the personal access token to create a PR. To create the &lt;a href=&#34;https://octokit.github.io/rest.js/v18#pulls-create&#34; target=&#34;_blank&#34;&gt;Pull Request&lt;/a&gt; we are using &lt;a href=&#34;https://github.com/marketplace/actions/github-script?version=v4.0.2&#34; target=&#34;_blank&#34;&gt;github-script&lt;/a&gt; that will send a REST API query thanks to the &lt;em&gt;github&lt;/em&gt; pre-authenticated &lt;a href=&#34;https://octokit.github.io/rest.js/v18&#34; target=&#34;_blank&#34;&gt;octokit/rest.js&lt;/a&gt; client with pagination plugins.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: CreatePR if apply succeed
        uses: actions/github-script@v4.0.2
        if: steps.apply.outcome == &#39;success&#39;
        with:
          github-token: ${{ secrets.PERSO_GITHUB_TOKEN }}
          script: |
            github.pulls.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: &amp;quot;Auto PR&amp;quot;,
              head: &amp;quot;dev&amp;quot;,
              base: &amp;quot;main&amp;quot;
            });
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Changing Terraform Cloud workspace in Github Actions</title>
      <link>https://netmemo.github.io/post/tf-workspace-var/</link>
      <pubDate>Thu, 12 Aug 2021 15:22:40 +0200</pubDate>
      
      <guid>https://netmemo.github.io/post/tf-workspace-var/</guid>
      <description>&lt;p&gt;This blog post is to explain how I did to automatically change Terraform Cloud workspace from Github Actions.
As explained in the documentation &lt;a href=&#34;https://www.terraform.io/docs/language/settings/backends/remote.html#workspaces&#34; target=&#34;_blank&#34;&gt;remote workspace&lt;/a&gt;, you can use different remote workspace by specifying the prefix of you workspace in the Terraform backend configuration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  backend &amp;quot;remote&amp;quot; {
    organization = &amp;quot;netmemo&amp;quot;

    workspaces {
      prefix = &amp;quot;netmemo-&amp;quot;
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, you only need to select the proper workspace by entering the &lt;em&gt;terraform workspace select [suffix]&lt;/em&gt; command.
The issue comes if you want to do it in a fully automated environment like with Github Actions. You need an extra step which is to set the &lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/automate-terraform?in=terraform/automation&#34; target=&#34;_blank&#34;&gt;TF_WORKSPACE&lt;/a&gt; variable.&lt;/p&gt;

&lt;p&gt;Below are the 2 steps that are needed to select a specific environment.
In this blog the workspace will be netmemo-dev where netmemo- is the prefix configured in the main.tf and dev the suffix configured in the .yml file of the Github Actions script.&lt;/p&gt;

&lt;p&gt;This step initializes Terraform and set the TF_WORKSPACE variable to indicate that we want to use the dev environment suffix before the initialization.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Terraform Init
        id: init
        run: terraform init
        env:
          TF_WORKSPACE: &amp;quot;dev&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we are not setting the &lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/automate-terraform?in=terraform/automation&#34; target=&#34;_blank&#34;&gt;TF_WORKSPACE&lt;/a&gt;, the init command will try to get the default workspace that doesn&amp;rsquo;t exist and you will have the following error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The currently selected workspace (default) does not exist.
  This is expected behavior when the selected workspace did not have an
  existing non-empty state. Please enter a number to select a workspace:
  
  1. dev
  2. prod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even after setting the &lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/automate-terraform?in=terraform/automation&#34; target=&#34;_blank&#34;&gt;TF_WORKSPACE&lt;/a&gt; variable, we still need to enter the &lt;em&gt;terraform workspace select&lt;/em&gt; command to provide the suffix of the workspace.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      - name: Terraform Workspace
        id: workspace
        run: terraform workspace select dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we forgot to enter the command to select workspace, the Terraform configuration section will try to load a workspace with only the prefix and trigger an error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: error starting operation: The configured &amp;quot;remote&amp;quot; backend encountered an unexpected error:

invalid value for workspace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find a full example of where we need to change workspace automatically in this blog post &lt;a href=&#34;https://netmemo.github.io/post/tf-gha-nsxt-cicd&#34; target=&#34;_blank&#34;&gt;Github Actions with Terraform Cloud for CI/CD of NSX-T&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSX-T Firewall rules as code with Terraform</title>
      <link>https://netmemo.github.io/post/nsxt-tf-firewall/</link>
      <pubDate>Thu, 29 Jul 2021 19:03:37 +0200</pubDate>
      
      <guid>https://netmemo.github.io/post/nsxt-tf-firewall/</guid>
      <description>

&lt;p&gt;This article is to show an example of how to manage NSX-T firewall rules as a code through Terraform.
You can find the project on my github account : &lt;a href=&#34;https://github.com/netmemo/nsxt-frac-tf-cm&#34; target=&#34;_blank&#34;&gt;nsxt-frac-tf-cm&lt;/a&gt; and &lt;a href=&#34;https://github.com/netmemo/nsxt-frac-tf-rm&#34; target=&#34;_blank&#34;&gt;nsxt-frac-tf-rm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will describe the structure of the project, how it works, the data model, the Terraform code explanation and finish with an example.&lt;/p&gt;

&lt;h1 id=&#34;structure-of-the-project&#34;&gt;Structure of the project&lt;/h1&gt;

&lt;p&gt;The diagram below shows a summary of how I organized the project in order to fully use infrastructre as code.&lt;br /&gt;
&lt;a href=&#34;nsxt-tf-firewall.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;nsxt-tf-firewall.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Below is the file structure&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Child modules

‚îú‚îÄ‚îÄ nsxt-frac-tf-cm 
 ¬†¬† ‚îú‚îÄ‚îÄ nsxt-tf-cm-dfw
 ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
 ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
 ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
 ¬†¬† ‚îú‚îÄ‚îÄ nsxt-tf-cm-grp
 ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
 ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
 ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
 ¬†¬† ‚îî‚îÄ‚îÄ nsxt-tf-cm-svc
 ¬†¬†     ‚îú‚îÄ‚îÄ main.tf
 ¬†¬†     ‚îú‚îÄ‚îÄ outputs.tf
 ¬†¬†     ‚îî‚îÄ‚îÄ variables.tf

#Root modules

‚îú‚îÄ‚îÄ nsxt-frac-tf-rm
    ‚îú‚îÄ‚îÄ nsxt-tf-rm-dfw
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ provider.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îî‚îÄ‚îÄ nsxt-tf-rm-grpsvc
     ¬†¬† ‚îú‚îÄ‚îÄ main.tf
     ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
     ¬†¬† ‚îú‚îÄ‚îÄ provider.tf
     ¬†¬† ‚îú‚îÄ‚îÄ terraform.tfvars
     ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In short, child modules are the logic, root modules are the variables.&lt;br /&gt;
You can then duplicate the root module according to the NSX-T environment you want to deploy and start using the variables for the environment.&lt;/p&gt;

&lt;h1 id=&#34;how-it-works&#34;&gt;How it works&lt;/h1&gt;

&lt;h2 id=&#34;child-module&#34;&gt;Child Module&lt;/h2&gt;

&lt;p&gt;You can find the logics and the code complexity in the Terraform child modules.&lt;br /&gt;
They will create the NSX-T resources. They allow to centralize the complexity. If you need to change your logic, you only need to modify the child module.&lt;br /&gt;
After the modification, from the root module you only need to pull the new modifications by reinitializing or refreshing the modules with &amp;ldquo;terraform init&amp;rdquo; and then you can start using the new features.&lt;/p&gt;

&lt;p&gt;There are tree child modules:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nsxt-tf-cm-svc&lt;br /&gt;
This is for the NSX-T (TCP/UDP/IP) services creation&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;nsxt-tf-cm-grp&lt;br /&gt;
This is for the NSX-T (TAG and IP) groups creation&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;nsxt-tf-cm-dfw&lt;br /&gt;
This is for the NSX-T policy and rules creation&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;root-module&#34;&gt;Root Module&lt;/h2&gt;

&lt;p&gt;It will call the child module with the variables associated with the NSX-T environment. You need to create a repository per environment. The only modifications you need to do is to change the variables.&lt;br /&gt;
The root module can be declined to different NSX-T environments/deployments as long as the variable structure used in the root module respects the child module data model.&lt;/p&gt;

&lt;p&gt;Root modules are separated in two :&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;nsxt-tf-rm-grpsvc&lt;/strong&gt;&lt;br /&gt;
This module will call the groups and the services child module an pass them the groups and services&amp;rsquo;s map variables.
As the groups and services can be centralized and be the same for all environments, I have prefered to managed them separatly from the policies and rules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;nsxt-tf-rm-dfw&lt;/strong&gt;&lt;br /&gt;
This is the policies and rules definition. This root module will use the terraform state output section of the root module &lt;em&gt;nsxt-tf-rm-grpsvc&lt;/em&gt; to get all the groups and services needed.
The &lt;em&gt;output.tf&lt;/em&gt; file is defined in &lt;em&gt;nsxt-tf-rm-grpsvc&lt;/em&gt; root module.&lt;/p&gt;

&lt;h1 id=&#34;data-model&#34;&gt;Data model&lt;/h1&gt;

&lt;p&gt;To have more details of the possible attributes used, you can refer to the NSX-T Terraform official &lt;a href=&#34;https://registry.terraform.io/providers/vmware/nsxt/latest/docs&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;services&#34;&gt;Services&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;map_svc&lt;/strong&gt;&lt;br /&gt;
This is the map variable passed to the child module.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This variable is a map of services map where the name is the service name&lt;/li&gt;
&lt;li&gt;Every service name contains lists.&lt;/li&gt;
&lt;li&gt;Every list name is the service type (IP, TCP or UDP).&lt;/li&gt;
&lt;li&gt;Every list is either a protocol number if the list is IP or port number if the list is TCP/UDP&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;map_svc = {
   NETMEMO-ESP = { IP = [&amp;quot;50&amp;quot;] }
   NETMEMO-NETBIOS = { TCP = [&amp;quot;135&amp;quot;,&amp;quot;137&amp;quot;,&amp;quot;139&amp;quot;,&amp;quot;139&amp;quot;], UDP = [&amp;quot;135&amp;quot;,&amp;quot;137&amp;quot;,&amp;quot;139&amp;quot;,&amp;quot;139&amp;quot;] }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;groups&#34;&gt;Groups&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;map_grp&lt;/strong&gt;&lt;br /&gt;
This is the map variable passed to the child module.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This variable is a map of groups map where the name is the group name.&lt;/li&gt;
&lt;li&gt;Every group name contains lists.&lt;/li&gt;
&lt;li&gt;Every list name is the group type (IP or TAG).&lt;/li&gt;
&lt;li&gt;Every list is either subnet if the list is IP or tag name if the list is TAG&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;map_grp = {
   NETMEMO-LOCAL = { IP = [&amp;quot;10.0.0.0/25&amp;quot;,&amp;quot;10.1.1.0/25&amp;quot;] }
   NETMEMO-NAS = { TAG = [&amp;quot;NAS&amp;quot;,&amp;quot;FILES&amp;quot;] }
   NETMEMO-ESX = { TAG = [&amp;quot;ESX&amp;quot;,&amp;quot;VMWARE&amp;quot;] }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;policies&#34;&gt;Policies&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;map_policies&lt;/strong&gt;&lt;br /&gt;
This is the map variable passed to the child module.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This variable is a map of policy map where the name is the policy name.&lt;/li&gt;
&lt;li&gt;Every policy name contains different attributes that are either string or map. The map named &amp;ldquo;rules&amp;rdquo; is to define the rules.&lt;/li&gt;
&lt;li&gt;Every &amp;ldquo;rules&amp;rdquo; map contains maps to define rules. Every map is a rule.&lt;/li&gt;
&lt;li&gt;Every rule map contains stings and list attrubutes (sources,destinations,services,scope)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;map_policies = {
   NETMEMO-POL1 = {
      category = &amp;quot;Application&amp;quot;
	  sequence_number = &amp;quot;10&amp;quot;
	  rules = {
	     netmemo-rule1 = {
		    display = &amp;quot;NETMEMO-NAS-R&amp;quot;
			sources = [&amp;quot;NETMEMO-NAS&amp;quot;]
			destinations = [&amp;quot;NETMEMO-NAS&amp;quot;]
			services = [&amp;quot;HTTPS&amp;quot;]
			scope = [&amp;quot;NETMEMO-NAS&amp;quot;]
			action = &amp;quot;ALLOW&amp;quot;
			disabled = &amp;quot;false&amp;quot;
	     }
	     netmemo-rule2 = {
		    display = &amp;quot;NETMEMO-ESX-R&amp;quot;
			sources = [&amp;quot;NETMEMO-ESX&amp;quot;]
			destinations = [&amp;quot;NETMEMO-ESX&amp;quot;]
			services = [&amp;quot;HTTPS&amp;quot;]
			scope = [&amp;quot;NETMEMO-ESX&amp;quot;]
			action = &amp;quot;ALLOW&amp;quot;
			disabled = &amp;quot;false&amp;quot;
	     }
	  }
   }
   NETMEMO-POL2 = {
      category = &amp;quot;Application&amp;quot;
	  sequence_number = &amp;quot;20&amp;quot;
	  rules = {
	     netmemo-rule1 = {
		    display = &amp;quot;NETMEMO-LOCAL-R&amp;quot;
			sources = [&amp;quot;NETMEMO-LOCAL&amp;quot;]
			destinations = [&amp;quot;NETMEMO-NAS&amp;quot;,&amp;quot;NETMEMO-ESX&amp;quot;]
			services = [&amp;quot;NETMEMO-NETBIOS&amp;quot;,&amp;quot;HTTPS&amp;quot;]
			scope = [&amp;quot;NETMEMO-NAS&amp;quot;,&amp;quot;NETMEMO-ESX&amp;quot;]
			action = &amp;quot;ALLOW&amp;quot;
			disabled = &amp;quot;false&amp;quot;
	     }
	  }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;code-explanation&#34;&gt;Code Explanation&lt;/h1&gt;

&lt;h2 id=&#34;root-module-1&#34;&gt;Root Module&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;nsxt-tf-cm-grp and nsxt-tf-cm-svc child modules&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These child modules use nested for_each with conditional nested dynamic.&lt;br /&gt;
A brief example of the nested for_each used with dynamic can be found &lt;a href=&#34;https://netmemo.github.io/post/tf-nsxt-nested-for-each/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. The conditional behavior is done thanks to the filter of the for loop. You can find the documentation on this &lt;a href=&#34;https://www.terraform.io/docs/language/expressions/for.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dynamic terraform blocks allow to create a block for all the elements in the map you give to the for_each loop. In our child modules, the element of the dynamic for_each loop are also filtered with a for loop and a if.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the map contains a TAG attribute we create the &lt;em&gt;criteria&lt;/em&gt; block with the TAG attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dynamic &amp;quot;criteria&amp;quot; {
#The for_each contains a for loop with filter to create the criteriia only if there is a list with the name TAG
for_each = { for key,val in each.value : key =&amp;gt; val if key == &amp;quot;TAG&amp;quot; }
  content {
     dynamic &amp;quot;condition&amp;quot; {
        #looping over the set to create every tags
        for_each = criteria.value

        content {
           key = &amp;quot;Tag&amp;quot;
           member_type = &amp;quot;VirtualMachine&amp;quot;
           operator = &amp;quot;EQUALS&amp;quot;
           value = condition.value
        }
     }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If the map contains a IP attribute we create the &lt;em&gt;criteria&lt;/em&gt; block with the IP attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dynamic &amp;quot;criteria&amp;quot; {
#The for_each contains a for loop with filter to create the criteriia only if there is a list with the name IP
for_each = { for key,val in each.value : key =&amp;gt; val if key == &amp;quot;IP&amp;quot; }
  content {
     ipaddress_expression  {
        ip_addresses = criteria.value
     }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The same logic is used to create services.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the map contains a TCP or UDP attribute we create the &lt;em&gt;l4_port_set_entry block&lt;/em&gt; with the TCP/UDP attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dynamic &amp;quot;l4_port_set_entry&amp;quot; {
  #each.value = map of TCP,UDP or IP list where l4_port_set_entry.key will be TCP or UDP
  #the for_each contains a for loop with filter to create the l4_port_set_entry only if there is a list with TCP or UDP as name
  for_each = { for key,val in each.value : key =&amp;gt; val if key == &amp;quot;TCP&amp;quot; || key == &amp;quot;UDP&amp;quot; }
      content {
         display_name = &amp;quot;${l4_port_set_entry.key}_${each.key}&amp;quot;
         protocol = l4_port_set_entry.key
         destination_ports = l4_port_set_entry.value
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If the map contains a IP attribute we create the &lt;em&gt;ip_protocol_entry block&lt;/em&gt; with the IP attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dynamic &amp;quot;ip_protocol_entry&amp;quot; {
  #each.value = map of TCP,UDP or IP list 
  #the for_each contains a for loop with filter to create the ip_protocol_entry only if there is a list with IP as name
  for_each = { for key,val in each.value : key =&amp;gt; val if key == &amp;quot;IP&amp;quot; }
      content {
         #[0] because the ip protocol will have a single IP protocol value in the set and the protocol attribut expect a number not a set
         protocol = ip_protocol_entry.value[0]
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;nsxt-tf-cm-dfw child modules&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The complexity of this module is to get the NSX-T &amp;ldquo;Path&amp;rdquo; attributes of the groups and services with their name defined in their respective map variable.
We want the map variables definition of the policies and rules to be as much friendly as possible and not dependant of values specific to the NSX-T implementation (path).
This will also allow us to reuse the policies variables values in other environments if the rules are generic for instance.&lt;/p&gt;

&lt;p&gt;In order to retreive the value we are looping over the &amp;ldquo;list&amp;rdquo; in the group or service attribute and &amp;ldquo;try&amp;rdquo; to get the path attribute.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source_groups = [for x in rule.value[&amp;quot;sources&amp;quot;] : try(var.nsxt_policy_grp_grp[x].path)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;child-module-1&#34;&gt;Child Module&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;nsxt-tf-rm-grpsvc&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is how to use a child module stored in git within a root module.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;module &amp;quot;nsxt-tf-cm-svc&amp;quot; {
   source = &amp;quot;git::https://github.com/netmemo/nsxt-frac-tf-cm.git//nsxt-tf-cm-svc&amp;quot;
   map_svc = var.map_svc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is the code of the &lt;em&gt;output.tf&lt;/em&gt; file that adds the groups and service maps into the terraform.state output section. It will allow other root modules to use these variables as &lt;em&gt;terraform_remote_state&lt;/em&gt; datasource.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output &amp;quot;grp&amp;quot; {
   value = module.nsxt-tf-cm-grp.grp
}

output &amp;quot;svc&amp;quot; {
   value = module.nsxt-tf-cm-svc.svc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;nsxt-tf-rm-dfw&lt;/strong&gt;&lt;br /&gt;
Below is the code to refer to a remote state, in this case the remote state is local.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;quot;terraform_remote_state&amp;quot; &amp;quot;grpsvc&amp;quot; {
   backend = &amp;quot;local&amp;quot;
   
   config = {
      path = &amp;quot;../nsxt-tf-rm-grpsvc/terraform.tfstate&amp;quot;
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Requirement:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Terraform 0.14+&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;NSX-T 3.0.2+&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Clone the root module &lt;a href=&#34;https://github.com/netmemo/nsxt-frac-tf-rm&#34; target=&#34;_blank&#34;&gt;nsxt-frac-tf-rm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make a test, you need to follow the steps below (clic to see the detail):&lt;/p&gt;

&lt;p&gt;&lt;details&gt;
&lt;summary&gt;1. Clone the root module repo&lt;/summary&gt;
&lt;strong&gt;git clone git@github.com:netmemo/nsxt-frac-tf-rm.git&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Cloning into &#39;nsxt-frac-tf-rm&#39;...
remote: Enumerating objects: 21, done.
remote: Counting objects: 100% (21/21), done.
remote: Compressing objects: 100% (15/15), done.
remote: Total 21 (delta 7), reused 20 (delta 6), pack-reused 0
Receiving objects: 100% (21/21), done.
Resolving deltas: 100% (7/7), done.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;2.Move to the cloned repo&lt;/summary&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd your directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;3.Initialize terraform&lt;/summary&gt;
&lt;strong&gt;terraform init&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Initializing modules...
Downloading git::https://github.com/netmemo/nsxt-frac-tf-cm.git for nsxt-tf-cm-dfw...
- nsxt-tf-cm-dfw in .terraform/modules/nsxt-tf-cm-dfw/nsxt-tf-cm-dfw

Initializing the backend...

Initializing provider plugins...
- terraform.io/builtin/terraform is built in to Terraform
- Finding vmware/nsxt versions matching &amp;quot;&amp;gt;= 3.1.1&amp;quot;...
- Installing vmware/nsxt v3.2.2...
- Installed vmware/nsxt v3.2.2 (signed by a HashiCorp partner, key ID 6B6B0F38607A2264)

Partner and community providers are signed by their developers.
If you&#39;d like to know more about provider signing, you can read about it here:
https://www.terraform.io/docs/cli/plugins/signing.html

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run &amp;quot;terraform init&amp;quot; in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running &amp;quot;terraform plan&amp;quot; to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;4.Edit the provider.tf file&lt;/summary&gt;
Change the host, the username and password according to your environment.
You need to either add the variable in the terraform.tfvars file or enter them when the prompt will ask you.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;provider &amp;quot;nsxt&amp;quot; {
   host = var.host
   username = &amp;quot;admin&amp;quot;
   password = var.password
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;5.Create the group and services&lt;/summary&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;terraform plan -out plan.out&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following
symbols:
  + create

Terraform will perform the following actions:

  # module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-ESX&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_group&amp;quot; &amp;quot;grp&amp;quot; {
      + display_name = &amp;quot;NETMEMO-ESX&amp;quot;
      + domain       = &amp;quot;default&amp;quot;
      + id           = (known after apply)
      + nsx_id       = (known after apply)
      + path         = (known after apply)
      + revision     = (known after apply)

      + criteria {
          + condition {
              + key         = &amp;quot;Tag&amp;quot;
              + member_type = &amp;quot;VirtualMachine&amp;quot;
              + operator    = &amp;quot;EQUALS&amp;quot;
              + value       = &amp;quot;ESX&amp;quot;
            }
          + condition {
              + key         = &amp;quot;Tag&amp;quot;
              + member_type = &amp;quot;VirtualMachine&amp;quot;
              + operator    = &amp;quot;EQUALS&amp;quot;
              + value       = &amp;quot;VMWARE&amp;quot;
            }
        }
    }

  # module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-LOCAL&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_group&amp;quot; &amp;quot;grp&amp;quot; {
      + display_name = &amp;quot;NETMEMO-LOCAL&amp;quot;
      + domain       = &amp;quot;default&amp;quot;
      + id           = (known after apply)
      + nsx_id       = (known after apply)
      + path         = (known after apply)
      + revision     = (known after apply)

      + criteria {

          + ipaddress_expression {
              + ip_addresses = [
                  + &amp;quot;10.0.0.0/25&amp;quot;,
                  + &amp;quot;10.1.1.0/25&amp;quot;,
                ]
            }
        }
    }

  # module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-NAS&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_group&amp;quot; &amp;quot;grp&amp;quot; {
      + display_name = &amp;quot;NETMEMO-NAS&amp;quot;
      + domain       = &amp;quot;default&amp;quot;
      + id           = (known after apply)
      + nsx_id       = (known after apply)
      + path         = (known after apply)
      + revision     = (known after apply)

      + criteria {
          + condition {
              + key         = &amp;quot;Tag&amp;quot;
              + member_type = &amp;quot;VirtualMachine&amp;quot;
              + operator    = &amp;quot;EQUALS&amp;quot;
              + value       = &amp;quot;NAS&amp;quot;
            }
          + condition {
              + key         = &amp;quot;Tag&amp;quot;
              + member_type = &amp;quot;VirtualMachine&amp;quot;
              + operator    = &amp;quot;EQUALS&amp;quot;
              + value       = &amp;quot;FILES&amp;quot;
            }
        }
    }

  # module.nsxt-tf-cm-svc.nsxt_policy_service.svc[&amp;quot;NETMEMO-ESP&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_service&amp;quot; &amp;quot;svc&amp;quot; {
      + display_name = &amp;quot;NETMEMO-ESP&amp;quot;
      + id           = (known after apply)
      + nsx_id       = (known after apply)
      + path         = (known after apply)
      + revision     = (known after apply)

      + ip_protocol_entry {
          + protocol = 50
        }
    }

  # module.nsxt-tf-cm-svc.nsxt_policy_service.svc[&amp;quot;NETMEMO-NETBIOS&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_service&amp;quot; &amp;quot;svc&amp;quot; {
      + display_name = &amp;quot;NETMEMO-NETBIOS&amp;quot;
      + id           = (known after apply)
      + nsx_id       = (known after apply)
      + path         = (known after apply)
      + revision     = (known after apply)

      + l4_port_set_entry {
          + destination_ports = [
              + &amp;quot;135&amp;quot;,
              + &amp;quot;137&amp;quot;,
              + &amp;quot;139&amp;quot;,
            ]
          + display_name      = &amp;quot;TCP_NETMEMO-NETBIOS&amp;quot;
          + protocol          = &amp;quot;TCP&amp;quot;
          + source_ports      = []
        }
      + l4_port_set_entry {
          + destination_ports = [
              + &amp;quot;135&amp;quot;,
              + &amp;quot;137&amp;quot;,
              + &amp;quot;139&amp;quot;,
            ]
          + display_name      = &amp;quot;UDP_NETMEMO-NETBIOS&amp;quot;
          + protocol          = &amp;quot;UDP&amp;quot;
          + source_ports      = []
        }
    }

Plan: 5 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + grp = {
      + NETMEMO-ESX   = {
          + conjunction       = []
          + criteria          = [
              + {
                  + condition             = [
                      + {
                          + key         = &amp;quot;Tag&amp;quot;
                          + member_type = &amp;quot;VirtualMachine&amp;quot;
                          + operator    = &amp;quot;EQUALS&amp;quot;
                          + value       = &amp;quot;ESX&amp;quot;
                        },
                      + {
                          + key         = &amp;quot;Tag&amp;quot;
                          + member_type = &amp;quot;VirtualMachine&amp;quot;
                          + operator    = &amp;quot;EQUALS&amp;quot;
                          + value       = &amp;quot;VMWARE&amp;quot;
                        },
                    ]
                  + ipaddress_expression  = []
                  + macaddress_expression = []
                  + path_expression       = []
                },
            ]
          + description       = null
          + display_name      = &amp;quot;NETMEMO-ESX&amp;quot;
          + domain            = &amp;quot;default&amp;quot;
          + extended_criteria = []
          + id                = (known after apply)
          + nsx_id            = (known after apply)
          + path              = (known after apply)
          + revision          = (known after apply)
          + tag               = []
        }
      + NETMEMO-LOCAL = {
          + conjunction       = []
          + criteria          = [
              + {
                  + condition             = []
                  + ipaddress_expression  = [
                      + {
                          + ip_addresses = [
                              + &amp;quot;10.0.0.0/25&amp;quot;,
                              + &amp;quot;10.1.1.0/25&amp;quot;,
                            ]
                        },
                    ]
                  + macaddress_expression = []
                  + path_expression       = []
                },
            ]
          + description       = null
          + display_name      = &amp;quot;NETMEMO-LOCAL&amp;quot;
          + domain            = &amp;quot;default&amp;quot;
          + extended_criteria = []
          + id                = (known after apply)
          + nsx_id            = (known after apply)
          + path              = (known after apply)
          + revision          = (known after apply)
          + tag               = []
        }
      + NETMEMO-NAS   = {
          + conjunction       = []
          + criteria          = [
              + {
                  + condition             = [
                      + {
                          + key         = &amp;quot;Tag&amp;quot;
                          + member_type = &amp;quot;VirtualMachine&amp;quot;
                          + operator    = &amp;quot;EQUALS&amp;quot;
                          + value       = &amp;quot;NAS&amp;quot;
                        },
                      + {
                          + key         = &amp;quot;Tag&amp;quot;
                          + member_type = &amp;quot;VirtualMachine&amp;quot;
                          + operator    = &amp;quot;EQUALS&amp;quot;
                          + value       = &amp;quot;FILES&amp;quot;
                        },
                    ]
                  + ipaddress_expression  = []
                  + macaddress_expression = []
                  + path_expression       = []
                },
            ]
          + description       = null
          + display_name      = &amp;quot;NETMEMO-NAS&amp;quot;
          + domain            = &amp;quot;default&amp;quot;
          + extended_criteria = []
          + id                = (known after apply)
          + nsx_id            = (known after apply)
          + path              = (known after apply)
          + revision          = (known after apply)
          + tag               = []
        }
    }
  + svc = {
      + NETMEMO-ESP     = {
          + algorithm_entry   = []
          + description       = null
          + display_name      = &amp;quot;NETMEMO-ESP&amp;quot;
          + ether_type_entry  = []
          + icmp_entry        = []
          + id                = (known after apply)
          + igmp_entry        = []
          + ip_protocol_entry = [
              + {
                  + description  = &amp;quot;&amp;quot;
                  + display_name = &amp;quot;&amp;quot;
                  + protocol     = 50
                },
            ]
          + l4_port_set_entry = []
          + nsx_id            = (known after apply)
          + path              = (known after apply)
          + revision          = (known after apply)
          + tag               = []
        }
      + NETMEMO-NETBIOS = {
          + algorithm_entry   = []
          + description       = null
          + display_name      = &amp;quot;NETMEMO-NETBIOS&amp;quot;
          + ether_type_entry  = []
          + icmp_entry        = []
          + id                = (known after apply)
          + igmp_entry        = []
          + ip_protocol_entry = []
          + l4_port_set_entry = [
              + {
                  + description       = &amp;quot;&amp;quot;
                  + destination_ports = [
                      + &amp;quot;135&amp;quot;,
                      + &amp;quot;137&amp;quot;,
                      + &amp;quot;139&amp;quot;,
                    ]
                  + display_name      = &amp;quot;TCP_NETMEMO-NETBIOS&amp;quot;
                  + protocol          = &amp;quot;TCP&amp;quot;
                  + source_ports      = []
                },
              + {
                  + description       = &amp;quot;&amp;quot;
                  + destination_ports = [
                      + &amp;quot;135&amp;quot;,
                      + &amp;quot;137&amp;quot;,
                      + &amp;quot;139&amp;quot;,
                    ]
                  + display_name      = &amp;quot;UDP_NETMEMO-NETBIOS&amp;quot;
                  + protocol          = &amp;quot;UDP&amp;quot;
                  + source_ports      = []
                },
            ]
          + nsx_id            = (known after apply)
          + path              = (known after apply)
          + revision          = (known after apply)
          + tag               = []
        }
    }

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Saved the plan to: plan.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;terraform apply plan.out&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;module.nsxt-tf-cm-svc.nsxt_policy_service.svc[&amp;quot;NETMEMO-NETBIOS&amp;quot;]: Creating...
module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-NAS&amp;quot;]: Creating...
module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-LOCAL&amp;quot;]: Creating...
module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-ESX&amp;quot;]: Creating...
module.nsxt-tf-cm-svc.nsxt_policy_service.svc[&amp;quot;NETMEMO-ESP&amp;quot;]: Creating...
module.nsxt-tf-cm-svc.nsxt_policy_service.svc[&amp;quot;NETMEMO-ESP&amp;quot;]: Creation complete after 0s [id=434fdc00-30e6-4072-a64b-a7e9534c80c2]
module.nsxt-tf-cm-svc.nsxt_policy_service.svc[&amp;quot;NETMEMO-NETBIOS&amp;quot;]: Creation complete after 0s [id=59a29e53-9901-44d2-9589-f770c36d87bb]
module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-ESX&amp;quot;]: Creation complete after 0s [id=0fb48626-d903-42cc-9513-b47e7626f44d]
module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-LOCAL&amp;quot;]: Creation complete after 0s [id=21d93fff-c08c-42dd-90da-d099dfa51163]
module.nsxt-tf-cm-grp.nsxt_policy_group.grp[&amp;quot;NETMEMO-NAS&amp;quot;]: Creation complete after 0s [id=bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb]

Apply complete! Resources: 5 added, 0 changed, 0 destroyed.
...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;6.Create the policies and rules&lt;/summary&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;terraform plan -out plan.out&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following
symbols:
  + create

Terraform will perform the following actions:

  # module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[&amp;quot;NETMEMO-POL1&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_security_policy&amp;quot; &amp;quot;policies&amp;quot; {
      + category        = &amp;quot;Application&amp;quot;
      + display_name    = &amp;quot;NETMEMO-POL1&amp;quot;
      + domain          = &amp;quot;default&amp;quot;
      + id              = (known after apply)
      + locked          = false
      + nsx_id          = (known after apply)
      + path            = (known after apply)
      + revision        = (known after apply)
      + sequence_number = 10
      + stateful        = true
      + tcp_strict      = (known after apply)

      + rule {
          + action                = &amp;quot;ALLOW&amp;quot;
          + destination_groups    = [
              + &amp;quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb&amp;quot;,
            ]
          + destinations_excluded = false
          + direction             = &amp;quot;IN_OUT&amp;quot;
          + disabled              = false
          + display_name          = &amp;quot;NETMEMO-NAS-R&amp;quot;
          + ip_version            = &amp;quot;IPV4_IPV6&amp;quot;
          + logged                = false
          + nsx_id                = (known after apply)
          + revision              = (known after apply)
          + rule_id               = (known after apply)
          + scope                 = [
              + &amp;quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb&amp;quot;,
            ]
          + sequence_number       = (known after apply)
          + services              = [
              + &amp;quot;/infra/services/HTTPS&amp;quot;,
            ]
          + source_groups         = [
              + &amp;quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb&amp;quot;,
            ]
          + sources_excluded      = false
        }
      + rule {
          + action                = &amp;quot;ALLOW&amp;quot;
          + destination_groups    = [
              + &amp;quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d&amp;quot;,
            ]
          + destinations_excluded = false
          + direction             = &amp;quot;IN_OUT&amp;quot;
          + disabled              = false
          + display_name          = &amp;quot;NETMEMO-ESX-R&amp;quot;
          + ip_version            = &amp;quot;IPV4_IPV6&amp;quot;
          + logged                = false
          + nsx_id                = (known after apply)
          + revision              = (known after apply)
          + rule_id               = (known after apply)
          + scope                 = [
              + &amp;quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d&amp;quot;,
            ]
          + sequence_number       = (known after apply)
          + services              = [
              + &amp;quot;/infra/services/HTTPS&amp;quot;,
            ]
          + source_groups         = [
              + &amp;quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d&amp;quot;,
            ]
          + sources_excluded      = false
        }
    }

  # module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[&amp;quot;NETMEMO-POL2&amp;quot;] will be created
  + resource &amp;quot;nsxt_policy_security_policy&amp;quot; &amp;quot;policies&amp;quot; {
      + category        = &amp;quot;Application&amp;quot;
      + display_name    = &amp;quot;NETMEMO-POL2&amp;quot;
      + domain          = &amp;quot;default&amp;quot;
      + id              = (known after apply)
      + locked          = false
      + nsx_id          = (known after apply)
      + path            = (known after apply)
      + revision        = (known after apply)
      + sequence_number = 20
      + stateful        = true
      + tcp_strict      = (known after apply)

      + rule {
          + action                = &amp;quot;ALLOW&amp;quot;
          + destination_groups    = [
              + &amp;quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d&amp;quot;,
              + &amp;quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb&amp;quot;,
            ]
          + destinations_excluded = false
          + direction             = &amp;quot;IN_OUT&amp;quot;
          + disabled              = false
          + display_name          = &amp;quot;NETMEMO-LOCAL-R&amp;quot;
          + ip_version            = &amp;quot;IPV4_IPV6&amp;quot;
          + logged                = false
          + nsx_id                = (known after apply)
          + revision              = (known after apply)
          + rule_id               = (known after apply)
          + scope                 = [
              + &amp;quot;/infra/domains/default/groups/0fb48626-d903-42cc-9513-b47e7626f44d&amp;quot;,
              + &amp;quot;/infra/domains/default/groups/bac7f9c7-d80c-4bfe-8663-d3c9f973e2fb&amp;quot;,
            ]
          + sequence_number       = (known after apply)
          + services              = [
              + &amp;quot;/infra/services/59a29e53-9901-44d2-9589-f770c36d87bb&amp;quot;,
              + &amp;quot;/infra/services/HTTPS&amp;quot;,
            ]
          + source_groups         = [
              + &amp;quot;/infra/domains/default/groups/21d93fff-c08c-42dd-90da-d099dfa51163&amp;quot;,
            ]
          + sources_excluded      = false
        }
    }

Plan: 2 to add, 0 to change, 0 to destroy.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;terraform apply plan.out&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[&amp;quot;NETMEMO-POL1&amp;quot;]: Creating...
module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[&amp;quot;NETMEMO-POL2&amp;quot;]: Creating...
module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[&amp;quot;NETMEMO-POL2&amp;quot;]: Creation complete after 1s [id=bab4c14e-30c6-4633-b9d7-db760844e0e7]
module.nsxt-tf-cm-dfw.nsxt_policy_security_policy.policies[&amp;quot;NETMEMO-POL1&amp;quot;]: Creation complete after 1s [id=867b5893-15e3-4dab-a2f0-c8084af790a7]

Apply complete! Resources: 2 added, 0 changed, 0 destroyed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;7.Check the result through the UI&lt;/summary&gt;
&lt;a href=&#34;nsx-services.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;nsx-services.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;nsx-groups.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;nsx-groups.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;nsx-dfw.png&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;nsx-dfw.png&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;br /&gt;
&lt;/details&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Subnet size and mixing workload in a subnet</title>
      <link>https://netmemo.github.io/post/subnets-size-mix/</link>
      <pubDate>Tue, 18 May 2021 22:28:39 +0200</pubDate>
      
      <guid>https://netmemo.github.io/post/subnets-size-mix/</guid>
      <description>

&lt;p&gt;Below are some thoughts about subnet sizing and workloads mixing.&lt;br /&gt;
Along my different jobs, one of the things that led to L2 extension and then brought risks and slew down or even blocked projects was big heterogeneous subnets.&lt;/p&gt;

&lt;h1 id=&#34;small-subnet-scale-out&#34;&gt;Small subnet (scale out)&lt;/h1&gt;

&lt;h2 id=&#34;pros&#34;&gt;Pros&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Agility, you can do &lt;a href=&#34;https://blog.ipspace.net/2020/12/50-shades-high-availability.html&#34; target=&#34;_blank&#34;&gt;swimlane&lt;/a&gt; application design and migration per application without jeopardising the entire company.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cons&#34;&gt;Cons&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;More entry in the routing table but that can be mitigated with summarization.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Loss of ip addresses. If we take a big /21 subnets and split it in 64 x /27 you lost ~ 200 IPs (does it really matter ? how full are the big subnets ?)&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;big-subnet-scale-up-with-a-mix-of-workload&#34;&gt;Big subnet (scale up) with a mix of workload&lt;/h1&gt;

&lt;h2 id=&#34;pros-1&#34;&gt;Pros&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;IP address saving. Does that worth it ? Most of the time the subnets are big just to anticipate growth and those IPs are lost anyway.&lt;/li&gt;
&lt;li&gt;Simplicity to allocate addresses. Only a couple of big subnets.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cons-1&#34;&gt;Cons&lt;/h2&gt;

&lt;p&gt;&lt;u&gt; Big subnets size &lt;/u&gt;&lt;/p&gt;

&lt;p&gt;Most of the time this is to mutualize applications.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can block application migration because not all the applications have the same requirements. If one of the applications doesn&amp;rsquo;t want to reIP its workloads and has been asked to migrate somewhere else, you might be forced to stretch L2.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;L2 broadcast domain is mitigated by arp suppression on modern fabric but still you have broadcast reaching every VM and thus can have troubles if the &lt;a href=&#34;http://yves-louis.com/DCI/wp-content/uploads/2015/10/VXLAN-Multipod-geographically-dispersed-white-paper-final.pdf&#34; target=&#34;_blank&#34;&gt;stormcontrol is not set properly&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;For all the firewalls in the entreprise that are not relying on tags, you might have to open firewall rules per host instead of per subnets.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt; Mix of workloads &lt;/u&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can block application migration because physical devices can&amp;rsquo;t move where the VM is going to and you don&amp;rsquo;t want to reIP the VM or the physical device.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;other-considerations&#34;&gt;Other considerations&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pets vs cattle : VM vs Container&lt;br /&gt;
In the above use cases I&amp;rsquo;m talking essentially of workloads that are not containers. For containers the paradigm is often different because most of the time containers don&amp;rsquo;t share the subnet with physical device and application developper doesn&amp;rsquo;t rely on the container IP address. What matter is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fully_qualified_domain_name&#34; target=&#34;_blank&#34;&gt;FQDN&lt;/a&gt; of the service. VM are still considered most of the time as pets and nobody wants to change the IP address mostly to avoid to change firewall rules or hard coded IP addresses in application.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;VLANs Scale&lt;br /&gt;
The number of supported VLAN are less relevent in virtualized environments because most of the time software scale better and you are not limited to 4K vlans (or even 2K in some case for ACI &lt;a href=&#34;https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/4-x/verified-scalability/Cisco-ACI-Verified-Scalability-Guide-422.html&#34; target=&#34;_blank&#34;&gt;Bridge Domain&lt;/a&gt; and EVPN &lt;a href=&#34;https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus9000/sw/92x/scalability/guide_923/b_Cisco_Nexus_9000_Series_NX-OS_Verified_Scalability_Guide_923.html&#34; target=&#34;_blank&#34;&gt;Layer 2 VNIs&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Suboptimal IP addressing plan&lt;br /&gt;
When you move an entire subnet somewhere else you can break the summarization of the company but you can always do the summarization again after all the applications have moved or even reIP the entire subnet afterwards. It&amp;rsquo;s less risky and complex than having a L2 streched &amp;ldquo;forever&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Distributed routing&lt;br /&gt;
Now, with distributed routing you don&amp;rsquo;t have to go to the network core where the default gateway was in the old days to route between subnets. The trafic have a better distribution in the fabric and you should have less risks to congest uplinks with EST/WEST trafic.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Terraform Refactoring State File</title>
      <link>https://netmemo.github.io/post/terraform-refactoring-state-file/</link>
      <pubDate>Thu, 25 Feb 2021 21:56:57 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/terraform-refactoring-state-file/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/docs/cli/commands/state/mv.html&#34; target=&#34;_blank&#34;&gt;https://www.terraform.io/docs/cli/commands/state/mv.html&lt;/a&gt;&lt;br /&gt;
On windows :&lt;br /&gt;
&lt;pre style=&#34;color:black&#34;&gt;
terraform state mv nsxt_policy_security_policy.policy1 nsxt_policy_security_policy.policies[\&amp;ldquo;policy1\&amp;ldquo;]
&lt;/pre&gt;
It move resources from a construct like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;locals {
  policy1= {
      rule1 = {
        source = [&amp;quot;src1&amp;quot;,&amp;quot;src2&amp;quot;]
      }
  }
  policy2 = {
      rule1 = {
        source = [&amp;quot;src3&amp;quot;,&amp;quot;src4&amp;quot;]
      }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To a structure like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;locals {
  policies = {
    policy1 = {
      rule1 = {
        source = [&amp;quot;src1&amp;quot;,&amp;quot;src2&amp;quot;]
      }
    }
    policy2 = {
      rule2 = {
        source = [&amp;quot;src3&amp;quot;,&amp;quot;src4&amp;quot;]
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main moving from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;nsxt_policy_security_policy&amp;quot; &amp;quot;policy1&amp;quot;{
  display_name = &amp;quot;policy1&amp;quot;
  category     = &amp;quot;Environment&amp;quot;
  dynamic &amp;quot;rule&amp;quot; {
    for_each = local.policy1
    content {
      source_groups = rule.value[&amp;quot;sources&amp;quot;]
    }
  }
}
resource &amp;quot;nsxt_policy_security_policy&amp;quot; &amp;quot;policy2&amp;quot;{
  display_name = &amp;quot;policy2&amp;quot;
  category     = &amp;quot;Environment&amp;quot;
  dynamic &amp;quot;rule&amp;quot; {
    for_each = local.policy2
    content {
      source_groups = rule.value[&amp;quot;sources&amp;quot;]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;nsxt_policy_security_policy&amp;quot; &amp;quot;policies&amp;quot; {
for_each = local.policies
  display_name = each.key
  category     = &amp;quot;Environment&amp;quot;
  dynamic &amp;quot;rule&amp;quot; {
    for_each = each.value
    content {
      source_groups = rule.value[&amp;quot;sources&amp;quot;]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The terraform state moving fromfrom 2 resources to 1 resource with 2 instances&lt;br /&gt;
From&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;nsxt_policy_security_policy&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;policy1&amp;quot;
  &amp;quot;instances&amp;quot; : [
    {
      ...
    }
  ]
},
{
  &amp;quot;type&amp;quot;: &amp;quot;nsxt_policy_security_policy&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;policy2&amp;quot;
  &amp;quot;instances&amp;quot; : [
    {
      ...
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;nsxt_policy_security_policy&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;policies&amp;quot;
  &amp;quot;instances&amp;quot; : [
    {
      &amp;quot;index_key&amp;quot;: &amp;quot;policy1&amp;quot;
    },
    {
      &amp;quot;index_key&amp;quot;: &amp;quot;policy2&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Python Package Offline</title>
      <link>https://netmemo.github.io/post/python-package-offline/</link>
      <pubDate>Thu, 25 Feb 2021 21:46:14 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/python-package-offline/</guid>
      <description>&lt;p&gt;To install python packages offline (with no internet access), the simplest way is to dowload the packages with the dependencies on a server with internet access and the below command.&lt;/p&gt;

&lt;pre&gt;
&lt;b&gt;C:\Users\Nono\Desktop\python&gt;pip download requests&lt;/b&gt;
Collecting requests  
  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)  
Collecting urllib3&lt;1.27,&gt;=1.21.1  
  Using cached urllib3-1.26.3-py2.py3-none-any.whl (137 kB)  
Collecting certifi&gt;=2017.4.17  
  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)  
Collecting idna&lt;3,&gt;=2.5  
  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)  
Collecting chardet&lt;5,&gt;=3.0.2  
  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)  
Saved c:\users\nono\desktop\python\requests-2.25.1-py2.py3-none-any.whl  
Saved c:\users\nono\desktop\python\certifi-2020.12.5-py2.py3-none-any.whl  
Saved c:\users\nono\desktop\python\chardet-4.0.0-py2.py3-none-any.whl  
Saved c:\users\nono\desktop\python\idna-2.10-py2.py3-none-any.whl  
Saved c:\users\nono\desktop\python\urllib3-1.26.3-py2.py3-none-any.whl  
Successfully downloaded requests certifi chardet idna urllib3
&lt;/pre&gt;

&lt;p&gt;You then need to move all the files to the offline server in a directory and deploy the packages with the below command :&lt;/p&gt;

&lt;pre&gt;
&lt;b&gt;pip install --no-index --find-links=file:C:\Users\Nono2\tools\python-modules\resquests requests&lt;/b&gt;
Looking in links: file:///C:\Users\Nono2\tools\python-modules\resquests  
Collecting requestsCollecting urllib3&lt;1.27,&gt;=1.21.1 (from requests)  
Collecting chardet&lt;5,&gt;=3.0.2 (from requests)  
Collecting idna&lt;3,&gt;=2.5 (from requests)Collecting certifi&gt;=2017.4.17 (from requests)  
Installing collected package: urllib3, chardet, idna, certifi, requests  
Successfully installed certifi-2020.12.5 chardet-4.0.0 idna-2.10 requests-2.25.1 urllib3-1.26.3
&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>N5600 Buffering</title>
      <link>https://netmemo.github.io/post/n5600-buffering/</link>
      <pubDate>Tue, 23 Feb 2021 22:24:44 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/n5600-buffering/</guid>
      <description>

&lt;h4 id=&#34;qos-voq&#34;&gt;QoS VOQ&lt;/h4&gt;

&lt;p&gt;On N5K, In the case of unicast traffic, VOQ is an ingress buffer pool for 3 ingress ports (1 ASIC = 3 ports). This buffer pool is split into n x reservable buffer of the size configured in the voq-limit command. If the ingress buffer is 16000 and the VOQ limit is 1024, that mean 16 flow can reserv buffers. When the shared buffer is exausted, the dedicated ingress buffer per port is used then when it&amp;rsquo;s full, the packet is droped.&lt;/p&gt;

&lt;p&gt;VOQ is reservable per egress port per class (pair).&lt;br /&gt;
With small VOQ limit, there is lot of buffer available for non congested flow.&lt;br /&gt;
With no VOQ limit, a congested port can use up to 50% of the total shared memory and those, 2 congested port can exaust all the ingress resources of an ASIC (3 ingress port) and drop can happen on this ASIC&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2015/pdf/BRKDCT-3100.pdf&#34; target=&#34;_blank&#34;&gt;https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2015/pdf/BRKDCT-3100.pdf&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2017/pdf/BRKDCN-3346.pdf&#34; target=&#34;_blank&#34;&gt;https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2017/pdf/BRKDCN-3346.pdf&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.cisco.com/c/en/us/support/docs/switches/nexus-6000-series-switches/200401-Nexus-5600-6000-Understanding-and-Troub.html&#34; target=&#34;_blank&#34;&gt;https://www.cisco.com/c/en/us/support/docs/switches/nexus-6000-series-switches/200401-Nexus-5600-6000-Understanding-and-Troub.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In case of a Nexus 5600. 1 ASIC = 3 * 40Gb or 12*10G ports. Cells are the units in which buffers are allocated. One cell is 320 Bytes. ASIC Ingress buffer size : 48840 of available total cells (16M), shared among all 3x40Gb ports. Egress buffer is 9Mb (dedicated buffer per ASIC).&lt;br /&gt;
VOQ = Ingress per output port/class queues. E.g with 114 ports on the switch, with 8 queues per port there would be 1152 VOQs. On N5600 Buffer can be shared accrod port and classes, giving more burst absorption capacity.&lt;/p&gt;

&lt;h4 id=&#34;default-buffer-allocation&#34;&gt;Default buffer allocation&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;minimum fixed buffer of 312 cells (100KB) is reserved per class (up to 8 classes) per ingress port, rest of the buffer is shared. This is done to guarantee minimum performance.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If there is only 1 class (the class-default for instance), there is only a single fixed/dedicated buffer per port.&lt;br /&gt;
- 44,331 cells of shared buffer available for data traffic for all ports. shared buffer is used first.&lt;br /&gt;
- any drop class can access half of the shared buffer- no-drop class (eg fcoe) can access complete shared buffer.&lt;br /&gt;
- &amp;ldquo;queue-limit&amp;rdquo; under &amp;ldquo;network-qos&amp;rdquo; policy specifies the dedicated buffer for each port and each class. The dedicated buffer can be used by the port for only that class of service.&lt;/p&gt;

&lt;h4 id=&#34;voq-limit&#34;&gt;voq-limit&lt;/h4&gt;

&lt;p&gt;Command &amp;ldquo;hardware unicast voq-limit [threshold &amp;hellip;]&amp;rdquo;
enabling voq-limit turns on a shared buffer threshold per each voq.&lt;br /&gt;
limits the amount of buffers usable on the ingress interface, for packets headed towards a specific VoQ (&amp;ldquo;egress port, class&amp;rdquo; pair). Drops happens per VOQ, when its packet in ingress buffer exceed threshold: when the traffic ingress on a port and it consumes all 1024 cells, it will get dropped as discards.&lt;/p&gt;

&lt;p&gt;when the second flow traffic comes in, it will tage another 1024 cells as well but in a different VOQ and will not get dropped; this way voq thresholding prevents the non-congested egress port traffic drop.&lt;/p&gt;

&lt;h4 id=&#34;hold-mitigation-and-voq-thresholding&#34;&gt;HOLD mitigation and VOQ thresholding&lt;/h4&gt;

&lt;p&gt;Below is discussed for scenario without voq-limit enabledCongestion on one egress port in one CoS eventually bleeds into the congestion of its corresponding VOQ on the ingress port. Once the limit is reached then traffic gets dropped.&lt;/p&gt;

&lt;p&gt;On N5600 ASIC Buffers are allocated per ingress port and are shared by all the egress ports that are seeing traffic from this ingressport.&lt;br /&gt;
A stuck or slow-draining egress port can causse all buffers on one or more ingress ports that are senfing traffic to the egress port to be exhausted, thereby affecting all traffic on these ingress port. This is Head of Line Blocking (HOLB) problem.&lt;br /&gt;
To avoid this scenario, the VOQ for unicast traffic may be configured with a voq-limit threshold, at which point the port will stop accepting any more packets for congested destination (drops the packets or pauses the affected class for non-drop class type). When the queue length decrease and goes below another threshold, the VOQ starts accepting packets again.&lt;br /&gt;
By default VOQ Thresholding is disabled for all classes.&lt;/p&gt;

&lt;h4 id=&#34;questions&#34;&gt;Questions&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;About the limit of 8000 cells, why not setting directly 16000 or removeing the limitation ?&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Setting larger voq-limit increases the change to improve burst absorption but also leaves non-congested VOQs to be more likely affected by congested VOQs (as the latter can dip more into shared buffer).Disadvantage of having a voq-limit is that when we have a burst traffic comming in (like for distributed storage/VSAN), it connot use more than configured threshold of allocated cells and the bursty flow will have drops even though there is un-used ingress buffer.It&amp;rsquo;s recommanded to remove voq-limit in case of bursty traffic.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Are the VoQ per UPC or per ingress interface ?&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;VOQs are per class per egress interface. E.g. With 114 ports on the switch with 8 queues there would be 1152 VOQs per port.If there is only 1 class (class-defauklt), the number of VOQ is matching the number of egress ports.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Our undestanding is that if we remove the command voq limit, we might have HOLB while with the command it&amp;rsquo;s not possible. Could you explain how is this possible to have HOLD if VoQ is always used ?&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This can happen because VOQ Thresholding (voq-limit) is not enabled by default. Therefore, each VOQ can borrow from shared buffer and some of non-congested VOQs would not be able to handle traffic due to lack of buffer space, even though they are not congested.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Effect of changing or removing VOQ-limit on the traffic.&lt;br /&gt;
There would be subsecond traffic interruption on all ports.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Effect of changing or removing VOQ-Limit on the FEX.&lt;br /&gt;
There should not be any effect on the FEX operation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Could you describe the difference in the behavior of buffers (shared, ingress dedicated per port, voq per cos, drop) with the command voq limit default, with the command voq limit configured with the max value, and without the voq limit command ?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A. Without voq-limitShared - used by all ingress ports by default. 3*40G (or) 12*10G ports compete for usage, when per-port buffers are exhausted.Dedicated - samll, reserved per port per class. Can be adjusted with queue-limit command.VOQ drop thresholds are disabled.Shared is used first, only then overflow to dedicated.One drop class per ASIC can take up to half if the shared buffer.If congestion is constant, it can result in blocking for other VOQs in the same ingress port.Slow draining port can affect others by consuming shared and dedicated buffers.ExempleUnicast traffic coming on a 40G ingress port and egress port are 2x10G. When one of the egress port 10G is congested, it moght be possible that a new ingress flow from this 40G interface to another 10G interface can get affected as well because of non-availability of the ingress buffer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;n5600-buffering.png&#34; alt=&#34;N5600 Buffers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When the traffic ingress on a port it first fills the shared buffer. Each flow can take up ti 50% of the total shared buffer available (22200 cells or ~7.1MB). After filling 50% of the buffer the same flow will utilize the per-port fixed buffer.When the new flow comes ingress on that port, it tries to fill in the 50% of shared buffer but if shared and per-port buffer are already filled, this traffic would be dropped.We can avoir the new flow, that is goignt o different no-congested port, from getting dropped by enabling VOQ thresholding.&lt;/p&gt;

&lt;p&gt;B. With voq-limit default value 1024Drop happend per VOQ, each limited by 1024 cells.Congestion in one VOQ has minimal chances to affect other VOQsVOQ drop thresholds are minimal, so burst traffic flow coming in it cannot use more than 1024 allocated cells and will have drops even though there is un-used buffer.when the traffic ingress on a port consumes all 1024 cells, it will get dropped as discards.When the second flow traffic with destination to another egress port comes in, it may consume 1024 cells as well but a different VOQ- so will not get dropped.&lt;/p&gt;

&lt;p&gt;C. with voq-limit default value 16384drop happens per VOQ, each limited by 16384 cells.congestion in one VOQ does not affect other VOQs if there is enough shared buffer left.VOQ drop thresholds are at maximum.Burst traffic flow coming can use up to 16384 allocated cells and will have drop everything above it. Other VOQs buffering at the same instance can use remaining buffer (but nor more than 16384each). In theory, 3 VOQs, that are fully congested at the same time, could take all the buffer. It is very difficult to prefict instant buffer usage due to unpredictable nature of bursty flows, so exact values should be taken from production, e.g. Try with 16384 and reduce the threshold if negative impact is seen on non-congested flows.&lt;/p&gt;

&lt;h4 id=&#34;couple-of-solutions-to-resolve-bursty-trafic-drop&#34;&gt;Couple of solutions to resolve bursty trafic drop:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Remove VOQ limit completel (heavy burst trafic)&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Increase VOQ Threshold to 8000 or 16384 (max) and monitor the situation with discards=&amp;gt; show hardware profile buffer monitor interface ethernet&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Spread congested links betweek different ASICs.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Implement policing of the traffic.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Terraform nested for_each for NSX-T with dynamic</title>
      <link>https://netmemo.github.io/post/tf-nsxt-nested-for-each/</link>
      <pubDate>Fri, 19 Feb 2021 22:06:50 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/tf-nsxt-nested-for-each/</guid>
      <description>&lt;p&gt;The post below shows how to create security policy groups for NSX-T with Terraform nested for_each loop and dynamic.&lt;br /&gt;
The variables are made from one map of list. Each list represents one group composed of tags.&lt;br /&gt;
&lt;a href=&#34;https://www.hashicorp.com/blog/hashicorp-terraform-0-12-preview-for-and-for-each&#34; target=&#34;_blank&#34;&gt;https://www.hashicorp.com/blog/hashicorp-terraform-0-12-preview-for-and-for-each&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;variable &amp;quot;mapgroups&amp;quot; {
  type      = map
  default   = {
    NBO         = [&amp;quot;NBO&amp;quot;]
    NBO-PROD    = [&amp;quot;NBO&amp;quot;,&amp;quot;PROD&amp;quot;]
  }
}


resource &amp;quot;nsxt_policy_group&amp;quot; &amp;quot;nbogroups&amp;quot; {
  for_each      = var.mapgroups

  display_name  = each.key
  criteria {

    dynamic &amp;quot;condition&amp;quot; {
      for_each = each.value

      content {
        key         = &amp;quot;Tag&amp;quot;
        member_type = &amp;quot;VirtualMachine&amp;quot;
        operator    = &amp;quot;EQUALS&amp;quot;
        value       = condition.value
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Create portable Terraform and plugins with Terraform-bundle for Windows</title>
      <link>https://netmemo.github.io/post/tf-bundle-windows/</link>
      <pubDate>Wed, 17 Feb 2021 21:32:37 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/tf-bundle-windows/</guid>
      <description>

&lt;p&gt;The steps below are what I have followed to create a terraform-bundle to use terraform with non default providers on a server that doesn&amp;rsquo;t have access to Internet. You can find the tool explanation in the below link.&lt;br /&gt;
&lt;a href=&#34;https://github.com/hashicorp/terraform/tree/master/tools/terraform-bundle&#34; target=&#34;_blank&#34;&gt;https://github.com/hashicorp/terraform/tree/master/tools/terraform-bundle&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;installation of golang with msi downloaded here&lt;br /&gt;
&lt;a href=&#34;https://golang.org/doc/install&#34; target=&#34;_blank&#34;&gt;https://golang.org/doc/install&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Clone the terraform repository to get the tool&lt;br /&gt;
&lt;a href=&#34;https://github.com/hashicorp/terraform.git&#34; target=&#34;_blank&#34;&gt;https://github.com/hashicorp/terraform.git&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd terraform-master
go install .\tools\terraform-bundle
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;check-the-terraform-version&#34;&gt;Check the terraform version&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\noyel\Desktop\tfforeach\nsxt&amp;gt;terraform version
Terraform v0.14.6
+ provider registry.terraform.io/vmware/nsxt v3.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-a-terraform-bundle-hcl-file&#34;&gt;Create a terraform-bundle.hcl file&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  # Version of Terraform to include in the bundle. An exact version number
  # is required.
  version = &amp;quot;0.14.6&amp;quot;
}

# Define which provider plugins are to be included
providers {
  # Include the newest &amp;quot;nsxt&amp;quot; provider version in the 1.0 series.
  nsxt = {
    source = &amp;quot;vmware/nsxt&amp;quot;
    versions = [&amp;quot;~&amp;gt; 3.0.0&amp;quot;]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-the-bundle&#34;&gt;Create the bundle&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\noyel\Desktop\tfforeach\nsxt&amp;gt;terraform-bundle package terraform-bundle.hcl
Fetching Terraform 0.14.6 core package...
Local plugin directory &amp;quot;.plugins&amp;quot; found; scanning for provider binaries.
No &amp;quot;.plugins&amp;quot; directory found, skipping local provider discovery.
- Finding vmware/nsxt versions matching &amp;quot;~&amp;gt; 3.0.0&amp;quot;...
- Installing vmware/nsxt v3.0.1...
Creating terraform_0.14.6-bundle2021021713_windows_amd64.zip ...
All done!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Move the zip file to the server you want to use. Unzip the file. For a basic utilization move the terraform.exe and plugins in the directory where your terraform files are.&lt;/p&gt;

&lt;h4 id=&#34;the-provider-tf-file-looks&#34;&gt;The provider.tf file looks&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  required_providers {
    nsxt = {
      source = &amp;quot;vmware/nsxt&amp;quot;
      version = &amp;quot;3.0.1&amp;quot;
    }
  }
}

provider &amp;quot;nsxt&amp;quot; {
   host = &amp;quot;1.2.3.4&amp;quot;
   username   = &amp;quot;admin&amp;quot;
   password   = &amp;quot;123&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When initialize Terraform with the init command, specify the plugins directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\bubibi\terraform\terraform init -plugin-dir=C:\Users\bubibi\terraform\plugins

Terraform has been successfuly initialized!
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Using Terraform for_each to create subnets in AWS VPC</title>
      <link>https://netmemo.github.io/post/tf-for-each/</link>
      <pubDate>Wed, 17 Feb 2021 20:59:49 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/tf-for-each/</guid>
      <description>

&lt;p&gt;An extended explanation of the differences between for_each, for and count can be find on the link below
&lt;a href=&#34;https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9&#34; target=&#34;_blank&#34;&gt;https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The two main drawbacks of using count are :&lt;br /&gt;
- Can&amp;rsquo;t be used to loop over inline blocks&lt;br /&gt;
- Difficult to remove entry from a list because it changes the index and those Terraform may want to destroy the resource because it has a different index&lt;/p&gt;

&lt;p&gt;Below is an example of the variables used to create subnets within AWS VPCs and the main file with the for_each.
The variables contain a map of subnets maps with cidr and az (availability zone) attributes.
The for_each loop over the map of subnets maps to create the subnets.&lt;/p&gt;

&lt;h4 id=&#34;variables-tf&#34;&gt;variables.tf&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;variable &amp;quot;tag_name&amp;quot; {
   default = &amp;quot;main-vpc&amp;quot;
}

variable &amp;quot;vpc-cidr&amp;quot; {
   default = &amp;quot;10.0.0.0/16&amp;quot;
}

variable &amp;quot;basename&amp;quot; {
   description = &amp;quot;Prefix used for all resources names&amp;quot;
   default = &amp;quot;nbo&amp;quot;
}

#map of maps for create subnets
variable &amp;quot;prefix&amp;quot; {
   type = map
   default = {
      sub-1 = {
         az = &amp;quot;use2-az1&amp;quot;
         cidr = &amp;quot;10.0.198.0/24&amp;quot;
      }
      sub-2 = {
         az = &amp;quot;use2-az2&amp;quot;
         cidr = &amp;quot;10.0.199.0/24&amp;quot;
      }
      sub-3 = {
         az = &amp;quot;use2-az3&amp;quot;
         cidr = &amp;quot;10.0.200.0/24&amp;quot;
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;main-tf&#34;&gt;main.tf&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;aws_vpc&amp;quot; &amp;quot;main-vpc&amp;quot; {
  cidr_block = var.vpc-cidr

  tags = {
    Name = var.tag_name
  }
}

resource &amp;quot;aws_subnet&amp;quot; &amp;quot;main-subnet&amp;quot; {
  for_each = var.prefix
 
  availability_zone_id = each.value[&amp;quot;az&amp;quot;]
  cidr_block = each.value[&amp;quot;cidr&amp;quot;]
  vpc_id     = aws_vpc.main-vpc.id

  tags = {
    Name = &amp;quot;${var.basename}-subnet-${each.key}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find the output of the terraform plan/apply, the terraform.state and the others tf files in the below links.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/netmemo/tf-for-each-exemple&#34; target=&#34;_blank&#34;&gt;https://github.com/netmemo/tf-for-each-exemple&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BFD on directly connected</title>
      <link>https://netmemo.github.io/post/bfd-directly-connected/</link>
      <pubDate>Wed, 02 Dec 2020 09:29:50 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/bfd-directly-connected/</guid>
      <description>&lt;p&gt;One article that can help understanding and make decisions about fast failover
&lt;a href=&#34;https://blog.ipspace.net/2020/11/detecting-network-failure.html&#34; target=&#34;_blank&#34;&gt;https://blog.ipspace.net/2020/11/detecting-network-failure.html&lt;/a&gt;
&lt;a href=&#34;https://blog.ipspace.net/2012/09/do-we-need-lacp-and-udld.html&#34; target=&#34;_blank&#34;&gt;https://blog.ipspace.net/2012/09/do-we-need-lacp-and-udld.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is there any benefit by enabling BFD on directly connected interface ?&lt;/p&gt;

&lt;p&gt;Sometime it can be enabled to help veryfied that there are no issues on physical layer and data link layer of an interface. It can help if on the data link layer you are not using UDLD or LACP.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ACI from an other angle</title>
      <link>https://netmemo.github.io/post/aci-other-angle/</link>
      <pubDate>Sat, 28 Nov 2020 19:11:36 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/aci-other-angle/</guid>
      <description>

&lt;h3 id=&#34;several-of-these-protocols-are-standards&#34;&gt;&amp;ldquo;Several of these protocols are standards&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;My understanding is that even if the protocols look standard, Cisco made some modifications on them : VXLAN (fiels to transport ACI Policies), ISIS (added the multidestination tree) and hence are note standard anymore.&lt;/p&gt;

&lt;h3 id=&#34;does-it-require-proprietary-server&#34;&gt;&amp;ldquo;Does it require proprietary server ?&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;Not prorietary servers but proprietary switches&amp;hellip;So you are locked in regarding the software and the hardware. Both can&amp;rsquo;t be decoupled. If you choose to move to another switch vendor, you need to change the hardware and sart learning new software and protocols skills. Previously while you probably still need to change the software and the hardware, you didn&amp;rsquo;t need to learn everything from scratch regarding the protocols.&lt;br /&gt;
In the past with Fabric Path, I have already had issues by beeing locked-in with both. When Cisco will end up the support, you then need to change the hardware and learn new software/protocols skills.&lt;/p&gt;

&lt;h3 id=&#34;security-lock-in&#34;&gt;Security lock-in&lt;/h3&gt;

&lt;p&gt;The security policies in ACI are another lock-in. If you use the Cisco Application Centric mode, it‚Äôs even worse. When Cisco decides it‚Äôs not bankable anymore and starts moving away from it, you will need to migrate the hardware, learn new software, new protocols and migrate the security to something completely different.&lt;br /&gt;
For some organisations, this scenario will be a nightmare and probably cost more money than investing regulary in people.&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t want to use the Application Centric mode and just use the Network Centric mode, you still have an expensive solution with lots of options you will pay for but never use. Moreover you will inherit all the software complexity and associated bugs for features that are useless for you.&lt;/p&gt;

&lt;h3 id=&#34;summ-up&#34;&gt;Summ up&lt;/h3&gt;

&lt;p&gt;Standard Protocols not so standard:&lt;br /&gt;
    ISIS (Multidestination tree ftag)&lt;br /&gt;
    BGP for multi site&lt;br /&gt;
    VXLAN (Field to transport ACI policies)&lt;/p&gt;

&lt;p&gt;Proprietary Protocols&lt;br /&gt;
    COOP&lt;/p&gt;

&lt;p&gt;Very little public documentation&lt;/p&gt;

&lt;p&gt;Lock in : Hardware, software, security&lt;br /&gt;
    Previously : some features or knobs were proprietary or tied to hardware but not the all system&lt;br /&gt;
    =&amp;gt; EIGRP, software lock in but you can ignore it if you want&lt;br /&gt;
    =&amp;gt; FP at the time of the launch no other option were  standard&lt;/p&gt;

&lt;p&gt;Software complexity due to all the features you don&amp;rsquo;t want + software has been thought to be application aware and even if you don&amp;rsquo;t want to use it, you will anyway inherit the code complexity of it.&lt;/p&gt;

&lt;p&gt;Everything centralized in a box right in the middle of the data path (Spine COOP devices).&lt;/p&gt;

&lt;p&gt;Stopped product:&lt;br /&gt;
&lt;ul&gt;
&lt;li&gt; Loadbalancing : ACE/CSS  &lt;/li&gt;
&lt;li&gt; Fabric Path  &lt;/li&gt;
&lt;li&gt; Ironport ?  &lt;/li&gt;
&lt;li&gt; iWAN  &lt;/li&gt;
&lt;li&gt; VPn Concentrator  &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;You need EVPN anyway for multisite.&lt;/p&gt;

&lt;p&gt;You need automation anyway because it&amp;rsquo;s too complex to manage via the GUI and you want to standardise all the conf.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arista basic ISIS-SR</title>
      <link>https://netmemo.github.io/post/aristabasesr/</link>
      <pubDate>Tue, 24 Nov 2020 18:12:05 +0100</pubDate>
      
      <guid>https://netmemo.github.io/post/aristabasesr/</guid>
      <description>

&lt;h3 id=&#34;isis-segment-routing-basics-for-arista-eos&#34;&gt;ISIS Segment routing basics for Arista EOS&lt;/h3&gt;

&lt;p&gt;References:&lt;br /&gt;
&lt;a href=&#34;https://www.arista.com/en/um-eos/eos-section-35-3-is-is-segment-routing&#34; target=&#34;_blank&#34;&gt;https://www.arista.com/en/um-eos/eos-section-35-3-is-is-segment-routing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;aristabasesr.png&#34; alt=&#34;arista isis sr&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;srgb&#34;&gt;SRGB&lt;/h4&gt;

&lt;p&gt;Segment Routing Golbal Block&lt;/p&gt;

&lt;h4 id=&#34;prefix-sid&#34;&gt;Prefix-SID&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s global and unique. It identify a prefix. It&amp;rsquo;s called anycast SID when it&amp;rsquo;s send by a group of router.&lt;/p&gt;

&lt;h4 id=&#34;node-sid&#34;&gt;Node-SID&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s global and unique. Only one per node. It identify the node.&lt;/p&gt;

&lt;h4 id=&#34;adjacent-sid&#34;&gt;Adjacent-SID&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s local and can use a dynamique range. It&amp;rsquo;s used to identify an interconnection between 2 nodes.&lt;br /&gt;
With Arista EOS 4.23 it&amp;rsquo;s automaticaly allocated from the isis dynamic range &lt;strong&gt;show mpls label ranges&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&#34;basic-config-for-arista&#34;&gt;Basic config for ARISTA&lt;/h4&gt;

&lt;p&gt;I have added the prefix-sid to test it end to end between the VPC but actually it can work without the prefix-sid. In the next article I will add the L3 VPN on top of SR without using prefix-sid
In bold you find the ISIS-SR specific command require to enable ISIS-SR&lt;/p&gt;

&lt;h5 id=&#34;r1&#34;&gt;R1&lt;/h5&gt;

&lt;pre style=&#34;color:black&#34;&gt;
interface Loopback1
   ip address 1.1.1.1/32
   &lt;b&gt;node-segment ipv4 index 1&lt;/b&gt;
   isis enable ISIS-SR
!

interface Ethernet1
   no switchport
   ip address 10.10.12.1/24
   isis enable ISIS-SR
!
interface Ethernet2
   no switchport
   ip address 10.10.10.1/24
   isis enable ISIS-SR
!
ip routing
!
mpls ip
!

router isis ISIS-SR
   net 10.0000.0010.0100.1001.00
   is-type level-2
   !
   address-family ipv4 unicast
   !
   &lt;b&gt;segment-routing mpls &lt;/b&gt; 
      &lt;b&gt;no shutdown&lt;/b&gt;
      prefix-segment 10.10.10.0/24 index 51
!
&lt;/pre&gt;

&lt;h5 id=&#34;r2&#34;&gt;R2&lt;/h5&gt;

&lt;pre style=&#34;color:black&#34;&gt;
interface Loopback1
   ip address 2.2.2.2/32
   &lt;b&gt;node-segment ipv4 index 2&lt;/b&gt;
   isis enable ISIS-SR
!

interface Ethernet1
   no switchport
   ip address 10.10.12.2/24
   isis enable ISIS-SR
!

interface Ethernet2
   no switchport
   ip address 10.10.23.2/24
   isis enable ISIS-SR
!

ip routing
!
mpls ip
!

router isis ISIS-SR
   net 10.0000.0020.0200.2002.00
   is-type level-2
   !
   address-family ipv4 unicast
   !
   &lt;b&gt;segment-routing mpls&lt;/b&gt;
      &lt;b&gt;no shutdown&lt;/b&gt;
!
&lt;/pre&gt;

&lt;h5 id=&#34;r3&#34;&gt;R3&lt;/h5&gt;

&lt;pre style=&#34;color:black&#34;&gt;
interface Loopback1
   ip address 3.3.3.3/32
   &lt;b&gt;node-segment ipv4 index 3&lt;/b&gt;
   isis enable ISIS-SR
!

interface Ethernet1
   no switchport
   ip address 10.10.23.3/24
   isis enable ISIS-SR
!
interface Ethernet2
   no switchport
   ip address 10.10.30.1/24
   isis enable ISIS-SR
!
ip routing
!
mpls ip
!

router isis ISIS-SR
   net 10.0000.0030.0300.3003.00
   is-type level-2
   !
   address-family ipv4 unicast
   !
   &lt;b&gt;segment-routing mpls&lt;/b&gt;
      &lt;b&gt;no shutdown&lt;/b&gt;
      prefix-segment 10.10.30.0/24 index 53
!
&lt;/pre&gt;

&lt;h3 id=&#34;show-commands&#34;&gt;show commands&lt;/h3&gt;

&lt;pre style=&#34;color:black&#34;&gt;
&lt;b&gt;R1#show isis segment-routing&lt;/b&gt;

System ID: R1                   Instance: ISIS-SR
SR supported Data-plane: MPLS                   SR Router ID: 1.1.1.1
SR Global Block( SRGB ): Base: 900000           Size: 65536           
Adj-SID allocation mode: SR-adjacencies
Adj-SID allocation pool: Base: 100000     Size: 16384

All Prefix Segments have    : P:0 E:0 V:0 L:0
IS-IS Reachability Algorithm : SPF (0)

Number of IS-IS segment routing capable peers: 2

Self-Originated Segment Statistics:
Node-Segments       : 1
Prefix-Segments     : 1
Proxy-Node-Segments : 0
Adjacency Segments  : 1
&lt;/pre&gt;

&lt;pre style=&#34;color:black&#34;&gt;
&lt;b&gt;R1#show mpls segment-routing bindings&lt;/b&gt;

1.1.1.1/32
   Local binding:  Label: imp-null
   Remote binding: Peer ID: 0020.0200.2002, Label: 900001
2.2.2.2/32
   Local binding:  Label: 900002
   Remote binding: Peer ID: 0020.0200.2002, Label: imp-null
3.3.3.3/32
   Local binding:  Label: 900003
   Remote binding: Peer ID: 0020.0200.2002, Label: 900003
10.10.10.0/24
   Local binding:  Label: imp-null
   Remote binding: Peer ID: 0020.0200.2002, Label: 900051
10.10.30.0/24
   Local binding:  Label: 900053
   Remote binding: Peer ID: 0020.0200.2002, Label: 900053
&lt;/pre&gt;

&lt;pre style=&#34;color:black&#34;&gt;
&lt;b&gt;R1#show isis segment-routing prefix-segments&lt;/b&gt;

System ID: 0010.0100.1001                       Instance: &#39;ISIS-SR&#39;
SR supported Data-plane: MPLS                   SR Router ID: 1.1.1.1

Node: 3      Proxy-Node: 0      Prefix: 2       Total Segments: 5

Flag Descriptions: R: Re-advertised, N: Node Segment, P: no-PHP
                   E: Explicit-NULL, V: Value, L: Local
Segment status codes: * - Self originated Prefix, L1 - level 1, L2 - level 2
  Prefix                      SID Type       Flags                   System ID       Level Protection
  ------------------------- ----- ---------- ----------------------- --------------- ----- ----------
* 1.1.1.1/32                    1 Node       R:0 N:1 P:0 E:0 V:0 L:0 0010.0100.1001  L2    unprotected
  2.2.2.2/32                    2 Node       R:0 N:1 P:0 E:0 V:0 L:0 0020.0200.2002  L2    unprotected
  3.3.3.3/32                    3 Node       R:0 N:1 P:0 E:0 V:0 L:0 0030.0300.3003  L2    unprotected
* 10.10.10.0/24                51 Prefix     R:0 N:0 P:0 E:0 V:0 L:0 0010.0100.1001  L2    unprotected
  10.10.30.0/24                53 Prefix     R:0 N:0 P:0 E:0 V:0 L:0 0030.0300.3003  L2    unprotected
&lt;/pre&gt;

&lt;pre style=&#34;color:black&#34;&gt;
&lt;b&gt;R1#show isis segment-routing adjacency-segments&lt;/b&gt;

System ID: R1                   Instance: ISIS-SR
SR supported Data-plane: MPLS                   SR Router ID: 1.1.1.1
Adj-SID allocation mode: SR-adjacencies
Adj-SID allocation pool: Base: 100000     Size: 16384
Adjacency Segment Count: 1
Flag Descriptions: F: Ipv6 address family, B: Backup, V: Value
                   L: Local, S: Set

Segment Status codes: L1 - Level-1 adjacency, L2 - Level-2 adjacency, P2P - Point-to-Point adjacency, LAN - Broadcast adjacency

Locally Originated Adjacency Segments
Adj IP Address  Local Intf     SID   SID Source                 Flags     Type  
--------------- ----------- ------- ------------ --------------------- -------- 
    10.10.12.2         Et1  100000      Dynamic   F:0 B:0 V:1 L:1 S:0   LAN L2  

 Protection 
----------- 
unprotected 
&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Bgp Multipath</title>
      <link>https://netmemo.github.io/post/bgp-multipath/</link>
      <pubDate>Mon, 11 May 2020 08:02:21 +0200</pubDate>
      
      <guid>https://netmemo.github.io/post/bgp-multipath/</guid>
      <description>

&lt;h3 id=&#34;maximum-path&#34;&gt;Maximum path&lt;/h3&gt;

&lt;p&gt;In the RIB + FIB ECMP (multipath)&lt;/p&gt;

&lt;p&gt;Is PIC supported by default ?&lt;br /&gt;
&lt;a href=&#34;https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/irg-bgp-mp-pic.html&#34; target=&#34;_blank&#34;&gt;https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/irg-bgp-mp-pic.html&lt;/a&gt;&lt;br /&gt;
=&amp;gt; With BGP Multipath, the BGP prefix-independant convergence (PIC) feature is supported&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins&gt; Attribut that should be identical &lt;/ins&gt;&lt;/strong&gt;&lt;br /&gt;
- Weight&lt;br /&gt;
- LP&lt;br /&gt;
- AS Path (AS Number unless relax us used, AS length)&lt;br /&gt;
- Origin Code&lt;br /&gt;
- MED&lt;br /&gt;
- IGP Metric Next hop should be different&lt;/p&gt;

&lt;h3 id=&#34;add-path&#34;&gt;Add Path&lt;/h3&gt;

&lt;p&gt;If the path are equal, allow to advertise more than one bes oath (need to test in eBGP).&lt;br /&gt;
Mostly used with BGP without MPLS. If MPLS is used it&amp;rsquo;s better to have an RD different for each VRF of each router (easier to troubleshoot).&lt;br /&gt;
&lt;a href=&#34;https://orhanergun.net/wp-content/uploads/2019/11/BGP-Add-path-vs-Shadow-RR-vs-Shadow-Session-vs-Unique-RD.pdf&#34; target=&#34;_blank&#34;&gt;https://orhanergun.net/wp-content/uploads/2019/11/BGP-Add-path-vs-Shadow-RR-vs-Shadow-Session-vs-Unique-RD.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;pic&#34;&gt;PIC&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2016/pdf/BRKRST-3321.pdf&#34; target=&#34;_blank&#34;&gt;https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2016/pdf/BRKRST-3321.pdf&lt;/a&gt;
I don&amp;rsquo;t neded it at the moment because we don&amp;rsquo;t need this level if convergence (couple of minutes vs ms if you have hundres of thousand of prefixes)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.ipspace.net/2012/01/prefix-independent-convergence-pic.html&#34; target=&#34;_blank&#34;&gt;https://blog.ipspace.net/2012/01/prefix-independent-convergence-pic.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The generic optimization of the RIB-to-FIB update process is known as Prefix-Independent Convergence (PIC) - if the routing protocols can pre-compute alternate paths, suitably designed FIB can use that information to cache alternate next hops. Updating such a FIB no longer involves numerous updates to individual prefixes; you have to change only the next hop reachability information.&lt;/p&gt;

&lt;h3 id=&#34;best-external&#34;&gt;Best External&lt;/h3&gt;

&lt;p&gt;Allow a router to advertise it&amp;rsquo;s best external path even if in it&amp;rsquo;s BGP table it does have a beter route from inside&lt;/p&gt;

&lt;h3 id=&#34;label-allocation&#34;&gt;LABEL ALLOCATION&lt;/h3&gt;

&lt;h5 id=&#34;per-vrf&#34;&gt;Per VRF&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins&gt; Cons &lt;/ins&gt;&lt;/strong&gt;&lt;br /&gt;
- IP Lookup needed after label lookup (can be a benefit, cf route sum issue)&lt;br /&gt;
- No granular load balancing because the bottom label is the same for all prefixes, if platform load balances on bottom label&lt;br /&gt;
- Potential forwarding loop during local traffic diversion to support PIC (Transient loop)&lt;br /&gt;
- No support for EIBGP multipath&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins&gt; Pros &lt;/ins&gt;&lt;/strong&gt;&lt;br /&gt;
- 1 label per vrf (less label used)&lt;/p&gt;

&lt;h5 id=&#34;per-ce&#34;&gt;Per CE&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins&gt; Cons &lt;/ins&gt;&lt;/strong&gt;&lt;br /&gt;
- No granular load balancing because the bottom label is the same for all prefixes from one CE, if platform load balances on bottom label&lt;br /&gt;
- eBGPload balancing &amp;amp; BGP PIC is not supported (it makes usage of label diversity), unless resilient per-ce label&lt;br /&gt;
- Only single hop eBGPsupported, no multihop&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins&gt; Pros &lt;/ins&gt;&lt;/strong&gt;&lt;br /&gt;
- No IP lookup needed after label lookup&lt;br /&gt;
- Per-CE : one MPLS label per next-hop (so per connected CE router)(Number of MPLS labels used is very low)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
